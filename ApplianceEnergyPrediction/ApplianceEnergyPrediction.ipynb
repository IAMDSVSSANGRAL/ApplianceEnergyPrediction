{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IAMDSVSSANGRAL/applianceenergyprediction/blob/main/Appliance_energy_prediction_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 -Samadhan Tangde**\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective:\n",
        "The objective of this project is to develop a regression model that accurately predicts the energy consumption of household appliances based on various input features. The model aims to provide insights into energy usage patterns and facilitate energy efficiency improvements in residential settings.\n",
        "\n",
        "Data:\n",
        "The project utilizes a dataset that contains information on household appliance energy consumption along with several relevant input features. The dataset includes variables such as temperature, humidity, time of day, and various appliance power readings. The data is collected over a specific time period and is representative of real-world residential energy usage scenarios.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "Exploratory Data Analysis (EDA):\n",
        "\n",
        "Perform a thorough analysis of the dataset to understand the distribution, statistics, and relationships among variables.\n",
        "Identify any missing values, outliers, or data quality issues that need to be addressed.\n",
        "Visualize the data using appropriate charts and graphs to gain insights into the patterns and trends.\n",
        "Data Preprocessing:\n",
        "\n",
        "Handle missing values by applying suitable imputation techniques or deciding on appropriate strategies for dealing with them.\n",
        "Address outliers and anomalies by considering various methods such as removal, transformation, or capping.\n",
        "Normalize or scale the data if necessary to ensure all features are on a similar scale.\n",
        "Feature Engineering:\n",
        "\n",
        "Explore the relationships between the input features and the target variable (appliance energy consumption) to identify potential feature engineering opportunities.\n",
        "Create new features, derive meaningful variables, or transform existing variables to capture important patterns or interactions in the data.\n",
        "Model Development:\n",
        "\n",
        "Split the dataset into training and testing sets for model development and evaluation.\n",
        "Select an appropriate regression algorithm (e.g., linear regression, decision tree regression, random forest regression) based on the project requirements and characteristics of the data.\n",
        "Train the model using the training data and tune hyperparameters to optimize performance.\n",
        "Evaluate the model's performance using various metrics such as mean squared error (MSE), mean absolute error (MAE), and R-squared.\n",
        "Model Evaluation and Interpretation:\n",
        "\n",
        "Assess the model's performance on the testing data to measure its ability to generalize to unseen data.\n",
        "Interpret the model's coefficients or feature importance to gain insights into the factors that have the most significant impact on appliance energy consumption.\n",
        "Validate the model's predictions against domain knowledge or external benchmarks to ensure its reliability and usefulness.\n",
        "Model Deployment and Recommendations:\n",
        "\n",
        "Deploy the trained model into a production environment or create a user-friendly interface for stakeholders to interact with the model.\n",
        "Provide recommendations based on the model's predictions and insights to improve energy efficiency, optimize appliance usage, or suggest modifications in residential settings.\n",
        "Conclusion:\n",
        "The Appliance Energy Prediction regression project aims to develop a robust regression model to accurately predict household appliance energy consumption. By analyzing and understanding the data, performing feature engineering, and building an effective regression model, the project provides valuable insights and recommendations for optimizing energy usage and promoting energy-efficient practices in residential settings.\n",
        "\n",
        "Note: This project summary provides a general outline and can be tailored based on specific requirements, dataset characteristics, and project goals."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/IAMDSVSSANGRAL/applianceenergyprediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, here is the problem statement broken down into bullet points using different phrases:\n",
        "\n",
        "- **Data Source**: The dataset spans approximately 4.5 months and includes information collected at 10-minute intervals. It consists of data from a ZigBee wireless sensor network monitoring temperature and humidity in a house, energy consumption recorded by m-bus energy meters, and weather data from Chievres Airport, Belgium.\n",
        "\n",
        "- **Data Averaging**: The wireless sensor network reports temperature and humidity every 3.3 minutes, but the data is averaged over 10-minute periods.\n",
        "\n",
        "- **Objective**: The primary goal is to develop a machine learning model capable of accurately predicting energy usage based on the provided features.\n",
        "\n",
        "- **Utility**: This predictive model has potential applications for building managers, energy companies, and policymakers. It can aid in optimizing energy consumption, reducing costs, and minimizing the environmental impact of energy usage.\n",
        "\n",
        "- **Influence Factors**: The model aims to consider a range of influencing factors, including temperature, humidity, illumination, and time of day, all of which can impact energy consumption in a building.\n",
        "\n",
        "- **Pattern and Trend Identification**: Building managers and energy firms can benefit from this model by identifying patterns and trends in energy consumption. This can help them make informed decisions, such as adjusting HVAC settings, optimizing lighting, or implementing energy-efficient solutions.\n",
        "\n",
        "- **Policymaker Applications**: Policymakers can also leverage the insights from this model to develop regulations and incentives that promote energy efficiency and sustainability.\n",
        "\n",
        "- **Random Variables**: The dataset includes random variables designed for testing regression models and filtering out non-predictive features.\n",
        "\n",
        "- **Integration of External Data**: External weather data from Chievres Airport, Belgium, was integrated into the dataset using date and time columns, enhancing the model's ability to make energy usage forecasts.\n",
        "\n",
        "- **Environmental Impact**: One of the broader goals is to contribute to reducing the environmental impact of energy usage through better management and decision-making."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -"
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "\n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "\n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "\n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "\n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime as dt\n",
        "\n",
        "# Import Data Visualisation Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly as pl\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "from pandas.plotting import scatter_matrix\n",
        "%matplotlib inline\n",
        "\n",
        "# Set the plot style and display options\n",
        "plt.style.use('ggplot')\n",
        "sns.set()\n",
        "\n",
        "# To display all the columns in Dataframe\n",
        "pd.set_option('display.max_columns', None)\n",
        "# Import Library to visualise missing data\n",
        "import missingno as mno\n",
        "\n",
        "# Import and Ignore warnings for better code readability,\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the data set\n",
        "data_raw = pd.read_csv('/content/drive/MyDrive/Santa/Regression capstone/data_application_energy.csv')"
      ],
      "metadata": {
        "id": "TmkdQzZojsHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a copy of data set\n",
        "data = data_raw.copy()"
      ],
      "metadata": {
        "id": "HJ8f2WTWL-YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "num_rows, num_cols = data.shape\n",
        "\n",
        "print(\"Number of rows:\", num_rows)\n",
        "print(\"Number of columns:\", num_cols)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your date column is named \"date_column\"\n",
        "data['date'] = pd.to_datetime(data['date'])"
      ],
      "metadata": {
        "id": "76kdxS8WdrbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting date as the index:\n",
        "data.set_index('date', inplace=True)"
      ],
      "metadata": {
        "id": "5kPxyGdERI8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count assinged a dataframe name 'df'\n",
        "df = data[data.duplicated()]"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#There is no duplicate rows in the data\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bVN4H8UU09Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is in the form of a Pandas DataFrame with 29 columns and 19,735 rows. It appears to be a dataset with multiple features and observations, but without the context of what this dataset represents, it's challenging to provide specific insights. However, I can offer some general insights you can gain from this data:\n",
        "\n",
        "1. **Data Size**: The dataset contains 19,735 data points, which is a significant amount of data.\n",
        "\n",
        "2. **Data Types**: Most of the columns contain numerical data, with 26 columns having float64 data type and 2 columns with int64 data type. The 'date' column seems to contain date values as objects.\n",
        "\n",
        "3. **Features**: The columns labeled 'T1,' 'T2,' 'T3,' etc., represent temperature measurements, while columns labeled 'RH_1,' 'RH_2,' 'RH_3,' etc., represent relative humidity measurements. 'Appliances' and 'lights' are integer columns, which might be related to energy consumption and lighting. Other columns have labels such as 'T_out' (outdoor temperature), 'Windspeed,' 'RH_out' (outdoor humidity), and more.\n",
        "\n",
        "4. **Data Completeness**: There are no missing values (non-null) in any of the columns, which is a good sign for data quality.\n",
        "\n",
        "5. **Memory Usage**: The dataset consumes 4.4+ MB of memory, which might be relevant for memory-constrained analyses.\n",
        "\n",
        "6. **NO Duplicate values**:We don't see any output, it's possible that there are no duplicated rows in your original DataFrame data.\n",
        "\n",
        "To gain more meaningful insights from this data, you'll need to have a clear understanding of what the dataset represents and what kind of analysis you want to perform. Depending on the context, you could explore relationships between different variables, conduct statistical analysis, visualize data, and build predictive models. Please provide more information about the dataset and your specific goals if you'd like more detailed insights."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "data.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The observation data consists of the following variables:**\n",
        "\n",
        "\n",
        "datetime year-month-day hour : minute:second\n",
        "\n",
        "Appliances: energy use in Wh [TARGETED]\n",
        "\n",
        "lights: energy use of light fixtures in the house in Wh\n",
        "\n",
        "T1: Temperature in kitchen area, in Celsius\n",
        "\n",
        "RH_1: Humidity in kitchen area, in %\n",
        "\n",
        "T2: Temperature in living room area, in Celsius\n",
        "\n",
        "RH_2:Humidity in living room area, in %\n",
        "\n",
        "T3:Temperature in laundry room area\n",
        "\n",
        "RH_3:Humidity in laundry room area, in %\n",
        "\n",
        "T4:Temperature in office room, in Celsius\n",
        "\n",
        "RH_4:Humidity in office room, in %\n",
        "\n",
        "T5:Temperature in bathroom, in Celsius\n",
        "\n",
        "RH_5:Humidity in bathroom, in %\n",
        "\n",
        "T6:Temperature outside the building (north side), in Celsius\n",
        "\n",
        "RH_6:Humidity outside the building (north side), in %\n",
        "\n",
        "T7:Temperature in ironing room , in Celsius\n",
        "\n",
        "RH_7:Humidity in ironing room, in %\n",
        "\n",
        "T8:Temperature in teenager room 2, in Celsius\n",
        "\n",
        "RH_8:Humidity in teenager room 2, in %\n",
        "\n",
        "T9:Temperature in parents room, in Celsius\n",
        "\n",
        "RH_9:Humidity in parents room, in %\n",
        "\n",
        "T_out:Temperature outside (from Chièvres weather station), in Celsius\n",
        "\n",
        "Press_mm_hg: (from Chièvres weather station), in mm Hg\n",
        "\n",
        "RH_out: Humidity outside (from Chièvres weather station), in %\n",
        "\n",
        "Windspeed: (from Chièvres weather station), in m/s\n",
        "\n",
        "Visibility: (from Chièvres weather station), in km\n",
        "\n",
        "Tdewpoint: (from Chièvres weather station), °C\n",
        "\n",
        "rv1: Random variable 1, nondimensional\n",
        "\n",
        "rv2: Rnadom variable 2, nondimensional"
      ],
      "metadata": {
        "id": "PYzlcej3OU5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Unique Values count for each variable.\n",
        "for i in data.columns.tolist():\n",
        "  print(\"The unique values in\",i, \"is\",data[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "LGAz2eAYwhmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Round the unique values to two decimal places\n",
        "rounded_unique_values = data.apply(lambda x: set(round(val, 2) for val in x))\n",
        "\n",
        "# Print the unique values for each feature\n",
        "for feature, unique in rounded_unique_values.items():\n",
        "    print(f'{feature}: {unique}')"
      ],
      "metadata": {
        "id": "_6QYnjVk4wNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating columns:\n",
        "temperature_column = [i for i in data.columns if \"T\" in i]\n",
        "humidity_column = [i for i in data.columns if \"RH\" in i]\n",
        "other = [i for i in data.columns if (\"T\" not in i)&(\"RH\" not in i)]"
      ],
      "metadata": {
        "id": "Kst3KQHVRrVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#close look on temprature column\n",
        "data[temperature_column].describe(include='all')"
      ],
      "metadata": {
        "id": "DzXNlgYLRtMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can derive several insights:\n",
        "\n",
        "1. **Count**:\n",
        "   - There are 19,735 data points for each of the temperature-related variables (T1, T2, T3, T4, T5, T6, T7, T8, T9, T_out, and Tdewpoint). This indicates that there are no missing values in these columns.\n",
        "\n",
        "2. **Mean (Average)**:\n",
        "   - The mean values for the temperature-related variables are in the range of 16.79°C to 26.26°C. The \"T3\" variable has the highest mean at approximately 22.27°C, while \"T5\" has the lowest mean at about 19.59°C.\n",
        "\n",
        "3. **Standard Deviation (std)**:\n",
        "   - The standard deviations for the temperature-related variables range from approximately 1.61°C to 2.20°C. Variables like \"T3\" and \"T4\" have relatively low variability, while \"T9\" has slightly higher variability.\n",
        "\n",
        "4. **Minimum (min)**:\n",
        "   - The minimum values for the temperature-related variables range from 15.10°C to 29.24°C. These values indicate the lower bounds of the temperature measurements.\n",
        "\n",
        "5. **25th Percentile (25%)**:\n",
        "   - The 25th percentile values represent the lower quartile of the data. For example, the 25th percentile of \"T2\" is approximately 18.79°C.\n",
        "\n",
        "6. **Median (50%)**:\n",
        "   - The median values (50th percentile) represent the middle values of the dataset. For instance, the median temperature \"T7\" is approximately 20.03°C.\n",
        "\n",
        "7. **75th Percentile (75%)**:\n",
        "   - The 75th percentile values represent the upper quartile of the data. The 75th percentile of \"T6\" is approximately 11.26°C.\n",
        "\n",
        "8. **Maximum (max)**:\n",
        "   - The maximum values represent the upper bounds of the temperature measurements. \"T4\" has the highest maximum value at approximately 26.20°C, while \"T5\" has the lowest maximum at about 25.79°C."
      ],
      "metadata": {
        "id": "u2zLtBh8XctQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#close look on humidity column\n",
        "data[humidity_column].describe()"
      ],
      "metadata": {
        "id": "w_j4XCxISHY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e can derive several insights regarding the relative humidity (RH) variables:\n",
        "\n",
        "1. **Count**:\n",
        "   - There are 19,735 data points for each of the RH-related variables (RH_1, RH_2, RH_3, RH_4, RH_5, RH_6, RH_7, RH_8, RH_9, and RH_out). This indicates that there are no missing values in these columns.\n",
        "\n",
        "2. **Mean (Average)**:\n",
        "   - The mean values for the relative humidity variables vary across the columns. For example, \"RH_5\" has the highest mean at approximately 50.95%, while \"RH_7\" has the lowest mean at around 35.39%. The \"RH_out\" variable, which represents outdoor relative humidity, has a mean of approximately 79.75%.\n",
        "\n",
        "3. **Standard Deviation (std)**:\n",
        "   - The standard deviations for the relative humidity variables also vary. \"RH_5\" has a standard deviation of approximately 9.02, indicating relatively higher variability, while \"RH_3\" has a lower standard deviation of around 3.25.\n",
        "\n",
        "4. **Minimum (min)**:\n",
        "   - The minimum values for the relative humidity variables indicate the lower bounds of the humidity measurements. For example, \"RH_6\" has a minimum of approximately 1.00% which look like there are outlier on lower bound of RH_6 and \"RH_out\" has a minimum of 24.00%.\n",
        "\n",
        "5. **25th Percentile (25%)**:\n",
        "   - The 25th percentile values represent the lower quartile of the data. \"RH_7\" has a 25th percentile value of approximately 31.50%.\n",
        "\n",
        "6. **Median (50%)**:\n",
        "   - The median values (50th percentile) represent the middle values of the dataset. \"RH_9\" has a median relative humidity of approximately 40.90%.\n",
        "\n",
        "7. **75th Percentile (75%)**:\n",
        "   - The 75th percentile values represent the upper quartile of the data. \"RH_4\" has a 75th percentile value of approximately 42.16%.\n",
        "\n",
        "8. **Maximum (max)**:\n",
        "   - The maximum values represent the upper bounds of the relative humidity measurements. \"RH_1\" has the highest maximum value at approximately 63.36%, and \"RH_out\" has the lowest maximum value at 100.00%.\n",
        "\n"
      ],
      "metadata": {
        "id": "f0n4GXA2YRTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[other].describe()"
      ],
      "metadata": {
        "id": "StZKZkcwSQpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can derive several insights regarding the variables Appliances, lights, Press_mm_hg, Windspeed, Visibility, rv1, and rv2:\n",
        "\n",
        "1. **Appliances**:\n",
        "   - The \"Appliances\" variable represents energy consumption related to appliances. The data ranges from a minimum of 10 to a maximum of 1080, with an average (mean) consumption of approximately 97.69. **The standard deviation is relatively high, indicating significant variability in appliance energy usage.**\n",
        "\n",
        "2. **Lights**:\n",
        "   - The \"lights\" variable shows energy consumption related to lighting. It varies from 0 to 70, with an average of approximately 3.80. The standard deviation suggests some variability in lighting energy consumption. **upto 75 percent of value have 0 values which is slightly ODD.**\n",
        "\n",
        "3. **Press_mm_hg**:\n",
        "   - \"Press_mm_hg\" represents atmospheric pressure. The pressure varies from 729.30 to 772.30, with an average of approximately 755.52. The data has relatively low variability.\n",
        "\n",
        "4. **Windspeed**:\n",
        "   - The \"Windspeed\" variable indicates wind speed and varies from 0 to 14. The average wind speed is about 4.04. The standard deviation suggests some variation in wind speed. **Maximum value is 14 which is very far from 75% of values that is 5.50**\n",
        "\n",
        "5. **Visibility**:\n",
        "   - \"Visibility\" represents the visibility in meters. It ranges from 1 to 66, with an average of approximately 38.33. The data exhibits relatively **high variability**.\n",
        "\n",
        "6. **rv1 and rv2**:\n",
        "   - The columns \"rv1\" and \"rv2\" have identical statistics, suggesting that they are likely **highly correlated or identical features**. They have a minimum value of approximately 0.0053 and a maximum value of around 49.9965."
      ],
      "metadata": {
        "id": "fg_iUznxVuXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to map current column names to new column names\n",
        "column_mapping = {'T1': 'KITCHEN_TEMP',\n",
        "    'RH_1': 'KITCHEN_HUM',\n",
        "    'T2': 'LIVING_TEMP',\n",
        "    'RH_2' :'LIVING_HUM',\n",
        "    'T3': 'BEDROOM_TEMP',\n",
        "    'RH_3':'BEDROOM_HUM',\n",
        "    'T4' : 'OFFICE_TEMP',\n",
        "    'RH_4' : 'OFFICE_HUM',\n",
        "    'T5' : 'BATHROOM_TEMP',\n",
        "    'RH_5': 'BATHROOM_HUM',\n",
        "    'T6':'OUTSIDE_TEMP_build',\n",
        "    'RH_6': 'OUTSIDE_HUM_build',\n",
        "    'T7': 'IRONING_ROOM_TEMP',\n",
        "    'RH_7' : 'IRONING_ROOM_HUM',\n",
        "    'T8' :'TEEN_ROOM_2_TEMP',\n",
        "    'RH_8' : 'TEEN_ROOM_HUM',\n",
        "    'T9': 'PARENTS_ROOM_TEMP',\n",
        "    'RH_9': 'PARENTS_ROOM_HUM',\n",
        "    'T_out' :'OUTSIDE_TEMP_wstn',\n",
        "    'RH_out' :'OUTSIDE_HUM_wstn'}\n",
        "\n",
        "# Rename the columns using the mapping\n",
        "data.rename(columns=column_mapping, inplace=True)"
      ],
      "metadata": {
        "id": "xE1jxXD7iQ5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "ZW2VQUgF9wi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new features\n",
        "data['month'] = data.index.month\n",
        "data['weekday'] = data.index.weekday\n",
        "data['hour'] = data.index.hour\n",
        "data['week'] = data.index.week\n",
        "data['day'] = data.index.day\n",
        "data['day_of_week'] = data.index.dayofweek"
      ],
      "metadata": {
        "id": "UGcDgyyT3nUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(2)"
      ],
      "metadata": {
        "id": "lXzEM5GB3sMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting values of the \"lights\" column:\n",
        "data['lights'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "4WZ0JUpVUwPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "77% value of lights column are 0 and it is not relevant for prediction. so we are going to drop this column"
      ],
      "metadata": {
        "id": "TBGBIpteVhXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the lights column:\n",
        "data.drop(columns='lights', inplace=True)"
      ],
      "metadata": {
        "id": "VJbGWhxOVcbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reorder the data for clear vision\n",
        "desired_order = ['KITCHEN_TEMP','LIVING_TEMP','BEDROOM_TEMP','OFFICE_TEMP','BATHROOM_TEMP','OUTSIDE_TEMP_build','IRONING_ROOM_TEMP','TEEN_ROOM_2_TEMP','PARENTS_ROOM_TEMP','OUTSIDE_TEMP_wstn',\n",
        "                 'KITCHEN_HUM','LIVING_HUM','BEDROOM_HUM','OFFICE_HUM','BATHROOM_HUM','OUTSIDE_HUM_build','IRONING_ROOM_HUM','TEEN_ROOM_HUM','PARENTS_ROOM_HUM','OUTSIDE_HUM_wstn',\n",
        "                 \"Tdewpoint\",\"Press_mm_hg\",\"Windspeed\",\"Visibility\",\"rv1\", \"rv2\",'month','weekday','hour','week','day','day_of_week',\"Appliances\"]\n",
        "#assinging new_data as new name of dataframe\n",
        "data = data.reindex(columns=desired_order)"
      ],
      "metadata": {
        "id": "6Xw2W7igdBfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail(2)"
      ],
      "metadata": {
        "id": "vdToJwPyiufd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AUTOEDA\n",
        "!pip install sweetviz\n",
        "import sweetviz as sv\n",
        "sweet_report = sv.analyze(data)\n",
        "sweet_report.show_html('sweet_report.html')"
      ],
      "metadata": {
        "id": "4hKGGwU4HsqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pivot table to aggregate the daily energy consumption\n",
        "daily_energy = data.pivot_table(values='Appliances', index='day', columns='month', aggfunc = 'mean')\n",
        "\n",
        "# Create a heatmap using the pivot table\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Daily Energy Consumption')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Day')\n",
        "plt.imshow(daily_energy, cmap='YlGnBu', aspect='auto')\n",
        "plt.colorbar(label='Energy Consumption')\n",
        "plt.xticks(range(0,5), ['Jan', 'Feb', 'Mar', 'Apr', 'May'])\n",
        "plt.yticks(range(1, 32))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xSZHw-DN80Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart used in the code is a heatmap. Heatmaps are chosen for this type of data visualization for several reasons:\n",
        "\n",
        "1. **Data Aggregation:** The data is being aggregated into a pivot table that summarizes daily energy consumption across different months and days. Heatmaps are effective for visualizing aggregated data and patterns.\n",
        "\n",
        "2. **Visualizing Two Dimensions:** Heatmaps are well-suited for displaying two dimensions of data simultaneously, in this case, 'month' and 'day.' Each cell in the heatmap represents a combination of these two dimensions.\n",
        "\n",
        "3. **Color Encoding:** Heatmaps use color to encode the values in each cell, making it easy to differentiate between high and low values. In this code, warmer colors (yellow-green) represent higher energy consumption, while cooler colors (blue) represent lower consumption.\n",
        "\n",
        "4. **Insight into Patterns:** Heatmaps are excellent for identifying patterns and trends in data, such as seasonal variations or correlations between days and months.\n",
        "\n",
        "5. **Customization:** The code provides customization options, such as setting the title, labels, and the color map (cmap), which allows for tailoring the visualization to the specific dataset and objectives.\n",
        "\n",
        "In this case, the heatmap is used to visualize and gain insights into daily energy consumption trends over different months and days, which is often essential for understanding patterns and optimizing energy usage."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mixed-color heatmap showing varying energy consumption across 5 months and 2 to 5 high-energy consumption days in each month provides several insights:\n",
        "\n",
        "1. **Seasonal Patterns:** The heatmap suggests that there are specific periods or seasons within each month when energy consumption is notably higher. This could be attributed to weather conditions, special occasions, or other external factors.\n",
        "\n",
        "2. **Weekends vs. Weekdays:** The heatmap might reveal a recurring pattern of higher energy consumption on weekends (e.g., Saturdays and Sundays) compared to weekdays. This could be due to increased activities or appliance usage on weekends.\n",
        "\n",
        "3. **Outliers and Anomalies:** Days with exceptionally high energy consumption (e.g., spikes) are clearly visible as isolated cells with warm colors. Investigating these outliers can help identify reasons behind sudden surges in energy usage.\n",
        "\n",
        "4. **Energy Efficiency:** The mixed heatmap suggests that there are days with moderate to lower energy consumption, indicating potential opportunities for improving energy efficiency. Analyzing these lower-consumption days can help in understanding what practices or conditions lead to reduced energy usage.\n",
        "\n",
        "5. **Month-to-Month Variation:** The heatmap allows for a visual comparison of energy consumption across different months. If a particular month consistently stands out with high consumption days, it may be linked to seasonal variations or specific factors unique to that month.\n",
        "\n",
        "6. **Decision Support:** Understanding these patterns and insights from the heatmap can be valuable for decision-making, such as optimizing energy management strategies, scheduling maintenance, or implementing energy-saving measures.\n",
        "\n",
        "In summary, the mixed-color heatmap highlights variations in energy consumption, enabling the identification of patterns, outliers, and opportunities for improving energy efficiency and consumption management."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the heatmap can potentially have both positive and negative business impacts, depending on how they are leveraged and addressed. Here's an evaluation of potential outcomes:\n",
        "\n",
        "**Positive Business Impacts**:\n",
        "\n",
        "1. **Energy Optimization**: Understanding when and why energy consumption is high allows businesses to optimize their energy usage during peak periods, potentially reducing costs and environmental impact.\n",
        "\n",
        "2. **Cost Savings**: Identifying opportunities for reducing energy consumption on lower-usage days can lead to cost savings in the long run. Implementing energy-efficient practices or technology can be financially beneficial.\n",
        "\n",
        "3. **Resource Allocation**: Recognizing seasonal patterns helps with allocating resources effectively. For instance, if specific months have consistently high energy consumption, businesses can plan for extra staffing, maintenance, or supplies.\n",
        "\n",
        "4. **Predictive Maintenance**: By pinpointing outliers in energy usage, organizations can proactively address issues related to appliances or systems that may be driving high consumption, leading to longer equipment lifespans and reduced downtime.\n",
        "\n",
        "**Negative Business Impacts**:\n",
        "\n",
        "1. **Operational Challenges**: The insights may reveal that energy consumption is consistently high during months where the business operations require it. In such cases, reducing energy usage may negatively impact productivity.\n",
        "\n",
        "2. **Seasonal Variability**: If the high-energy consumption months are due to unavoidable external factors (e.g., extreme weather conditions), it may be challenging to mitigate the effects, leading to increased operational costs and potential service disruptions.\n",
        "\n",
        "3. **Capital Investment**: Implementing energy-efficient technologies or practices may require significant upfront investments. The negative impact could be felt in the short term before cost savings are realized.\n",
        "\n",
        "4. **User Comfort**: Efforts to reduce energy consumption may result in discomfort for occupants (e.g., temperature adjustments). Striking the right balance between energy efficiency and user comfort is essential.\n",
        "\n",
        "The key lies in how organizations respond to these insights. By taking a strategic approach and investing in energy-efficient solutions, many businesses can achieve positive outcomes, including cost savings and reduced environmental impact. However, there may be initial challenges and costs to overcome, especially if high energy consumption is closely tied to essential operations or external factors. To minimize negative impacts, it's crucial for businesses to find the right balance between energy efficiency and operational requirements while leveraging the insights for informed decision-making."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the day of the week values to their respective names\n",
        "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "data['day_of_week'] = data['day_of_week'].map(lambda x: day_names[x])\n",
        "\n",
        "# Create a box plot or violin plot to compare energy consumption across different days of the week\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='day_of_week', y='Appliances', data=data, order=day_names)  # or sns.violinplot()\n",
        "plt.title('Appliance Energy Consumption by Day of the Week')\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Energy Consumption')"
      ],
      "metadata": {
        "id": "vFnJADwHDwAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a box plot (or violin plot) to visualize energy consumption by day of the week because it provides several advantages for this type of analysis:\n",
        "\n",
        "1. **Comparison of Distributions**: Box plots or violin plots allow us to compare the distribution of energy consumption across different days of the week. This is important because it reveals variations, central tendencies, and potential outliers.\n",
        "\n",
        "2. **Categorical Data**: We are comparing energy consumption across different categories (days of the week), making box plots a suitable choice. They help us understand how the dependent variable (energy consumption) varies for each category.\n",
        "\n",
        "3. **Identification of Outliers**: Box plots show outliers as individual data points, making it easy to identify unusual energy consumption patterns on specific days.\n",
        "\n",
        "4. **Summary Statistics**: Box plots display summary statistics, such as medians, quartiles, and potential skewness in the data, providing a comprehensive view of the distribution.\n",
        "\n",
        "5. **Order and Clarity**: By setting the order of the x-axis (day of the week) and labeling it, we ensure the plot is both organized and interpretable.\n",
        "\n",
        "Overall, box plots (or violin plots) are an effective choice for summarizing and comparing energy consumption data across different days of the week, enabling easy identification of patterns, outliers, and central tendencies."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The box plot reveals that energy consumption on weekdays (Monday to Friday) tends to stay within the range of 0 to 200, with the majority clustered around the lower end. However, it also highlights numerous outliers with energy consumption above 200, particularly on weekdays. This suggests that while most days exhibit lower energy use, there are frequent instances of high energy consumption, possibly due to specific events or activities. The insights aid in identifying patterns and potential areas for energy optimization during weekdays."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights can potentially lead to a positive business impact as they provide a deeper understanding of energy consumption patterns. By recognizing the occurrences of high energy consumption on weekdays, businesses can take actions to optimize and reduce energy usage during peak periods, leading to cost savings and environmental benefits.\n",
        "\n",
        "However, there is also a potential for negative growth if these energy spikes are due to essential business operations. In such cases, reducing energy consumption might negatively impact productivity or customer service. Therefore, the business should carefully evaluate the specific causes behind these outliers before implementing changes to ensure a balance between energy optimization and maintaining operational efficiency."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a line plot to show the trend of energy consumption over time\n",
        "import plotly.express as px\n",
        "\n",
        "# Assuming you have a DataFrame 'data' with a datetime index\n",
        "fig = px.line(data, x=data.index, y='Appliances', title='Energy Consumption of Appliances Over Time')\n",
        "fig.update_xaxes(title_text='Date', tickangle=-45)\n",
        "fig.update_yaxes(title_text='Energy Consumption')\n",
        "\n",
        "# Show the Plotly figure\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "oDsn83aNjoxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a line plot using Plotly Express because it effectively illustrates the trend of energy consumption over time. A line plot is suitable for displaying how a continuous variable (in this case, energy consumption) changes with respect to time. It allows for a clear visualization of any patterns, seasonality, or trends in the data. In addition, Plotly Express provides interactive features that enable users to explore and analyze the data points interactively, enhancing the understanding of temporal patterns in energy consumption."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line plot of energy consumption over time reveals several insights:\n",
        "\n",
        "1. **Seasonal Patterns**: There is a noticeable repetitive pattern in energy consumption. This could be related to changing seasons, weather conditions, or external factors.\n",
        "\n",
        "2. **Spikes and Dips**: The presence of high and low spikes indicates variations in energy usage. High spikes might correspond to specific events or periods of increased appliance usage, while dips could signify lower usage during certain timeframes.\n",
        "\n",
        "3. **Trends**: There might be overall trends in energy consumption, such as gradual increases or decreases over time. Identifying these trends is crucial for energy management.\n",
        "\n",
        "4. **Anomalies**: The chart also highlights outliers or anomalies in energy consumption. These are data points significantly deviating from the usual pattern and could be worth investigating for their underlying causes.\n",
        "\n",
        "5. **Patterns Over Time**: The chart provides insights into how energy consumption changes over time, which can be valuable for predicting future demand, optimizing energy usage, and making informed decisions about resource allocation.\n",
        "\n",
        "Overall, the line plot is a useful tool for understanding the temporal behavior of energy consumption, which can inform decisions related to energy management, cost optimization, and resource planning."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained can have a positive business impact. Understanding seasonal patterns and anomalies can lead to more efficient energy management, cost savings, and resource allocation. However, if not acted upon, the presence of outliers could lead to negative consequences, such as increased energy costs or equipment wear. Proactive measures are essential to leverage the insights positively."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the lights column:\n",
        "data.drop(columns='day_of_week', inplace=True)"
      ],
      "metadata": {
        "id": "AWWj0N4zlMRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Examining the outlier in the dataset\n",
        "# Assuming 'data' is your DataFrame\n",
        "num_columns = len(data.columns)\n",
        "fig, axes = plt.subplots(nrows=num_columns, figsize=(8, num_columns*6))\n",
        "\n",
        "for i, column in enumerate(data.columns):\n",
        "    # Exclude 'day_of_week' from the visualization\n",
        "    if column != 'day_of_week':\n",
        "        data.boxplot(column=column, ax=axes[i])\n",
        "        axes[i].set_title(f'Box Plot for {column}')\n",
        "        axes[i].set_xlabel('Column')\n",
        "        axes[i].set_ylabel('Values')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart, a series of box plots for each feature in the dataset, was chosen to visualize the distribution, central tendency, and outliers in the data for each variable. It helps in quickly identifying any extreme values or variations in the data and is a useful exploratory tool for data analysis and outlier detection."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The box plots reveal that several features in the dataset have values extending beyond the whiskers, indicating the presence of outliers. These outliers suggest the existence of extreme data points that deviate from the typical distribution, potentially affecting statistical analysis and model performance. Identifying and handling these outliers is essential for accurate analysis and predictive modeling."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from identifying outliers can have a positive business impact by enabling data-driven decisions. It helps in identifying anomalies and errors in data, improving data quality, and enhancing model accuracy. For example, outlier detection in energy consumption data can lead to more accurate predictions and better energy management, potentially reducing costs.\n",
        "\n",
        "However, failing to address outliers may have a negative impact on business. Outliers can distort analysis and model predictions, leading to inaccurate decision-making. In the context of energy consumption, ignoring extreme values could result in inefficient energy use, increased costs, and negative environmental impacts.\n",
        "\n",
        "Therefore, addressing outliers is essential to ensure data accuracy, optimize resource utilization, and make informed, positive business decisions.\n",
        "\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "#close look on four columns\n",
        "fig_sub = make_subplots(rows=1, cols=4, shared_yaxes=False)\n",
        "\n",
        "fig_sub.add_trace(go.Box(y=data['Appliances'].values,name='Appliances'),row=1, col=1)\n",
        "fig_sub.add_trace(go.Box(y=data['Windspeed'].values,name='Windspeed'),row=1, col=2)\n",
        "fig_sub.add_trace(go.Box(y=data['Visibility'].values,name='Visibility'),row=1, col=3)\n",
        "fig_sub.add_trace(go.Box(y=data['Press_mm_hg'].values,name='Press_mm_hg'),row=1, col=4)\n",
        "\n",
        "fig_sub.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked this specific chart because it allows for a side-by-side comparison of the distributions of four different columns: 'Appliances,' 'Windspeed,' 'Visibility,' and 'Press_mm_hg.' Using box plots in a single row helps visualize the spread, central tendency, and presence of outliers in these numerical features, enabling a quick and effective comparison. This visualization is particularly useful for assessing the distribution and potential variations within these specific columns."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals several insights:\n",
        "\n",
        "1. **Appliances Distribution:** The 'Appliances' column exhibits a wide range of values, indicating varying energy consumption.\n",
        "\n",
        "2. **Windspeed Range:** 'Windspeed' data has a broad distribution with potential outliers, suggesting fluctuating wind speeds.\n",
        "\n",
        "3. **Visibility Distribution:** 'Visibility' data also varies significantly, signifying varying levels of visibility.\n",
        "\n",
        "4. **Pressure Variability:** 'Press_mm_hg' displays variations and potential outliers, indicating fluctuations in atmospheric pressure.\n",
        "\n",
        "These insights can be vital for understanding the data's distribution and identifying potential data anomalies."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from this visualization can have a positive business impact and some insights that may be a concern for negative growth:\n",
        "\n",
        "Positive Impact:\n",
        "1. **Energy Consumption Patterns:** Understanding the distribution of energy consumption ('Appliances') can help businesses optimize energy usage and potentially reduce costs.\n",
        "\n",
        "Concern for Negative Growth:\n",
        "1. **Windspeed Outliers:** The presence of outliers in 'Windspeed' might lead to extreme weather conditions. High wind speeds can damage infrastructure, potentially resulting in increased maintenance or repair costs.\n",
        "\n",
        "2. **Pressure Variability:** Fluctuations in atmospheric pressure ('Press_mm_hg') can impact climate control systems. It may result in higher energy consumption for temperature regulation, potentially increasing operational costs.\n",
        "\n",
        "In summary, while insights related to energy consumption are positive for optimizing energy use, extreme weather conditions and atmospheric pressure variability could pose challenges leading to increased costs and potentially negative growth. Businesses should focus on strategies to mitigate these concerns."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame with the energy consumption data\n",
        "# You can group the data by hour and calculate the mean energy consumption for each hour\n",
        "hourly_energy = data.groupby('hour')['Appliances'].mean()\n",
        "\n",
        "# Create a line chart to visualize the hourly energy consumption patterns\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(hourly_energy.index, hourly_energy.values, marker='o', linestyle='-')\n",
        "plt.title('Hourly Energy Consumption Patterns')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Energy Consumption (mean)')\n",
        "plt.xticks(range(24))\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this specific line chart to visualize the hourly energy consumption patterns because it effectively displays how energy consumption varies throughout the day. It helps identify peak hours and trends in energy usage, which is valuable for businesses to optimize operations and potentially reduce energy costs. The chart provides a clear and concise representation of hourly consumption, making it easy to interpret and act upon."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the chart are as follows:\n",
        "\n",
        "1. **Peak Hours:** There is a notable rise in energy consumption from 6 am to 6 pm. This suggests that the majority of energy-intensive activities or appliance usage occurs during these hours, which is common in households and businesses as people wake up, start their day, and engage in various activities.\n",
        "\n",
        "2. **Evening Decline:** After 6 pm, there is a decline in energy consumption. This decline is likely due to people winding down their activities, turning off or using fewer appliances, and eventually going to sleep.\n",
        "\n",
        "3. **Midday Dip:** There is a small dip in energy consumption between 11 am and 3 pm. This could be attributed to reduced activity during the midday hours when people might be at work or engaged in activities outside the home, resulting in lower energy usage.\n",
        "\n",
        "These insights can be valuable for businesses and individuals looking to optimize energy consumption, potentially reducing costs during peak hours and making informed decisions about energy management.\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights can help create a positive business impact in various ways:\n",
        "\n",
        "**Positive Business Impact:**\n",
        "1. **Cost Savings:** Understanding the hourly energy consumption patterns allows businesses to implement strategies to reduce energy usage during peak hours. This can lead to cost savings, especially in commercial and industrial settings where energy costs are significant.\n",
        "\n",
        "2. **Resource Allocation:** Businesses can better allocate resources by scheduling energy-intensive tasks or processes during periods of lower energy consumption. This optimization can improve efficiency and reduce operational costs.\n",
        "\n",
        "3. **Sustainability:** By identifying periods of high energy consumption, businesses can focus on reducing their carbon footprint during peak hours. This aligns with sustainability goals and environmental responsibility, which is a positive aspect for businesses in today's eco-conscious world.\n",
        "\n",
        "However, there are no specific insights in the chart that directly lead to negative growth. The insights are mainly related to energy consumption patterns, and optimizing energy usage typically results in positive outcomes, such as cost savings and sustainability.\n",
        "\n",
        "Therefore, by leveraging these insights, businesses can make informed decisions to enhance their energy efficiency and operational effectiveness, ultimately having a positive impact on their bottom line and sustainability efforts."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame with the relevant columns (e.g., 'KITCHEN_TEMP', 'OUTSIDE_TEMP_build', and 'Appliances')\n",
        "# You can create a scatter plot with a regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='KITCHEN_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can create a scatter plot with a regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='LIVING_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can create a scatter plot with a regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='BEDROOM_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can create a scatter plot with a regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='OFFICE_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can create a scatter plot with a regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='BATHROOM_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can create a scatter plot with a regression line for outdoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='OUTSIDE_TEMP_build', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can also create a similar scatter plot and regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='IRONING_ROOM_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Outdoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Outdoor Temperature (OUTSIDE_TEMP_build)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can also create a similar scatter plot and regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='TEEN_ROOM_2_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Outdoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Outdoor Temperature (OUTSIDE_TEMP_build)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can also create a similar scatter plot and regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='PARENTS_ROOM_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Outdoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Outdoor Temperature (OUTSIDE_TEMP_build)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can also create a similar scatter plot and regression line for outdoor temperature data from weather station vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='OUTSIDE_TEMP_wstn', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Outdoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Outdoor Temperature (OUTSIDE_TEMP_build)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose scatter plots with regression lines to visualize the relationship between indoor and outdoor temperatures (in different rooms) and energy consumption. These charts are valuable because they allow us to:\n",
        "\n",
        "1. **Identify Relationships:** Scatter plots help us visually assess whether there's any apparent correlation between indoor/outdoor temperatures and energy consumption.\n",
        "\n",
        "2. **Regression Line:** The regression line provides insights into the direction and strength of the relationship. For example, if the line has a positive slope, it indicates that as temperatures increase, energy consumption tends to increase.\n",
        "\n",
        "3. **Multiple Features:** I created multiple charts to explore the relationship with various indoor and outdoor temperatures, providing a comprehensive view of the impact of temperature on energy usage in different parts of a building.\n",
        "\n",
        "4. **Alpha for Transparency:** I used 'alpha' to make the individual data points semi-transparent, making it easier to see dense areas of data.\n",
        "\n",
        "These charts are particularly helpful for understanding how temperature variations influence energy consumption, which is crucial for energy management and optimizing heating and cooling systems in buildings."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The spread of data points across the scatter plot and the relatively horizontal regression line suggest that there is a weak or nearly no linear relationship between indoor or outdoor temperatures and energy consumption.\n",
        "\n",
        "Insights:\n",
        "\n",
        "1. **Weak Relationship:** The horizontal regression line indicates that as indoor or outdoor temperatures change, there is little effect on energy consumption. This implies that energy consumption is not significantly driven by temperature variations, at least within the range observed in the dataset.\n",
        "\n",
        "2. **Non-Linearity:** The wide dispersion of data points suggests that other factors likely contribute to energy consumption variations. Non-linear relationships or interactions with other variables might be at play.\n",
        "\n",
        "3. **No Clear Trend:** There's no apparent trend indicating that energy consumption consistently increases or decreases with temperature changes. This lack of a consistent pattern is important for managing energy efficiently.\n",
        "\n",
        "4. **Other Influencing Factors:** The weak relationship emphasizes the importance of considering other factors like occupancy, time of day, or appliance usage patterns that might be more significant in explaining energy consumption variations.\n",
        "\n",
        "Overall, the charts suggest that while temperature certainly plays a role in energy consumption, it is not the dominant factor in this dataset. Understanding the specific factors that drive energy consumption is crucial for optimizing energy use and building management."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights suggest no strong correlation between temperature and energy consumption. This can be positive if businesses aim to optimize energy use through other means, like appliance efficiency and usage patterns. However, a negative impact could occur if a business invests in temperature control systems expecting significant energy savings based on temperature alone. Careful consideration of factors is essential."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame with the relevant columns (e.g., 'hour' and 'Appliances')\n",
        "# You can create a line chart to show energy consumption throughout the day\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Group the data by hour and calculate the mean energy consumption for each hour\n",
        "hourly_energy = data.groupby('hour')['Appliances'].mean()\n",
        "\n",
        "# Split the data into daytime (6:00 AM to 6:00 PM) and nighttime (6:00 PM to 6:00 AM)\n",
        "daytime_energy = hourly_energy[6:18]\n",
        "\n",
        "nighttime_energy= hourly_energy[0:6].append(hourly_energy[18:24])\n",
        "\n",
        "# Plot the daytime and nighttime energy consumption\n",
        "plt.plot(daytime_energy.index, daytime_energy.values, label='Daytime', marker='o',color = 'r')\n",
        "plt.plot(nighttime_energy.index, nighttime_energy.values, label='Nighttime', marker='o',color = 'b')\n",
        "\n",
        "plt.title('Energy Consumption Throughout the Day')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Mean Energy Consumption')\n",
        "plt.xticks(range(24))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a line chart to visualize the energy consumption throughout the day, specifically distinguishing between daytime and nighttime consumption. This chart effectively shows how energy usage varies over a 24-hour period, highlighting the differences between daytime and nighttime patterns."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight from the chart is that there is a noticeable pattern in energy consumption throughout the day. Energy consumption increases during the daytime, with the highest consumption occurring in the afternoon. In contrast, energy consumption decreases during the nighttime hours, reaching its lowest point in the early morning. This suggests that energy usage is influenced by the time of day, with higher demand during daytime hours."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights can potentially help create a positive business impact. Understanding the pattern of energy consumption throughout the day allows businesses to optimize energy management. For example, they can implement strategies to reduce energy consumption during peak hours, thus reducing energy costs and promoting sustainability. Additionally, businesses can adjust energy production or distribution strategies to meet the varying demand throughout the day, potentially reducing operational costs.\n",
        "\n",
        "However, if businesses do not adapt to these insights and continue to consume energy at a consistent rate, it may lead to negative consequences, such as increased energy costs during peak hours and potential strain on energy infrastructure. Therefore, taking action based on these insights is crucial for a positive business impact."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame with relevant columns (e.g., 'weekday' and 'Appliances')\n",
        "# You can create a line chart to compare energy consumption on weekdays vs. weekends\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Group the data by 'weekday' and calculate the mean energy consumption for weekdays and weekends\n",
        "weekday_energy = data[data['weekday'] < 5].groupby('hour')['Appliances'].mean()\n",
        "weekend_energy = data[data['weekday'] >= 5].groupby('hour')['Appliances'].mean()\n",
        "\n",
        "# Plot energy consumption for weekdays and weekends\n",
        "plt.plot(weekday_energy.index, weekday_energy.values, label='Weekdays', marker='o')\n",
        "plt.plot(weekend_energy.index, weekend_energy.values, label='Weekends', marker='o')\n",
        "\n",
        "plt.title('Energy Consumption on Weekdays vs. Weekends')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Mean Energy Consumption')\n",
        "plt.xticks(range(24))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this chart because it effectively compares energy consumption patterns between weekdays and weekends, providing insights into how energy usage varies based on the day of the week. It helps identify whether there are differences in energy consumption behavior during workdays (weekdays) and non-workdays (weekends). This information can be valuable for businesses and households to make informed decisions regarding energy management and resource allocation."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight from the chart is that energy consumption patterns significantly differ between weekdays and weekends. On weekdays, there is a notable increase in energy consumption during the evening hours, likely due to people returning home from work and using household appliances. In contrast, on weekends, energy consumption peaks during the afternoon, suggesting that people may engage in more activities or use appliances differently during non-working days. This information can help businesses and individuals optimize energy usage and potentially reduce costs during specific times of the week."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights can potentially help create a positive business impact. By understanding the differences in energy consumption patterns between weekdays and weekends, businesses and utility providers can develop strategies to optimize energy production and distribution. For example, they can adjust energy generation and distribution schedules to meet the varying demands throughout the week. This optimization can lead to more efficient energy utilization, reduced costs, and potentially a more environmentally friendly operation.\n",
        "\n",
        "However, it's essential to consider potential negative impacts. For instance, if businesses or utility providers do not adapt to these insights and continue to supply energy uniformly throughout the week, it could lead to overproduction, increased costs, and potentially negative environmental consequences. Therefore, the key is to leverage these insights to make informed decisions that align energy supply with demand patterns for overall positive business and environmental impacts."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing distributions using Histograms:\n",
        "data.hist(figsize=(17, 20), grid=True);"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It produces a set of histograms for visualizing the distributions of data. Histograms are used to display the frequency or count of data points within specified bins or intervals. They help you understand the data's central tendency, spread, and underlying patterns. It's a common way to visualize the distribution of a single variable and identify features like modes, skewness, or presence of multiple subpopulations in the data. This type of plot is used to explore and understand the shape and characteristics of individual variables in your dataset."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When certain features (day, week, hour, weekday, month, rv2, rv1, visibility, windspeed, parent_room_hum, outside_hum_wstn, outside_hum_build, bathroom_hum, office_hum, bedroom_hum) do not follow a normal distribution while others do, it suggests that these features might not be normally distributed or may have different underlying distributions. This insight can have several implications for your data analysis:\n",
        "\n",
        "1. Modeling Assumptions: When using statistical models that assume normality (e.g., linear regression), features not following normal distribution may violate model assumptions. This can affect the model's accuracy and reliability.\n",
        "\n",
        "2. Feature Engineering: Non-normally distributed features may require transformations (e.g., log, square root) to make them more closely resemble a normal distribution. This can improve model performance.\n",
        "\n",
        "3. Outliers: Non-normally distributed features may indicate the presence of outliers or extreme values that need to be addressed, as they can influence model outcomes.\n",
        "\n",
        "4. Data Understanding: It's important to understand the nature of the underlying distributions for each feature to make informed decisions about data preprocessing and model selection.\n",
        "\n",
        "5. Skewed Data: Skewed distributions (e.g., right-skewed or left-skewed) can affect the interpretation of statistical metrics and may require different statistical techniques.\n",
        "\n",
        "In summary, non-normally distributed features should be carefully handled in your analysis, considering potential transformations or alternative modeling techniques to account for their distribution characteristics."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained about non-normally distributed features will likely have a positive business impact. Addressing these issues can lead to improved model accuracy and reliability, better feature engineering, and outlier management, all of which contribute to better decision-making and resource optimization. No negative growth is anticipated as these actions are geared towards enhancing data quality and analysis."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "correlation_matrix = data.corr()\n",
        "plt.figure(figsize=(21, 18))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"RdYlGn\")\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OtkZZyU1fiOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "kWQIG9S_eNiJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap is used for visualization to quickly identify relationships between variables. It provides a color-coded matrix where colors represent the strength and direction of correlations. It's a powerful tool for understanding data patterns, identifying key features, and guiding feature selection in machine learning."
      ],
      "metadata": {
        "id": "krK6vZrmeOkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the color of a correlation heatmap indicates a non-linear relationship, it suggests that simple linear models may not capture the underlying data patterns effectively. This insight implies the need for more complex, **non-linear modeling techniques** or feature engineering to better represent and predict the data."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "0BeAeC4fuwwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insight about non-linear relationships in the correlation heatmap is important for improving model accuracy and predictive power. While it may necessitate more complex modeling techniques, this can lead to more accurate predictions and valuable insights. Therefore, the insights are likely to have a positive business impact by enabling better decision-making and resource optimization, rather than leading to negative growth."
      ],
      "metadata": {
        "id": "bArM7jHAuyrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of column names in your dataset\n",
        "columns = data.columns\n",
        "\n",
        "# Determine the number of rows and columns for subplots\n",
        "num_rows = len(columns)\n",
        "num_cols = 1\n",
        "\n",
        "# Create subplots with specified number of rows and columns\n",
        "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 80))\n",
        "\n",
        "# Iterate over each column (excluding \"Appliances\") and create pair plot\n",
        "for i, column in enumerate(columns):\n",
        "    #if column != \"Appliances\":\n",
        "        sns.scatterplot(data=data, x=\"Appliances\", y=column, ax=axes[i])\n",
        "        axes[i].set_xlabel(\"Appliances\")\n",
        "        axes[i].set_ylabel(column)\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pzVpJ9VlZ_qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot is a visual tool that displays pairwise relationships between variables in a dataset. It's beneficial for exploratory data analysis, revealing patterns, correlations, and outliers. Pair plots help understand how variables interact, guiding feature selection and data preprocessing in machine learning. They are especially useful when dealing with multivariate data, identifying potential associations, and making informed decisions for model building and feature engineering."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are  many pairs in a pair plot exhibit heteroscedasticity (unequal variance) between variables, it suggests that the spread or variability of one variable changes with the values of another variable. This can indicate that the relationship between those variables is not constant across the entire range of values.\n",
        "\n",
        "Insights you can gain from observing heteroscedasticity in pair plots:\n",
        "\n",
        "1. **Potential Issues**: Heteroscedasticity can be problematic for regression models, as they often assume homoscedasticity (constant variance). Detecting heteroscedasticity alerts you to the need for potential model adjustments.\n",
        "\n",
        "2. **Modeling Choices**: You might need to consider alternative models that can handle heteroscedastic data better, like robust regression techniques.\n",
        "\n",
        "3. **Feature Engineering**: It may be necessary to transform or engineer the variables to mitigate heteroscedasticity, such as applying logarithmic transformations or normalizing the data.\n",
        "\n",
        "4. **Outliers**: Heteroscedasticity can sometimes be attributed to outliers. Identifying these outliers and understanding their impact on the relationship between variables is essential.\n",
        "\n",
        "5. **Subgroup Analysis**: You might need to analyze subsets of your data where heteroscedasticity is less pronounced to better understand the relationships within those subsets.\n",
        "\n",
        "In summary, observing heteroscedasticity in pair plots alerts you to potential issues in your data and models, prompting you to make informed decisions regarding data preprocessing, modeling choices, and outlier handling."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "5Wkza5GOvRq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from observing heteroscedasticity in pair plots are essential for making data-informed decisions. Addressing heteroscedasticity can lead to more accurate models, improved predictions, and better decision-making. While it may require adjustments in modeling and feature engineering, these actions are likely to have a positive impact on business outcomes, enabling more reliable insights, resource optimization, and better planning. Therefore, there are no insights that lead to negative growth in this context."
      ],
      "metadata": {
        "id": "PXxVsqYqvS0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no significant linear relationship between the independent variables and the appliance energy consumption.\n",
        "\n",
        "Alternative Hypothesis (H1): There is a significant linear relationship between the independent variables and the appliance energy consumption."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "OBBrJOQ5cjCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Extract the two continuous variables you want to test\n",
        "column_to_drop = ['Appliances']\n",
        "independent_variables = data.drop(column_to_drop, axis = 1)\n",
        "dependent_variable = data['Appliances']\n",
        "\n",
        "# Step 2: Perform the Correlation Test (Pearson correlation)\n",
        "correlation_coefficients, p_values = [], []\n",
        "for feature in independent_variables.columns:\n",
        "    correlation_coefficient, p_value = pearsonr(independent_variables[feature], dependent_variable)\n",
        "    correlation_coefficients.append(correlation_coefficient)\n",
        "    p_values.append(p_value)\n",
        "\n",
        "# Step 3: Interpret the Results for each feature\n",
        "alpha = 0.05  # Significance level (commonly set to 0.05)\n",
        "for i, feature in enumerate(independent_variables.columns):\n",
        "    print(f\"Correlation Coefficient for '{feature}': {correlation_coefficients[i]:.4f}\")\n",
        "    print(f\"P-value for '{feature}': {p_values[i]:.4f}\")\n",
        "\n",
        "    if p_values[i] < alpha:\n",
        "        print(\"Result: The correlation is statistically significant (reject H0).\\n\")\n",
        "    else:\n",
        "        print(\"Result: There is no significant correlation (fail to reject H0).\\n\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the practical implementation provided earlier, the statistical test used to obtain the p-value is the Pearson correlation coefficient test. The Pearson correlation coefficient, also known as Pearson's r or simply r, is a measure of the linear relationship between two continuous variables."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The p-value obtained from the test indicates the probability of observing the calculated correlation coefficient (or a more extreme value) if the null hypothesis is true. The null hypothesis (H0) in this context states that there is no significant linear relationship between the two variables.\n",
        "\n",
        "By comparing the p-value to a chosen significance level (alpha), commonly set to 0.05 (5%), we can determine whether to reject or fail to reject the null hypothesis. If the p-value is less than alpha, we reject the null hypothesis, suggesting a statistically significant correlation. If the p-value is greater than alpha, we fail to reject the null hypothesis, indicating no significant correlation.\n",
        "\n",
        "This test is appropriate when you want to assess the strength and direction of the linear relationship between two continuous variables. It is commonly used to explore the association between variables in correlation analysis and is widely used in various fields of research and data analysis."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import missingno as msno\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the null matrix\n",
        "msno.matrix(data)\n",
        "\n",
        "# Customizing the plot\n",
        "plt.title('Null Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Luckily there are no missing value in our dataset. No missing values imputation required**"
      ],
      "metadata": {
        "id": "qOnQ9TPjpJTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "df= data.copy()\n",
        "col_list = list(df.describe().columns)\n",
        "\n",
        "#find the outliers using boxplot\n",
        "plt.figure(figsize=(25, 20))\n",
        "plt.suptitle(\"Box Plot\", fontsize=18, y=0.95)\n",
        "\n",
        "for n, ticker in enumerate(col_list):\n",
        "\n",
        "    ax = plt.subplot(8, 4, n + 1)\n",
        "\n",
        "    plt.subplots_adjust(hspace=0.5, wspace=0.2)\n",
        "\n",
        "    sns.boxplot(x=df[ticker],color='pink', ax = ax)\n",
        "\n",
        "    # chart formatting\n",
        "    ax.set_title(ticker.upper())\n"
      ],
      "metadata": {
        "id": "DGTCoxgkaWIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def find_outliers_iqr(data):\n",
        "    # Calculate the first quartile (Q1) and third quartile (Q3) for each column\n",
        "    q1 = data.quantile(0.25)\n",
        "    q3 = data.quantile(0.75)\n",
        "\n",
        "    # Calculate the interquartile range (IQR) for each column\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    # Calculate the lower and upper bounds for outliers for each column\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "    # Check for outliers in each column and count the number of outliers\n",
        "    outliers_count = (data < lower_bound) | (data > upper_bound)\n",
        "    num_outliers = outliers_count.sum()\n",
        "\n",
        "    return num_outliers\n",
        "\n",
        "\n",
        "outliers_per_column = find_outliers_iqr(data)\n",
        "print(\"Number of outliers per column:\")\n",
        "print(outliers_per_column.sort_values(ascending = False))\n",
        "\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "for ftr in col_list:\n",
        "  print(ftr,'\\n')\n",
        "  q_25= np.percentile(df[ftr], 25)\n",
        "  q_75 = np.percentile(df[ftr], 75)\n",
        "  iqr = q_75 - q_25\n",
        "  print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q_25, q_75, iqr))\n",
        "  # calculate the outlier cutoff\n",
        "  cut_off = iqr * 1.5\n",
        "  lower = q_25 - cut_off\n",
        "  upper = q_75 + cut_off\n",
        "  print(f\"\\nlower = {lower} and upper = {upper} \\n \")\n",
        "  # identify outliers\n",
        "  outliers = [x for x in df[ftr] if x < lower or x > upper]\n",
        "  print('Identified outliers: %d' % len(outliers))\n",
        "  #removing outliers\n",
        "  if len(outliers)!=0:\n",
        "\n",
        "    def bin(row):\n",
        "      if row[ftr]> upper:\n",
        "        return upper\n",
        "      if row[ftr] < lower:\n",
        "        return lower\n",
        "      else:\n",
        "        return row[ftr]\n",
        "\n",
        "\n",
        "\n",
        "    data[ftr] =  df.apply (lambda row: bin(row), axis=1)\n",
        "    print(f\"{ftr} Outliers Removed\")\n",
        "  print(\"\\n-------\\n\")"
      ],
      "metadata": {
        "id": "gpoGcJ-laLK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25, 20))\n",
        "plt.suptitle(\"Box Plot without Outliers\", fontsize=18, y=0.95)\n",
        "#plot the all figures in loop with boxplot\n",
        "for n, ticker in enumerate(col_list):\n",
        "\n",
        "    ax = plt.subplot(8, 4, n + 1)\n",
        "\n",
        "    plt.subplots_adjust(hspace=0.5, wspace=0.2)\n",
        "\n",
        "    sns.boxplot(x=data[ticker],color='g' ,ax = ax)\n",
        "\n",
        "    # chart formatting\n",
        "    ax.set_title(ticker.upper())\n"
      ],
      "metadata": {
        "id": "ZrdPC2Srdk2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the shape after\n",
        "data.shape"
      ],
      "metadata": {
        "id": "iFTzKc6UcU8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# create new features\n",
        "# create a column average building temperature based on all temperature\n",
        "data['Average_building_Temperature']=data[['KITCHEN_TEMP','LIVING_TEMP','BEDROOM_TEMP','OFFICE_TEMP','BATHROOM_TEMP','IRONING_ROOM_TEMP','TEEN_ROOM_2_TEMP','PARENTS_ROOM_TEMP']].mean(axis=1)\n",
        "#create a column of difference between outside and inside temperature\n",
        "data['Temperature_difference']=abs(data['Average_building_Temperature']-data['OUTSIDE_TEMP_build'])\n",
        "\n",
        "#create a column average building humidity\n",
        "data['Average_building_humidity']=data[['KITCHEN_HUM','LIVING_HUM','BEDROOM_HUM', 'OFFICE_HUM','BATHROOM_HUM','IRONING_ROOM_HUM','TEEN_ROOM_HUM','PARENTS_ROOM_HUM']].mean(axis=1)\n",
        "#create a column of difference between outside and inside building humidity\n",
        "data['Humidity_difference']=abs(data['OUTSIDE_HUM_build']-data['Average_building_humidity'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "THq_WDcy0cvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop random variables as it does not look like that much important while predicting the output\n",
        "columns_to_drop = ['rv1','rv2']\n",
        "data.drop(columns_to_drop, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "C_kdVVTSKQRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "WD_vkrWMV8EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finding the skewed and symmetrical data"
      ],
      "metadata": {
        "id": "8N_Na8hJcJcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the skewness in the dataset to check the distribution\n",
        "skewness = data.skew()\n",
        "\n",
        "#ginding the absolute value\n",
        "abs(skewness)\n",
        "\n",
        "# setting up the threshold\n",
        "skewness_threshold = 0.5\n",
        "\n",
        "# Separate features into symmetrical and skewed based on skewness threshold\n",
        "symmetrical_features = skewness[abs(skewness) < skewness_threshold].index\n",
        "skewed_features = skewness[abs(skewness) >= skewness_threshold].index\n",
        "\n",
        "# Create new DataFrames for symmetrical and skewed features\n",
        "print('FEATURES FOLLOWED SYMMETRICAL DISTRIBUTION :')\n",
        "symmetrical_data = data[symmetrical_features]\n",
        "print(symmetrical_features)\n",
        "\n",
        "print('FEATURES FOLLOWED SKEWED DISTRIBUTION :')\n",
        "skewed_data = data[skewed_features]\n",
        "print(skewed_features)\n"
      ],
      "metadata": {
        "id": "1fSKxWkHCi-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the skewed data\n",
        "skewed_data"
      ],
      "metadata": {
        "id": "9f_lt6SWHEGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the liabrary\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# Initialize the PowerTransformer\n",
        "power_transformer = PowerTransformer()\n",
        "\n",
        "# Fit and transform the data using the PowerTransformer\n",
        "power_transformed = pd.DataFrame(power_transformer.fit_transform(skewed_data))\n",
        "power_transformed.columns = skewed_data.columns\n"
      ],
      "metadata": {
        "id": "XYFTzJnZHRA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the power transformed data\n",
        "power_transformed"
      ],
      "metadata": {
        "id": "_lWwTodiIVeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index to the default integer index\n",
        "symmetrical_data.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "wIsZBYKLKSxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the symmetrical data\n",
        "symmetrical_data"
      ],
      "metadata": {
        "id": "FqbmBhCgOSwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate horizontally (along columns)\n",
        "tranformed_data = pd.concat([symmetrical_data, power_transformed], axis=1)"
      ],
      "metadata": {
        "id": "-axrMYS-Ju78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the transformed data\n",
        "tranformed_data"
      ],
      "metadata": {
        "id": "XYxVg0eOJ7De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Yes My data needs transformation specially skewed data , i used power transformaiton to solve this concern"
      ],
      "metadata": {
        "id": "dMFDTrcIcfEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Scaling the DATA set"
      ],
      "metadata": {
        "id": "yxxTpRupBq2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the desired liabrary\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = pd.DataFrame(scaler.fit_transform(tranformed_data))\n",
        "scaled_data.columns = tranformed_data.columns\n",
        "scaled_data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction is a crucial technique in machine learning and data analysis, and it is often needed for several reasons:\n",
        "\n",
        "1. **Curse of Dimensionality:** As the number of features (dimensions) in your dataset increases, the volume of the data space grows exponentially. This can lead to sparsity, making it difficult to collect sufficient data to model relationships effectively. Dimensionality reduction can help mitigate this problem by reducing the number of features while retaining essential information.\n",
        "\n",
        "2. **Overfitting:** Models trained on high-dimensional data are more likely to overfit the training data, meaning they perform well on training data but poorly on unseen data. By reducing dimensionality, you can reduce the complexity of the model and enhance its generalization capabilities.\n",
        "\n",
        "3. **Computational Efficiency:** High-dimensional data can strain computational resources and increase the time required for training and prediction. Dimensionality reduction can make algorithms more efficient.\n",
        "\n",
        "4. **Visualization:** It's challenging to visualize and interpret data in high-dimensional spaces. Reducing dimensionality allows for more accessible data exploration and visualization.\n",
        "\n",
        "5. **Feature Engineering:** Some features may be redundant or irrelevant, and dimensionality reduction helps in identifying and removing them. This can improve model performance and understanding of data.\n",
        "\n",
        "6. **Collinearity:** High-dimensional data often contains multicollinearity, where features are highly correlated. Dimensionality reduction can alleviate this issue and help in extracting meaningful and uncorrelated features.\n",
        "\n",
        "Common techniques for dimensionality reduction include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and feature selection methods. The choice of technique depends on the specific dataset and the goals of the analysis. In summary, dimensionality reduction is needed to simplify high-dimensional data, improve model performance, and enhance the interpretability and efficiency of data analysis."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize a PCA instance without specifying the number of components\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA model to your standardized data\n",
        "pca.fit(scaled_data)\n",
        "\n",
        "# Calculate the cumulative explained variance\n",
        "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Create an elbow plot to visualize the explained variance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Elbow Plot')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Create a PCA instance and specify the number of components you want to retain\n",
        "# For example, if you want to retain 10 components, set n_components=10\n",
        "n_components = 10\n",
        "pca = PCA(n_components=n_components)\n",
        "\n",
        "# Fit the PCA model to your standardized data and transform it\n",
        "transformed_data_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "# The variable 'transformed_data_pca' now contains your data in the reduced-dimensional space with 'n_components' principal components.\n",
        "\n",
        "# You can also access explained variance to see how much variance is explained by each component\n",
        "explained_variance = pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "sHTojg5sUyJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the variances of the pca that we extract and there importance in predicting the output\n",
        "explained_variance"
      ],
      "metadata": {
        "id": "MtWCswByX8B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating the total of  explained_variance  which needs to be more than 90%\n",
        "explained_variance.sum()"
      ],
      "metadata": {
        "id": "dLNiowtirSjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize a PCA instance without specifying the number of components\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA model to your standardized data\n",
        "pca.fit(scaled_data)\n",
        "\n",
        "# Calculate the explained variance for each component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Create a scree plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance')\n",
        "plt.title('Scree Plot for PCA')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "u_lp1LmNxB5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the shape after pca\n",
        "transformed_data_pca.shape"
      ],
      "metadata": {
        "id": "e5IshvswYBBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_data_pca"
      ],
      "metadata": {
        "id": "vJSrc4FGVPKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#assinign the independent and dependent feature\n",
        "x = transformed_data_pca\n",
        "y = data['Appliances']"
      ],
      "metadata": {
        "id": "bNJtj0r0CDnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the data into 80/20 ration\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=3)"
      ],
      "metadata": {
        "id": "as7VuE-HPHsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 - Simple Linear Regression Model"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the mdoel\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#defining the object\n",
        "reg = LinearRegression()\n",
        "reg.fit(x_train, y_train)\n",
        "\n",
        "#training dataset score\n",
        "training_score = reg.score(x_train, y_train)\n",
        "\n",
        "#predicting the value\n",
        "y_pred = reg.predict(x_test)\n",
        "\n",
        "#calculating the training accuracy\n",
        "print(\"Train score:\" ,training_score)\n",
        "\n",
        "#calculating the MSE\n",
        "MSE  = mean_squared_error((y_test),(y_pred))\n",
        "print(\"Test MSE :\" , MSE)\n",
        "\n",
        "#calculating the testing accuracy\n",
        "r2 = r2_score((y_test),(y_pred))\n",
        "print(\"Test R2 :\" ,r2)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot to compare the predicted values against the actual values.\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3UPe56CAxrZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "o8NLZeV1a2uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model used is a Linear Regression model, a simple and interpretable machine learning algorithm. Linear Regression is used for predicting a continuous target variable based on one or more input features. It models the relationship between the input features and the target variable as a linear equation.\n",
        "\n",
        "Performance Evaluation:\n",
        "1. **Train Score (R-squared)**: The train score of approximately 0.679 indicates that the model explains about 67.9% of the variance in the training data. A higher R-squared is generally better, and this suggests that the model captures a significant portion of the variation in the data.\n",
        "\n",
        "2. **Test Mean Squared Error (MSE)**: The test MSE of approximately 568.88 measures the average squared difference between predicted and actual values. Lower MSE is desirable, and this value suggests that, on average, the model's predictions have a moderate error in the test data.\n",
        "\n",
        "3. **Test R-squared (R2)**: The test R2 score of about 0.684 implies that the model accounts for roughly 68.4% of the variance in the test data. A higher R2 score indicates a better fit to the test data.\n",
        "\n",
        "In summary, the Linear Regression model has a moderate level of predictive power. It explains a significant portion of the variance in both the training and test data. The model's performance is reasonable, but there may still be room for improvement in reducing the mean squared error for more accurate predictions. Further evaluation with additional metrics and potentially exploring more complex models may be considered for fine-tuning."
      ],
      "metadata": {
        "id": "jiC0wGYOed97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "\n",
        "# Create a Linear Regression model (you can replace this with any other regression model)\n",
        "model = LinearRegression()\n",
        "\n",
        "# Define hyperparameter search space (you can customize this based on your model)\n",
        "param_dist = {'fit_intercept': [True, False],\n",
        "              'copy_X': [True, False],\n",
        "              'positive':[True, False]}\n",
        "\n",
        "# Perform RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,\n",
        "                                   n_iter=10, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the RandomizedSearchCV to find the best hyperparameters\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Train the best model with the entire training dataset\n",
        "best_model.fit(x_train, y_train)\n",
        "\n",
        "training_score_val = best_model.score(x_train, y_train)\n",
        "# Evaluate the best model on the test set\n",
        "test_predictions = best_model.predict(x_test)\n",
        "\n",
        "# Calculate evaluation metrics for the test predictions (e.g., mean squared error)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(y_test, test_predictions)\n",
        "r2 = r2_score((y_test),(test_predictions))\n",
        "\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "\n",
        "#visual of training score\n",
        "print(\"Train score:\" ,training_score_val)\n",
        "print(\"Test MSE:\", mse)\n",
        "print(\"Test R2:\", r2)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.displot(test_predictions - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "SkDXF8syg0iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zUw-OVnIgpoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used in this case is \"RandomizedSearchCV.\"\n",
        "\n",
        "RandomizedSearchCV is chosen for several reasons:\n",
        "1. **Efficiency**: Compared to GridSearchCV, RandomizedSearchCV explores a random subset of hyperparameter combinations, making it more efficient when there are a large number of possible hyperparameter settings.\n",
        "\n",
        "2. **Exploration**: It provides a balance between random exploration and an exhaustive search. It randomly samples hyperparameters within specified ranges, which can be beneficial in discovering hidden, effective configurations.\n",
        "\n",
        "3. **Parallelization**: It allows parallel processing, using multiple CPU cores (specified by `n_jobs=-1`), which speeds up the search process.\n",
        "\n",
        "4. **Scoring**: The choice of 'neg_mean_squared_error' as the scoring metric indicates that the search aims to minimize the mean squared error, a common metric for regression tasks.\n",
        "\n",
        "RandomizedSearchCV efficiently explores a range of hyperparameter settings, leading to the discovery of a set of hyperparameters that perform well. In this case, it led to a model with the same training score as the initial model, indicating that the optimal hyperparameters did not significantly improve the model's performance. However, it's a valuable technique to systematically search for optimal hyperparameters and can lead to substantial performance improvements in other cases."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon performing hyperparameter optimization using RandomizedSearchCV, there doesn't seem to be a significant improvement in model performance compared to the initial model. The training score remains the same, and the evaluation metrics on the test data also show similar values. The Mean Squared Error (MSE) and R-squared (R2) values remain approximately unchanged. This suggests that the initial model's hyperparameters were already reasonably effective, and the hyperparameter search did not lead to noticeable enhancements in this particular case. Further exploration or considering different models may be necessary to achieve substantial improvements."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "4DGpWFhQja7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each evaluation metric provides valuable insights into the model's performance and its potential impact on the business:\n",
        "\n",
        "1. **Training Score (R-squared)**:\n",
        "   - **Indication**: R-squared measures the proportion of variance in the target variable explained by the model. A high R-squared indicates a good fit to the training data.\n",
        "   - **Business Implication**: A high training R-squared suggests that the model captures a significant portion of the variance in the training data. This can be beneficial for understanding the relationships between input features and the target variable within the business context. However, a very high R-squared could indicate overfitting, which may not generalize well to new data.\n",
        "\n",
        "2. **Test Mean Squared Error (MSE)**:\n",
        "   - **Indication**: MSE quantifies the average squared difference between the model's predictions and actual values in the test data. Lower MSE values are desirable.\n",
        "   - **Business Implication**: A lower test MSE indicates that the model's predictions are closer to the actual values on average. This suggests that the model's predictions have a smaller error, making it more reliable for business applications. Reduced errors can lead to cost savings, improved decision-making, and better resource allocation.\n",
        "\n",
        "3. **Test R-squared (R2)**:\n",
        "   - **Indication**: R2 on the test data measures how well the model explains the variance in new, unseen data. A higher R2 indicates better predictive power.\n",
        "   - **Business Implication**: A high test R2 suggests that the model generalizes well to new data, making it valuable for making predictions in a business context. It signifies that the model maintains its predictive performance beyond the training data, which can lead to more accurate forecasting, better resource planning, and improved business outcomes.\n",
        "\n",
        "In summary, while the model shows good performance on the training data, indicating a strong understanding of relationships within that dataset, it's equally important to evaluate its performance on test data. The low test MSE and high test R2 suggest that the model is reasonably accurate and generalizes well. The business impact includes improved decision-making, better resource allocation, and potentially cost savings, as the model's predictions align closely with the actual values, benefiting various business applications. However, continued monitoring and potential model refinement may be necessary to ensure long-term business success."
      ],
      "metadata": {
        "id": "OrGhlayXjnJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - Polynomial Regression model\n"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already split your data into x_train, x_test, y_train, and y_test\n",
        "\n",
        "# Choose the degree of the polynomial (e.g., 2 for quadratic)\n",
        "degree = 2\n",
        "\n",
        "# Create a Polynomial Regression model using a pipeline\n",
        "polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "\n",
        "# Fit the model to the training data\n",
        "polyreg.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = polyreg.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Calculate the R2 score for the training data\n",
        "training_r2 = polyreg.score(x_train, y_train)\n",
        "\n",
        "print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared (R2) Score: {r2:.2f}\")"
      ],
      "metadata": {
        "id": "ymH6IB2GjXg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "kmERsQaXYTc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xUrAeYExZ_Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "WVTu0r-3tWqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used in this scenario is a Polynomial Regression model with a degree of 2. Polynomial Regression is a type of linear regression where polynomial features are generated from the original features to capture more complex relationships between the independent and dependent variables.\n",
        "\n",
        "Performance Evaluation:\n",
        "\n",
        "1. **Training R-squared (R2) Score:** The training R2 score of approximately 0.68 indicates that the model explains about 68% of the variance in the training data, suggesting a moderate fit.\n",
        "\n",
        "2. **Mean Squared Error (MSE):** The MSE of approximately 568.88 represents the average squared difference between the predicted and actual values. Lower MSE values are better, indicating that the model's predictions are relatively close to the actual values.\n",
        "\n",
        "3. **Test R-squared (R2) Score:** The test R2 score of approximately 0.68 is consistent with the training R2 score, suggesting that the model generalizes well to unseen data.\n",
        "\n",
        "In summary, the Polynomial Regression model performs moderately well, explaining a significant portion of the variance in both the training and test datasets. The low MSE indicates relatively accurate predictions. The consistent training and test R2 scores suggest that the model is not overfitting and can make reliable predictions on new data."
      ],
      "metadata": {
        "id": "zxinTGqUtbuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "KcEF3md1kE4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "\n",
        "# Create a Polynomial Regression model without specifying the degree\n",
        "polyreg = make_pipeline(PolynomialFeatures(), LinearRegression())\n",
        "\n",
        "# Define a range of polynomial degrees to be tested\n",
        "param_grid = {'polynomialfeatures__degree': range(1, 3)}\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(polyreg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best polynomial degree\n",
        "best_degree = grid_search.best_params_['polynomialfeatures__degree']\n",
        "\n",
        "# Create a Polynomial Regression model with the best degree\n",
        "best_polyreg = make_pipeline(PolynomialFeatures(degree=best_degree), LinearRegression())\n",
        "\n",
        "# Perform cross-validation to evaluate the model\n",
        "cv_scores = cross_val_score(best_polyreg, x_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "cv_r2_scores = cross_val_score(best_polyreg, x_train, y_train, cv=5, scoring='r2')\n",
        "\n",
        "# Calculate the mean squared error and R2 score\n",
        "mse_cv = -cv_scores.mean()\n",
        "r2_cv = cv_r2_scores.mean()\n",
        "\n",
        "# Fit the best model to the training data\n",
        "best_polyreg.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = best_polyreg.predict(x_test)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Polynomial Degree: {best_degree}\")\n",
        "print(f\"Cross-Validation Mean Squared Error: {mse_cv:.2f}\")\n",
        "print(f\"Cross-Validation R-squared (R2) Score: {r2_cv:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared (R2) Score: {r2:.2f}\")"
      ],
      "metadata": {
        "id": "dyEaycxPXV0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "tb8NDSy7jq3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KT0g7g0si6jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "7Kmb0xk8jvH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we used the GridSearchCV technique for hyperparameter optimization. Here's why it was chosen:\n",
        "\n",
        "GridSearchCV systematically searches through a predefined hyperparameter grid, in this case, the polynomial degree of a Polynomial Regression model, to find the best combination that minimizes the chosen scoring metric (in this case, negative mean squared error). This technique was chosen for several reasons:\n",
        "\n",
        "1. **Exhaustive Search:** GridSearchCV explores all possible combinations of hyperparameters within the specified grid, ensuring that no promising settings are missed. This is particularly valuable when you don't have prior knowledge about the best hyperparameter values.\n",
        "\n",
        "2. **Automated Hyperparameter Tuning:** It automates the process of hyperparameter tuning, saving time and reducing the risk of manual errors.\n",
        "\n",
        "3. **Cross-Validation:** GridSearchCV uses cross-validation to estimate the model's performance with different hyperparameter settings, providing a more reliable assessment of how the model is likely to perform on unseen data.\n",
        "\n",
        "4. **Objective Optimization:** It optimizes hyperparameters based on a specified scoring metric, which allows you to tailor the model to achieve your specific goals. In this case, it minimizes mean squared error, which is a common choice for regression problems.\n",
        "\n",
        "5. **Ease of Use:** GridSearchCV is easy to implement and integrate into your machine learning workflow using Scikit-Learn.\n",
        "\n",
        "In summary, GridSearchCV is a systematic and effective technique for finding the best hyperparameters for your model, ensuring that it performs well on both the training data and unseen data. It's widely used in machine learning because of its simplicity and robustness in hyperparameter optimization."
      ],
      "metadata": {
        "id": "VdZDbferjx5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this scenario, you applied cross-validation to a Polynomial Regression model with a degree of 2, and here's the comparison between the original model and the cross-validated model:\n",
        "\n",
        "Original Model:\n",
        "- Training R-squared (R2) Score: 0.78\n",
        "- Mean Squared Error: 404.51\n",
        "- Test R-squared (R2) Score: 0.78\n",
        "\n",
        "Cross-Validated Model:\n",
        "- Cross-Validation Mean Squared Error: 415.34\n",
        "- Cross-Validation R-squared (R2) Score: 0.78\n",
        "- Mean Squared Error: 404.51\n",
        "- Test R-squared (R2) Score: 0.78\n",
        "\n",
        "It appears that the cross-validated model has a slightly higher Cross-Validation Mean Squared Error (415.34) compared to the original model's Mean Squared Error (404.51). However, the R-squared (R2) scores for both the original model and the cross-validated model are the same (0.78).\n",
        "\n",
        "In this case, the cross-validated model doesn't show a significant improvement in terms of Cross-Validation Mean Squared Error or R-squared (R2) score compared to the original model. The values are quite similar. Cross-validation is typically used to provide a more reliable estimate of a model's performance and assess its generalization to unseen data. In this instance, it confirms that the original model's performance is consistent across different folds of the data."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "i2pN9ajttoKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's analyze each of the evaluation metrics in the context of a business problem and discuss their potential business impact based on the Polynomial Regression model with a degree of 2:\n",
        "\n",
        "1. **Training R-squared (R2) Score (0.78):**\n",
        "   - **Indication:** R-squared measures the proportion of the variance in the dependent variable that is explained by the independent variables. An R2 score of 0.78 indicates that approximately 78% of the variance in the target variable is accounted for by the model.\n",
        "   - **Business Impact:** This high R-squared score suggests that the model captures a significant portion of the underlying patterns in the data. The business impact is that the model is relatively effective in explaining and predicting the target variable, which can be valuable for decision-making and forecasting.\n",
        "\n",
        "2. **Mean Squared Error (MSE) (404.51):**\n",
        "   - **Indication:** MSE quantifies the average squared difference between predicted and actual values. A lower MSE is desirable as it indicates that the model's predictions are closer to the actual values.\n",
        "   - **Business Impact:** An MSE of 404.51 means that, on average, the model's predictions have a squared error of this value. Lower MSE implies that the model is making more accurate predictions, which can lead to cost savings, better resource allocation, and improved decision-making in various business applications.\n",
        "\n",
        "3. **R-squared (R2) Score (0.78):**\n",
        "   - **Indication:** The R-squared score on the test data confirms the model's ability to explain the variance in unseen data, similar to the training R2.\n",
        "   - **Business Impact:** Consistency between training and test R2 scores (0.78) indicates that the model generalizes well and is not overfitting. This means that the model can make reliable predictions on new, unseen data, which is crucial for making informed business decisions.\n",
        "\n",
        "In summary, the evaluated Polynomial Regression model (degree = 2) exhibits strong performance metrics, suggesting that it effectively captures the underlying relationships in the data. The business impact of this model includes the ability to make accurate predictions, explain variance in the target variable, and generalize well to new data. This can lead to improved decision-making, better resource allocation, and potentially cost savings in various business applications, such as sales forecasting, risk assessment, or quality control."
      ],
      "metadata": {
        "id": "PYDbdWHWtpt2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 - RIDGE Regression Model"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already created the 'x_train', 'x_test', 'y_train', and 'y_test' datasets\n",
        "# 'x_train' and 'x_test' are the results of polynomial regression on PCA-transformed data\n",
        "\n",
        "# Create a PolynomialFeatures instance (with degree=2 for quadratic features)\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Transform the data to include polynomial features\n",
        "x_train_poly = poly_features.fit_transform(x_train)\n",
        "x_test_poly = poly_features.transform(x_test)\n",
        "\n",
        "# Create a Ridge regression model\n",
        "ridge_reg = Ridge(alpha=1.0)  # You can adjust the alpha parameter (regularization strength)\n",
        "\n",
        "# Fit the Ridge model to the training data\n",
        "ridge_reg.fit(x_train_poly, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = ridge_reg.predict(x_test_poly)\n",
        "\n",
        "# Calculate R-squared (R2) for the test data\n",
        "test_r2 = ridge_reg.score(x_test_poly, y_test)\n",
        "\n",
        "# Calculate R-squared (R2) for the training data\n",
        "training_r2 = ridge_reg.score(x_train_poly, y_train)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for the test data\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(f\"Test R-squared (R2) Score: {test_r2:.2f}\")\n",
        "print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")"
      ],
      "metadata": {
        "id": "gxks1j-natPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "JlupRzxBAIqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V0hHNR6qAQp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TRu5wzPguD_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used is Ridge Regression with polynomial features (degree=2). Here's the explanation and performance summary based on the provided evaluation metrics:\n",
        "\n",
        "**Model Explanation:**\n",
        "Ridge Regression is a linear regression technique that includes L2 regularization to prevent overfitting. It's combined with Polynomial Features, which generates new features by considering interactions between variables. The degree=2 indicates quadratic features.\n",
        "\n",
        "**Performance Evaluation:**\n",
        "1. **Test R-squared (R2) Score: 0.78**\n",
        "   - The R2 score measures the proportion of variance in the target variable explained by the model.\n",
        "   - A score of 0.78 indicates that approximately 78% of the variance is accounted for, suggesting the model's effectiveness in explaining test data.\n",
        "\n",
        "2. **Training R-squared (R2) Score: 0.78**\n",
        "   - Consistency between training and test R2 scores (0.78) suggests that the model generalizes well to new data and is not overfitting.\n",
        "\n",
        "3. **Mean Squared Error (MSE): 404.51**\n",
        "   - MSE quantifies the average squared difference between predicted and actual values.\n",
        "   - A lower MSE (404.51) implies accurate predictions, with predictions on average being close to actual values.\n",
        "\n",
        "**Model Performance:**\n",
        "- The model exhibits strong predictive capabilities, explaining 78% of the variance in the test data.\n",
        "- Consistency between training and test R2 scores indicates reliable generalization.\n",
        "- The low MSE demonstrates accurate predictions, which can lead to improved decision-making and resource allocation.\n",
        "\n",
        "Overall, this Ridge Regression model with polynomial features (degree=2) performs well and can be valuable in applications like predictive modeling, where understanding relationships between variables is crucial for informed decision-making."
      ],
      "metadata": {
        "id": "FzWbYwX0uH3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n"
      ],
      "metadata": {
        "id": "aSkQDwTWxvud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have 'x' and 'y' as your data and target variable\n",
        "\n",
        "# Create a PolynomialFeatures instance (with degree=3 for cubic features)\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Create a Ridge regression model\n",
        "ridge_reg = Ridge()\n",
        "\n",
        "# Create a pipeline with the polynomial features and Ridge regression\n",
        "pipeline = Pipeline([\n",
        "    ('polynomial_features', poly_features),\n",
        "    ('ridge_regression', ridge_reg)\n",
        "])\n",
        "\n",
        "# Define hyperparameters and values to search\n",
        "param_grid = {\n",
        "    'ridge_regression__alpha': [0.001, 0.01, 0.1, 1]  # You can adjust the alpha values\n",
        "}\n",
        "\n",
        "# Perform Grid Search with Cross-Validation\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(x, y)  # Use the full dataset for cross-validation\n",
        "\n",
        "# Get the best hyperparameters from the grid search\n",
        "best_alpha = grid_search.best_params_['ridge_regression__alpha']\n",
        "\n",
        "# Create a Ridge regression model with the best hyperparameters\n",
        "best_ridge_reg = Ridge(alpha=best_alpha)\n",
        "\n",
        "# Fit the Ridge model to the training data\n",
        "best_ridge_reg.fit(x_train, y_train)\n",
        "\n",
        "# Calculate cross-validated R-squared (R2) scores\n",
        "cv_scores = cross_val_score(best_ridge_reg, x_train, y_train, cv=5, scoring='r2')\n",
        "\n",
        "# Calculate R-squared (R2) score on the test data\n",
        "test_r2 = best_ridge_reg.score(x_test, y_test)\n",
        "\n",
        "print(f\"Best Alpha: {best_alpha}\")\n",
        "print(f\"Cross-Validated R-squared (R2) Scores: {cv_scores}\")\n",
        "print(f\"Mean R-squared (R2) Score: {np.mean(cv_scores):.2f}\")\n",
        "print(f\"Training R-squared (R2) Score: {best_ridge_reg.score(x_train, y_train):.2f}\")\n",
        "print(f\"Test R-squared (R2) Score: {test_r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "sxNXg6XccnYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "z-qh-ma1p6yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dF-mi86ygzrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Which hyperparameter optimization technique have you used and why?\n"
      ],
      "metadata": {
        "id": "YKyOrusqoWV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the **GridSearchCV** technique for hyperparameter optimization. Here's why it was chosen:\n",
        "\n",
        "1. **GridSearchCV Systematic Search:** GridSearchCV exhaustively explores a predefined hyperparameter grid, systematically testing all possible combinations of hyperparameters. In this case, it optimizes the alpha hyperparameter for Ridge regression.\n",
        "\n",
        "2. **Cross-Validation:** GridSearchCV employs cross-validation to estimate a model's performance with different hyperparameter settings. This provides a more reliable assessment of how the model is likely to perform on unseen data, helping to avoid overfitting.\n",
        "\n",
        "3. **Objective Optimization:** GridSearchCV optimizes hyperparameters based on a specified scoring metric (in this case, negative mean squared error), allowing you to fine-tune the model for specific objectives.\n",
        "\n",
        "4. **Automation:** GridSearchCV automates the process of hyperparameter tuning, saving time and reducing the risk of manual errors. It's a convenient and widely-used tool in machine learning workflows.\n",
        "\n",
        "In summary, GridSearchCV was chosen for its systematic search, cross-validation, and objective optimization capabilities. It helps find the best hyperparameters for the Ridge regression model, ensuring optimal performance and generalization to unseen data."
      ],
      "metadata": {
        "id": "i78QWwwWoewU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "GJgYOywluOR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two models being compared are Polynomial Ridge Regression (degree=2) and Ridge Regression with different hyperparameters. Let's analyze the improvement:\n",
        "\n",
        "**Polynomial Ridge Regression (degree=2):**\n",
        "- **Cross-Validated R-squared (R2) Scores**: The model achieves a mean R2 score of 0.68 across cross-validation folds. This indicates its ability to explain about 68% of the variance in the data.\n",
        "- **Training R-squared (R2) Score**: The consistency between training and cross-validation R2 scores (both 0.68) suggests the model generalizes well.\n",
        "- **Test R-squared (R2) Score**: The test R2 score is also 0.68, indicating the model's capability to predict unseen data effectively.\n",
        "- **Cross-Validation Mean Squared Error (415.34)**: The model exhibits a reasonable performance with an average MSE, signifying the average squared difference between predictions and actual values.\n",
        "- **R-squared (R2) Score (0.78)**: This score of 0.78 in the original model showcases a higher ability to explain variance, suggesting a relatively strong model.\n",
        "\n",
        "**Comparison:**\n",
        "- The Polynomial Ridge model maintains consistent R2 scores between training, cross-validation, and test datasets, indicating good generalization.\n",
        "- However, the original model with Ridge Regression, without polynomial features, exhibits a higher cross-validation R2 score (0.78), indicating better explanation of variance. This might be due to its flexibility in capturing complex relationships with the right hyperparameters.\n",
        "\n",
        "**Summary:**\n",
        "- The Polynomial Ridge model offers decent performance, but it doesn't surpass the original Ridge Regression model in terms of cross-validated R2 scores.\n",
        "- The original model, with careful hyperparameter tuning, shows a better ability to explain variance and generalizes well to unseen data.\n",
        "- Therefore, the original Ridge Regression model, with appropriate hyperparameters, appears to be the stronger choice for this particular task.\n",
        "\n"
      ],
      "metadata": {
        "id": "rOe2CVTauPe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "-JOW0zRat3TE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The significance of each evaluation metric in the context of a business problem and the potential business impact of the Ridge Regression model with Polynomial Features (degree=2):\n",
        "\n",
        "1. **Cross-Validated R-squared (R2) Scores (0.68)**:\n",
        "   - **Indication:** R-squared measures the proportion of variance in the target variable explained by the model. A score of 0.68 suggests that the model can explain about 68% of the variance in the data.\n",
        "   - **Business Impact:** This level of explanation of variance can be valuable for businesses in decision-making. It means the model captures a significant portion of the underlying patterns, aiding in understanding and forecasting.\n",
        "\n",
        "2. **Cross-Validation Mean Squared Error (415.34)**:\n",
        "   - **Indication:** MSE quantifies the average squared difference between predicted and actual values. Lower MSE is desirable as it indicates accurate predictions.\n",
        "   - **Business Impact:** The model's ability to achieve a lower MSE (415.34) implies accurate predictions, which can lead to cost savings, resource allocation efficiency, and reduced errors in various business processes.\n",
        "\n",
        "3. **R-squared (R2) Score (0.78)**:\n",
        "   - **Indication:** The R2 score on the original model indicates that approximately 78% of the variance in the target variable is explained. It represents a strong level of explanation.\n",
        "   - **Business Impact:** A high R2 score means the model provides valuable insights for businesses. It aids in making informed decisions, optimizing operations, and forecasting outcomes.\n",
        "\n",
        "**Overall Business Impact:**\n",
        "- The Ridge Regression model with Polynomial Features (degree=2) demonstrates its value in business applications by effectively explaining variance and providing accurate predictions.\n",
        "- The model's ability to generalize well ensures it can perform reliably on unseen data, a crucial aspect in practical business scenarios.\n",
        "- By utilizing this model, businesses can make data-driven decisions, improve resource allocation, reduce costs, and enhance their overall efficiency and effectiveness.\n",
        "\n",
        "In summary, the Ridge Regression model offers businesses the potential for improved decision-making, more accurate forecasting, and better utilization of resources, ultimately leading to enhanced operational performance and competitiveness.\n"
      ],
      "metadata": {
        "id": "ZSR2IjzGt4sL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4 - Lasso Regression Model"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already created the 'x_train', 'x_test', 'y_train', and 'y_test' datasets\n",
        "# 'x_train' and 'x_test' are the results of polynomial regression on PCA-transformed data\n",
        "\n",
        "# Create a PolynomialFeatures instance (with degree=3 for cubic features)\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Transform the data to include polynomial features\n",
        "x_train_poly = poly_features.fit_transform(x_train)\n",
        "x_test_poly = poly_features.transform(x_test)\n",
        "\n",
        "# Create a Lasso regression model\n",
        "lasso_reg = Lasso(alpha=1.0)  # You can adjust the alpha parameter (regularization strength)\n",
        "\n",
        "# Fit the Lasso model to the training data\n",
        "lasso_reg.fit(x_train_poly, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = lasso_reg.predict(x_test_poly)\n",
        "\n",
        "# Calculate R-squared (R2) for the test data\n",
        "test_r2 = lasso_reg.score(x_test_poly, y_test)\n",
        "\n",
        "# Calculate R-squared (R2) for the training data\n",
        "training_r2 = lasso_reg.score(x_train_poly, y_train)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for the test data\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"Test R-squared (R2) Score: {test_r2:.2f}\")\n",
        "print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "vGUJFicTB4Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t724bczkg1YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Tbrj6Z5UHvJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The machine learning model used in this scenario is a Polynomial Lasso Regression model. Here's an explanation of the model and its performance using evaluation metric scores:\n",
        "\n",
        "**Model Explanation**:\n",
        "- **Polynomial Lasso Regression**: This model combines Polynomial Regression and Lasso (L1 regularization) to predict a target variable based on polynomial combinations of the input features. Polynomial features allow the model to capture complex relationships between variables, while Lasso regression adds a regularization term to prevent overfitting.\n",
        "\n",
        "**Performance Evaluation**:\n",
        "- **Test R-squared (R2) Score: 0.75**:\n",
        "  - This metric represents the proportion of the variance in the target variable (y) that can be explained by the model. An R-squared score of 0.75 indicates that the model accounts for 75% of the variance in the test data. In business terms, this means that the model provides a reasonable fit to the test data, capturing a significant portion of the underlying relationships.\n",
        "\n",
        "- **Training R-squared (R2) Score: 0.76**:\n",
        "  - Similar to the test R-squared score, the training R-squared measures how well the model fits the training data. It's slightly higher (0.76) than the test R-squared, which indicates that the model may be slightly overfitting the training data.\n",
        "\n",
        "- **Mean Squared Error (MSE): 447.74**:\n",
        "  - The MSE quantifies the average squared difference between the predicted and actual values. In this case, an MSE of 447.74 indicates the average prediction error. A lower MSE is generally preferred, but the interpretation depends on the specific context of the business.\n",
        "\n",
        "**Business Impact**:\n",
        "- A test R-squared score of 0.75 indicates that the model is capable of explaining a significant portion of the variance in the test data. This means that it can make reasonably accurate predictions for the business, which can be valuable for tasks such as demand forecasting, pricing strategies, or risk assessment.\n",
        "\n",
        "- The training R-squared of 0.76 suggests that the model has good explanatory power on the training data. However, further model tuning might be required to balance the trade-off between fit and overfitting.\n",
        "\n",
        "- The MSE value (447.74) represents the magnitude of prediction errors. The business impact of this depends on the context. For example, in finance, this error may be acceptable for some applications, while in precision engineering, it may need improvement.\n",
        "\n",
        "In summary, the Polynomial Lasso Regression model provides a reasonably good fit to the data, and its performance should be assessed in the specific business context to determine its suitability and potential impact."
      ],
      "metadata": {
        "id": "_WfirAn8HwZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "FV4bbI3rBx1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have 'x' and 'y' as your data and target variable\n",
        "\n",
        "# Split the data into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=3)\n",
        "\n",
        "# Create a PolynomialFeatures instance (with degree=3 for cubic features)\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Transform the data to include polynomial features\n",
        "x_train_poly = poly_features.fit_transform(x_train)\n",
        "x_test_poly = poly_features.transform(x_test)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "x_train_poly = scaler.fit_transform(x_train_poly)\n",
        "x_test_poly = scaler.transform(x_test_poly)\n",
        "\n",
        "# Create a Lasso regression model\n",
        "lasso_reg = Lasso(max_iter=10000)  # Increase max_iter and adjust the alpha parameter if needed\n",
        "\n",
        "# Define hyperparameters and values to search\n",
        "param_grid = {\n",
        "    'alpha': [0.001, 0.01, 0.1, 1, 10]  # You can adjust the alpha values\n",
        "}\n",
        "\n",
        "# Perform Grid Search with Cross-Validation\n",
        "grid_search = GridSearchCV(lasso_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(x_train_poly, y_train)  # Use the training data for cross-validation\n",
        "\n",
        "# Get the best hyperparameters from the grid search\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "\n",
        "# Create a Lasso regression model with the best hyperparameters\n",
        "best_lasso_reg = Lasso(alpha=best_alpha, max_iter=10000)\n",
        "\n",
        "# Fit the Lasso model to the training data\n",
        "best_lasso_reg.fit(x_train_poly, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = best_lasso_reg.predict(x_test_poly)\n",
        "\n",
        "# Calculate R-squared (R2) for the test data\n",
        "test_r2 = best_lasso_reg.score(x_test_poly, y_test)\n",
        "\n",
        "# Calculate R-squared (R2) for the training data\n",
        "training_r2 = best_lasso_reg.score(x_train_poly, y_train)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for the test data\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for the training data\n",
        "training_mse = mean_squared_error(y_train, best_lasso_reg.predict(x_train_poly))\n",
        "\n",
        "# Calculate cross-validated R-squared (R2) scores\n",
        "cv_scores = cross_val_score(best_lasso_reg, x, y, cv=5, scoring='r2')\n",
        "\n",
        "print(f\"Best Alpha: {best_alpha}\")\n",
        "print(f\"Test R-squared (R2) Score: {test_r2:.2f}\")\n",
        "print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Training Mean Squared Error (MSE): {training_mse:.2f}\")\n",
        "print(f\"Cross-Validated R-squared (R2) Scores: {cv_scores}\")\n",
        "print(f\"Mean R-squared (R2) Score: {np.mean(cv_scores):.2f}\")"
      ],
      "metadata": {
        "id": "HefHlXLpIh0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "j_DT8rtIB_KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hdY1YGc2g2uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "pgZGPjukCDkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the provided code, the hyperparameter optimization technique used is Grid Search with Cross-Validation (GridSearchCV). Here's an explanation of why this technique is chosen:\n",
        "\n",
        "**Grid Search with Cross-Validation (GridSearchCV)**:\n",
        "- GridSearchCV is a systematic approach to hyperparameter tuning that searches for the best combination of hyperparameters from a predefined grid of values.\n",
        "- The chosen hyperparameter to optimize in this case is the \"alpha\" value for Lasso regression. Alpha controls the strength of L1 regularization, which affects feature selection and model complexity.\n",
        "- The grid includes different values of alpha to test, ranging from 0.001 to 10. Grid search explores these alpha values systematically to find the best one.\n",
        "- Cross-validation is used to assess model performance with different alpha values. A 5-fold cross-validation is performed, where the training data is divided into five subsets, and the model is trained and evaluated multiple times.\n",
        "- GridSearchCV selects the alpha value that yields the best performance in terms of a scoring metric (neg_mean_squared_error in this case).\n",
        "\n",
        "**Why Grid Search with Cross-Validation**:\n",
        "- GridSearchCV is a widely used hyperparameter optimization technique because it ensures a robust evaluation of hyperparameters by cross-validation, preventing overfitting.\n",
        "- It systematically searches through a range of hyperparameters, making it comprehensive and methodical.\n",
        "- It provides an automated and efficient way to tune hyperparameters without manual trial and error.\n",
        "- The selected scoring metric 'neg_mean_squared_error' is used to measure the model's prediction error, and the best hyperparameters are chosen based on minimizing this error.\n",
        "\n",
        "Overall, Grid Search with Cross-Validation is chosen to find the optimal alpha value for Lasso regression in a systematic and data-driven way, improving the model's ability to generalize to new data. It's a common practice in machine learning to ensure the model's robustness and performance."
      ],
      "metadata": {
        "id": "8ginfcseCEaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "IWkQP2VfCFHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there is a noticeable improvement in model performance after hyperparameter tuning using Grid Search with Cross-Validation. Here's a summary of the improvements:\n",
        "\n",
        "**After Hyperparameter Tuning**:\n",
        "- Best Alpha: 0.001 (a small value)\n",
        "- Test R-squared (R2) Score: 0.78 (up from 0.75)\n",
        "- Training R-squared (R2) Score: 0.78 (no significant change)\n",
        "- Mean Squared Error (MSE): 401.03 (down from 447.74)\n",
        "- Training Mean Squared Error (MSE): 414.06 (down from a higher value)\n",
        "- Cross-Validated R-squared (R2) Scores: An array of values averaging to 0.65\n",
        "\n",
        "**Before Hyperparameter Tuning**:\n",
        "- Test R-squared (R2) Score: 0.75\n",
        "- Training R-squared (R2) Score: 0.76\n",
        "- Mean Squared Error (MSE): 447.74\n",
        "\n",
        "The key improvements are in the test R-squared (R2) score and the mean squared error (MSE). The test R-squared score increased from 0.75 to 0.78, indicating a better fit of the model to the test data. The MSE decreased from 447.74 to 401.03, suggesting that the model's predictions are closer to the actual values, resulting in lower prediction errors.\n",
        "\n",
        "The improvements demonstrate that the model's predictive performance is enhanced after hyperparameter tuning. It is now better at explaining the variance in the test data, making it a more effective model for making predictions. The cross-validated R-squared scores also indicate more consistent model performance across different subsets of the data."
      ],
      "metadata": {
        "id": "MS3r_EO3YoAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "2WKputEICGu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, here's an explanation of each evaluation metric and its indication towards business, as well as the potential business impact of the ML model:\n",
        "\n",
        "1. **Test R-squared (R2) Score**:\n",
        "   - **Indication**: R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables. A higher R2 score (closer to 1) indicates that a larger proportion of the variance in the target variable is explained by the model.\n",
        "   - **Business Impact**: A high R2 score suggests that the model is effective in explaining and predicting business outcomes. It means that the model is capable of making accurate predictions, which can help in strategic decision-making. For example, in sales forecasting, a high R2 score indicates that the model can provide reliable sales predictions, leading to better inventory management and resource allocation.\n",
        "\n",
        "2. **Training R-squared (R2) Score**:\n",
        "   - **Indication**: Similar to the test R2 score, the training R2 score measures how well the model fits the training data. It indicates the goodness of fit between the model and the training data.\n",
        "   - **Business Impact**: While a high training R2 score is desirable, it should be considered alongside the test R2 score. If the training R2 is significantly higher than the test R2, it might indicate overfitting. Overfitting can lead to poor generalization on unseen data, potentially impacting business decisions.\n",
        "\n",
        "3. **Mean Squared Error (MSE)**:\n",
        "   - **Indication**: MSE quantifies the average squared difference between predicted values and actual values. Lower MSE values indicate smaller prediction errors.\n",
        "   - **Business Impact**: Lower MSE implies that the model's predictions are closer to the actual values. This can result in cost savings and increased efficiency. For instance, in manufacturing, a model with low MSE can help minimize defects and resource wastage.\n",
        "\n",
        "4. **Training Mean Squared Error (MSE)**:\n",
        "   - **Indication**: Similar to test MSE, training MSE measures the average squared difference between predicted and actual values but on the training data.\n",
        "   - **Business Impact**: Training MSE is a useful diagnostic metric. If it is significantly lower than test MSE, it may signal overfitting. Identifying and addressing overfitting can lead to a more reliable model for business applications.\n",
        "\n",
        "5. **Cross-Validated R-squared (R2) Scores**:\n",
        "   - **Indication**: Cross-validation R2 scores measure the model's performance on multiple subsets of the data to assess its robustness and generalization.\n",
        "   - **Business Impact**: High and consistent cross-validated R2 scores indicate that the model's performance is stable across different data partitions. This means the model is reliable and can be trusted for business decisions.\n",
        "\n",
        "In summary, a good ML model should have high test R2 scores, low MSE values, and stable cross-validated R2 scores. These metrics indicate the model's ability to make accurate predictions, which can lead to better business decisions, cost savings, and increased efficiency. However, it's crucial to balance model performance on the test and training data to avoid overfitting."
      ],
      "metadata": {
        "id": "aVMr4s3eCHd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5 - elastic net Regression Model"
      ],
      "metadata": {
        "id": "SAu0wU95I9lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import  ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
        "\n",
        "# Specify the degree of polynomial (you can change this based on your data)\n",
        "degree = 2\n",
        "\n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=degree)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "X_test_poly = poly_features.transform(X_test)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "ElasticNet_model = ElasticNet(alpha=1.0)\n",
        "\n",
        "# Train the model using the polynomial features\n",
        "ElasticNet_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = ElasticNet_model.predict(X_train_poly)\n",
        "test_predictions = ElasticNet_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)"
      ],
      "metadata": {
        "id": "A7aJUHSjJEc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "p8bCsNqFCoMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1LjuNiUfg39K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "FCZ-Wy25Hz_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used is Polynomial Regression with Elastic Net regularization. Here's an explanation of the model and its performance using evaluation metric score charts:\n",
        "\n",
        "**Polynomial Regression (Degree 2):**\n",
        "\n",
        "- **Model Explanation**: Polynomial regression extends linear regression by introducing polynomial features of the original data. The degree parameter (in this case, degree=2) indicates that quadratic polynomial features are added to the dataset. Elastic Net is used as the regularization technique to prevent overfitting and provide a balance between L1 (Lasso) and L2 (Ridge) regularization.\n",
        "\n",
        "**Model Performance:**\n",
        "\n",
        "- **Train MSE (Mean Squared Error)**: The training MSE measures the average squared difference between the model's predictions and the actual target values on the training data. In this case, the training MSE is approximately 575.42.\n",
        "\n",
        "- **Test MSE (Mean Squared Error)**: The test MSE quantifies the average squared difference between the model's predictions and the actual target values on the test data. Here, the test MSE is approximately 584.88.\n",
        "\n",
        "- **Train R-squared (R2)**: The training R2 score is a measure of how well the model fits the training data. It indicates the proportion of the variance in the target variable that is explained by the model. In this case, the training R2 is approximately 0.69.\n",
        "\n",
        "- **Test R-squared (R2)**: The test R2 score assesses the model's performance on unseen data, indicating how well it generalizes. It's similar to the training R2 but for the test data. Here, the test R2 is approximately 0.69.\n",
        "\n",
        "**Performance Evaluation:**\n",
        "\n",
        "- The training and test MSE values are relatively close, indicating that the model's predictions on unseen data (test MSE) are consistent with its performance on the training data. This suggests that the model generalizes well.\n",
        "\n",
        "- The R2 scores, both for training and testing, are around 0.69. This implies that the model can explain approximately 69% of the variance in the target variable. While not exceptionally high, it indicates a reasonable level of predictive capability.\n",
        "\n",
        "- The degree of 2 suggests that quadratic polynomial features are introduced, potentially capturing non-linear relationships between features and the target variable.\n",
        "\n",
        "**Business Impact:**\n",
        "\n",
        "- Polynomial regression, with its ability to capture non-linear relationships, can be beneficial in situations where simple linear models fall short. For instance, in finance, it can help predict stock prices based on multiple factors.\n",
        "\n",
        "- The test MSE indicates that the model's predictions are relatively close to the actual values. In business scenarios, this accuracy can be crucial. For example, in retail, it can be used to forecast demand for products accurately.\n",
        "\n",
        "- The elastic net regularization helps prevent overfitting, making the model more robust for real-world applications.\n",
        "\n",
        "- However, the model's predictive performance can be further improved by adjusting the degree of the polynomial features and fine-tuning the regularization parameters.\n",
        "\n",
        "In summary, the Polynomial Regression model with Elastic Net regularization shows potential for accurate predictions, particularly when non-linear relationships are present in the data. Fine-tuning hyperparameters could further enhance its performance in practical business applications."
      ],
      "metadata": {
        "id": "JBumnRdzH1OQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "Tyd3H1_2C6Nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create a Ridge Regression model\n",
        "ElasticNet_model = ElasticNet()\n",
        "\n",
        "# Perform Cross-Validation and Hyperparameter Tuning\n",
        "param_grid = {'alpha': [0.1, 1.0, 10.0]}  # Define the hyperparameter grid\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=ElasticNet_model, param_grid=param_grid,\n",
        "                           scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV to find the best degree and alpha\n",
        "grid_search.fit(X_train_poly, y_train)\n",
        "\n",
        "# Get the best degree and alpha from the GridSearchCV results\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = best_model.predict(X_train_poly)\n",
        "test_predictions = best_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Best Alpha:\", best_alpha)\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)"
      ],
      "metadata": {
        "id": "X57hdecTOHvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "mg0xbnKJC-x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IViw6ojTg48L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "ZxM-XvGaDFSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used is GridSearchCV. GridSearchCV systematically searches for the best combination of hyperparameters from a predefined grid of hyperparameter values. Here's why GridSearchCV was chosen:\n",
        "\n",
        "**Reasons for Using GridSearchCV:**\n",
        "\n",
        "1. **Comprehensive Search:** GridSearchCV exhaustively searches through all possible combinations of hyperparameters within the specified grid. This is important when you want to ensure that no optimal combination is missed.\n",
        "\n",
        "2. **Scoring Function:** GridSearchCV uses a scoring function, which is set to 'neg_mean_squared_error' in this case. It helps identify the combination of hyperparameters that minimizes the mean squared error (MSE) most effectively. In regression tasks, MSE is a common evaluation metric used to measure prediction accuracy.\n",
        "\n",
        "3. **Cross-Validation:** GridSearchCV integrates cross-validation (in this case, 5-fold cross-validation) to assess how well the model performs on different subsets of the training data. This helps ensure that the selected hyperparameters generalize well to unseen data.\n",
        "\n",
        "4. **Best Model Selection:** Once the search is complete, GridSearchCV identifies the best combination of hyperparameters and the associated model. This best model is then used for making predictions and evaluating performance.\n",
        "\n",
        "5. **Ease of Use:** GridSearchCV is easy to use and a convenient tool for finding optimal hyperparameters without manually testing each combination.\n",
        "\n",
        "In summary, GridSearchCV is chosen as it provides a rigorous, automated, and efficient approach to hyperparameter tuning, allowing you to find the best set of hyperparameters for your ElasticNet model while incorporating cross-validation for robust performance assessment."
      ],
      "metadata": {
        "id": "UGZLq4sSDGyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "CFKCCe3HDI4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there is a noticeable improvement in the model's performance after applying cross-validation and hyperparameter tuning. Here's a summary of the changes:\n",
        "\n",
        "**Before Cross-Validation and Hyperparameter Tuning:**\n",
        "- Train MSE: 575.42\n",
        "- Test MSE: 584.88\n",
        "- Train R-squared: 0.6871\n",
        "- Test R-squared: 0.6856\n",
        "\n",
        "**After Cross-Validation and Hyperparameter Tuning:**\n",
        "- Best Alpha: 0.1\n",
        "- Train MSE: 412.46\n",
        "- Test MSE: 418.45\n",
        "- Train R-squared: 0.7757\n",
        "- Test R-squared: 0.7751\n",
        "\n",
        "**Improvements:**\n",
        "1. **MSE Reduction:** The mean squared error (MSE) for both the training and test datasets has decreased. A lower MSE indicates that the model's predictions are closer to the actual values, which is a sign of improved accuracy.\n",
        "\n",
        "2. **R-squared Improvement:** The R-squared (R2) values for both training and test data have increased. R2 measures how well the model explains the variance in the data. Higher R2 values indicate that a larger proportion of the variance in the data is captured by the model.\n",
        "\n",
        "3. **Optimal Alpha:** The best alpha parameter, identified through cross-validation, helps regularize the model effectively. In this case, an alpha value of 0.1 was selected.\n",
        "\n",
        "Overall, the model's performance has improved, as indicated by lower MSE and higher R-squared values for both training and test datasets. The cross-validation and hyperparameter tuning process has led to a more accurate and better-fitting ElasticNet model."
      ],
      "metadata": {
        "id": "7h8OkPT9DL50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "x06JEx1rDOR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure, let's explain the evaluation metrics and their indications towards the business impact of the ElasticNet model:\n",
        "\n",
        "1. **Mean Squared Error (MSE):**\n",
        "   - **Indication:** MSE measures the average squared difference between predicted and actual values. A lower MSE indicates that the model's predictions are closer to the actual values.\n",
        "   - **Business Impact:** A lower MSE suggests that the model is making more accurate predictions, which can be valuable for businesses. It leads to reduced errors in forecasting and decision-making, potentially resulting in cost savings and better resource allocation.\n",
        "\n",
        "2. **R-squared (R2) Score:**\n",
        "   - **Indication:** R2 measures the proportion of the variance in the dependent variable (target) that is explained by the independent variables (features). Higher R2 values indicate that a larger proportion of the variance is captured by the model.\n",
        "   - **Business Impact:** A higher R2 suggests that the model provides a better fit to the data. In a business context, this means that more variability in the target variable is explained by the features, leading to improved understanding of factors affecting outcomes. This can aid in better decision-making, product development, and customer satisfaction.\n",
        "\n",
        "3. **Best Alpha (Hyperparameter):**\n",
        "   - **Indication:** Alpha is a hyperparameter used for regularization. The best alpha value is determined through cross-validation and represents the optimal trade-off between model complexity and accuracy.\n",
        "   - **Business Impact:** Selecting the best alpha helps in controlling overfitting and underfitting, ensuring the model's generalization to unseen data. It leads to a model that is well-tailored to the business problem, minimizing the risk of overcomplicating the model or missing important relationships in the data.\n",
        "\n",
        "The business impact of these improvements is significant. A more accurate model with a lower MSE and higher R2 leads to more reliable predictions and insights, which can be leveraged in various business scenarios. For instance, businesses can make better decisions, reduce costs, improve product recommendations, optimize marketing campaigns, and enhance customer satisfaction. Moreover, the optimal hyperparameters ensure that the model is efficient and cost-effective, reducing the risk of unnecessary complexities and errors."
      ],
      "metadata": {
        "id": "DLBIqZe3DP38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 6 - Ranfom Forest Regressor"
      ],
      "metadata": {
        "id": "iPocHRDIgkRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Create a Random Forest Regressor model\n",
        "rf_model = RandomForestRegressor(n_estimators=20, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions_rf = rf_model.predict(X_train_poly)\n",
        "test_predictions_rf = rf_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse_rf = mean_squared_error(y_train, train_predictions_rf)\n",
        "test_mse_rf = mean_squared_error(y_test, test_predictions_rf)\n",
        "\n",
        "train_r2_rf = r2_score(y_train, train_predictions_rf)\n",
        "test_r2_rf = r2_score(y_test, test_predictions_rf)\n",
        "\n",
        "print(\"Random Forest Regressor:\")\n",
        "print(\"Train MSE:\", train_mse_rf)\n",
        "print(\"Test MSE:\", test_mse_rf)\n",
        "print(\"Train R-squared:\", train_r2_rf)\n",
        "print(\"Test R-squared:\", test_r2_rf)"
      ],
      "metadata": {
        "id": "CvU_uznThPU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_rf - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "BQakyycMEBL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions_rf)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YFoh7Fe4g54R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "e9JNG_aFH4P0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, let's explain the Random Forest Regressor model and its performance using evaluation metric score chart:\n",
        "\n",
        "**Random Forest Regressor:**\n",
        "- **Model Description:** The Random Forest Regressor is an ensemble machine learning model that combines multiple decision trees to make predictions. In regression tasks, it averages the predictions from individual trees to generate the final output. This ensemble approach helps improve prediction accuracy and reduces overfitting.\n",
        "\n",
        "**Performance Evaluation:**\n",
        "\n",
        "1. **Train Mean Squared Error (MSE):**\n",
        "   - **Indication:** A low training MSE (26.36) signifies that the model fits the training data very well. It accurately predicts the target variable for data it has seen during training.\n",
        "   - **Business Impact:** In a business context, a low training MSE is highly favorable. It means the model can make precise predictions for known data, which is valuable for applications like inventory optimization, where accurate forecasts are critical to reduce costs.\n",
        "\n",
        "2. **Test Mean Squared Error (MSE):**\n",
        "   - **Indication:** The test MSE (136.52) is slightly higher than the training MSE, indicating some degree of overfitting. Nevertheless, the model's predictions on the test data are reasonably accurate.\n",
        "   - **Business Impact:** Although there is overfitting, it is not severe. The model still provides good predictions on unseen data. The business impact depends on the specific use case, but in scenarios like predicting housing prices, this level of accuracy could be beneficial.\n",
        "\n",
        "3. **Train R-squared (R2) Score:**\n",
        "   - **Indication:** A high training R-squared score (0.986) suggests that the model effectively explains a significant portion of the variance in the training data. It captures the variation in the target variable well.\n",
        "   - **Business Impact:** A high training R-squared score is advantageous for business applications where understanding and interpreting the factors driving certain outcomes are essential. For instance, in sales forecasting, knowing the drivers of sales can inform strategy decisions.\n",
        "\n",
        "4. **Test R-squared (R2) Score:**\n",
        "   - **Indication:** The test R-squared score (0.927) reflects the model's ability to explain variance in the test data. It indicates that the model generalizes its predictive power well to new, unseen data.\n",
        "   - **Business Impact:** The high test R-squared score is significant for business applications. It demonstrates that the model's predictions are likely to be reliable for future decision-making. This is crucial in applications such as customer churn prediction for retention strategies.\n",
        "\n",
        "**Overall Business Implications:**\n",
        "The Random Forest Regressor exhibits outstanding predictive performance, with relatively minor overfitting. Its low training MSE, high R-squared scores, and excellent test performance make it a valuable asset for various business applications. The specific business impact depends on the use case, but it's well-suited for tasks requiring precise predictions and insights into data patterns. Further model refinement can be explored to mitigate overfitting, if necessary."
      ],
      "metadata": {
        "id": "XQ8oo4OSH5Z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "dvH1l01QEPfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Create a Random Forest Regressor model\n",
        "rf_model = RandomForestRegressor(n_estimators=20, random_state=42)\n",
        "\n",
        "# Define scoring functions\n",
        "scoring = {\n",
        "    'mse': make_scorer(mean_squared_error),\n",
        "    'r2': make_scorer(r2_score)}\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "test_predictions_rf = rf_model.predict(X_test_poly)\n",
        "\n",
        "# Calculate Test MSE and Test R-squared\n",
        "test_mse_rf = mean_squared_error(y_test, test_predictions_rf)\n",
        "test_r2_rf = r2_score(y_test, test_predictions_rf)\n",
        "\n",
        "# Perform cross-validation\n",
        "k = 5  # Number of folds (you can adjust this as needed)\n",
        "mse_scores = -cross_val_score(rf_model, X_train_poly, y_train, cv=k, scoring=scoring['mse'])\n",
        "r2_scores = cross_val_score(rf_model, X_train_poly, y_train, cv=k, scoring=scoring['r2'])\n",
        "\n",
        "# Calculate the mean and standard deviation of MSE and R-squared\n",
        "mean_mse = np.mean(mse_scores)\n",
        "mean_r2 = np.mean(r2_scores)\n",
        "\n",
        "# Print the cross-validation results\n",
        "print(\"Cross-Validation Results for Random Forest Regressor:\")\n",
        "print(f\"Train MSE: {mean_mse:.2f} \")\n",
        "print(f\"Train R-squared: {mean_r2:.2f} \")\n",
        "print(f\"Test MSE: {test_mse_rf:.2f}\")\n",
        "print(f\"Test R-squared: {test_r2_rf:.2f}\")\n"
      ],
      "metadata": {
        "id": "CElotrAA91r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_rf - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "KvAFuKIuoTQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions_rf)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IvQHTP0zg7IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "NpHoaRqNEZvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we uses cross-validation with a Random Forest Regressor to assess model performance, and it also calculates mean squared error (MSE) and R-squared (R2) scores. However, it does not perform hyperparameter optimization within this code snippet.\n",
        "\n",
        "In cross-validation, the data is divided into multiple subsets (folds), and the model is trained and evaluated multiple times, providing a more robust assessment of its performance. While this technique helps ensure that the model's performance is consistent across different data splits, it doesn't optimize hyperparameters. Hyperparameter optimization typically involves adjusting model parameters to achieve the best possible performance.\n",
        "\n",
        "To perform hyperparameter optimization, you would need to use techniques like Grid Search or Randomized Search in combination with cross-validation. These techniques involve systematically varying hyperparameters and evaluating the model's performance to find the best set of hyperparameters for your specific task.\n",
        "\n",
        "If you have already performed hyperparameter optimization separately and selected the best hyperparameters, you can apply them to your Random Forest Regressor model for improved performance. However, the code snippet you provided does not include hyperparameter tuning."
      ],
      "metadata": {
        "id": "2pIFBm-GEZqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ucueqBQuEZa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there is a notable improvement when comparing the Random Forest Regressor's performance metrics before and after applying cross-validation:\n",
        "\n",
        "Before Cross-Validation:\n",
        "- Train MSE: 26.36\n",
        "- Test MSE: 136.52\n",
        "- Train R-squared: 0.986\n",
        "- Test R-squared: 0.927\n",
        "\n",
        "After Cross-Validation:\n",
        "- Train MSE: -179.65\n",
        "- Train R-squared: 0.900\n",
        "- Test MSE: 136.52\n",
        "- Test R-squared: 0.930\n",
        "\n",
        "It's essential to note that the Train MSE before cross-validation was 26.36, which suggests that the model might have been overfitting the training data. However, after applying cross-validation, the Train MSE significantly decreased to -179.65, indicating an improvement in the model's generalization to unseen data. The Test R-squared score also increased from 0.927 to 0.930, indicating better predictive performance on the test dataset. This suggests that cross-validation helped improve the model's ability to generalize and perform well on unseen data."
      ],
      "metadata": {
        "id": "v8LXqxCOEZL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "cH7-EUflEY2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, let's explain the significance of each evaluation metric in the context of a business and its impact on the business using the Random Forest Regressor as an example:\n",
        "\n",
        "1. **Mean Squared Error (MSE)**:\n",
        "   - **Business Significance**: MSE measures the average squared difference between the predicted and actual values. In a business context, it quantifies the model's accuracy, particularly with respect to prediction errors.\n",
        "   - **Business Impact**: Lower MSE values are preferred as they indicate that the model's predictions are closer to the actual values. A lower MSE translates to more accurate predictions, which can lead to cost savings and better decision-making. For instance, in finance, lower MSE can help in better risk assessment and investment decisions.\n",
        "\n",
        "2. **R-squared (R2) Score**:\n",
        "   - **Business Significance**: R-squared measures the proportion of variance in the dependent variable that is explained by the independent variables. It assesses the model's goodness of fit.\n",
        "   - **Business Impact**: A higher R-squared score indicates that a larger portion of the variance in the target variable is explained by the model. In business, a high R-squared implies that the model is capturing the underlying relationships well. This can lead to more reliable forecasts, which are crucial for resource allocation and strategic planning.\n",
        "\n",
        "3. **Cross-Validation Results**:\n",
        "   - **Business Significance**: Cross-validation provides an estimate of a model's performance on unseen data. It helps assess the model's ability to generalize.\n",
        "   - **Business Impact**: The improvement in cross-validation metrics, such as Train MSE and Test R-squared, suggests better generalization and reduced overfitting. For businesses, this means that the model will perform consistently well when used with new data, resulting in more reliable decision-making and potentially higher profitability.\n",
        "\n",
        "In the case of the Random Forest Regressor example, the improved cross-validation results (specifically, a decrease in Train MSE and an increase in Test R-squared) indicate that the model generalizes better. This means it will perform more consistently when making predictions on new data, which is crucial for decision-making in a business context. It can lead to cost savings, optimized processes, and improved overall business performance."
      ],
      "metadata": {
        "id": "Vtk-F3vYEYdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 7 - GRADIENT BOOSTING"
      ],
      "metadata": {
        "id": "cv3Xato39JSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Create a Gradient Boosting Regressor model\n",
        "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gb_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions_gb = gb_model.predict(X_train_poly)\n",
        "test_predictions_gb = gb_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse_gb = mean_squared_error(y_train, train_predictions_gb)\n",
        "test_mse_gb = mean_squared_error(y_test, test_predictions_gb)\n",
        "\n",
        "train_r2_gb = r2_score(y_train, train_predictions_gb)\n",
        "test_r2_gb = r2_score(y_test, test_predictions_gb)\n",
        "\n",
        "print(\"Gradient Boosting Regressor:\")\n",
        "print(\"Train MSE:\", train_mse_gb)\n",
        "print(\"Test MSE:\", test_mse_gb)\n",
        "print(\"Train R-squared:\", train_r2_gb)\n",
        "print(\"Test R-squared:\", test_r2_gb)"
      ],
      "metadata": {
        "id": "EhZ-3mgo84eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_gb - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "qord_ocNoMbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions_gb)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Eq7bV1jOhELY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "V97fnq-UH9TM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model used in this case is the Gradient Boosting Regressor, a powerful ensemble machine learning technique for regression tasks. It combines multiple decision trees to create a strong predictive model. Here's an explanation of its performance using evaluation metrics:\n",
        "\n",
        "**Model Used:** Gradient Boosting Regressor\n",
        "- **Description:** Gradient Boosting Regressor is an ensemble learning method that builds an additive model in a forward stage-wise manner. It fits a sequence of weak learners (usually decision trees) and adapts them to correct the errors made by its predecessors. This results in a strong predictive model.\n",
        "\n",
        "**Performance Evaluation Using Evaluation Metric Score Chart:**\n",
        "\n",
        "- **Train Mean Squared Error (MSE):** The training Mean Squared Error represents the average squared difference between the actual and predicted values for the training dataset. In this case, it's 335.82. A lower MSE indicates that the model is better at fitting the training data.\n",
        "\n",
        "- **Test Mean Squared Error (MSE):** The test Mean Squared Error measures how well the model generalizes to unseen data. Here, it's 382.84. It's slightly higher than the training MSE, indicating some degree of overfitting, but the difference is not very large.\n",
        "\n",
        "- **Train R-squared (R2) Score:** The training R-squared score quantifies the proportion of variance in the target variable explained by the model. It's 0.817, indicating that the model explains about 81.7% of the variance in the training data.\n",
        "\n",
        "- **Test R-squared (R2) Score:** The test R-squared score reflects the model's ability to explain the variance in the test data. Here, it's 0.794, which means the model accounts for about 79.4% of the variance in the test set.\n",
        "\n",
        "**Interpretation:**\n",
        "\n",
        "- The model demonstrates decent predictive performance, as indicated by the R-squared scores. It's capturing a substantial portion of the variance in both the training and test datasets, suggesting that it's learning meaningful patterns from the data.\n",
        "\n",
        "- The test MSE is slightly higher than the training MSE, indicating some overfitting, but the performance on the test data is still reasonable.\n",
        "\n",
        "- The model could potentially benefit from fine-tuning its hyperparameters to further improve performance or reduce overfitting.\n",
        "\n",
        "Overall, the Gradient Boosting Regressor is a promising model for this regression task, but additional optimization and fine-tuning might be explored to further enhance its predictive capabilities."
      ],
      "metadata": {
        "id": "u1po2DT1H-fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "c3p3tc8oE6hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # We can use this code snippet for cross validation\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.ensemble import GradientBoostingRegressor\n",
        "# import numpy as np\n",
        "\n",
        "# # Create a Gradient Boosting Regressor\n",
        "# gb_model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# # Define a parameter grid for hyperparameter tuning\n",
        "# param_grid = {\n",
        "#     'n_estimators': [50, 100],\n",
        "#     'learning_rate': [0.01, 0.1],\n",
        "#     'max_depth': [3, 4]\n",
        "# }\n",
        "\n",
        "# # Create a GridSearchCV object with 5-fold cross-validation\n",
        "# grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# # Fit the GridSearchCV object on your data\n",
        "# grid_search.fit(X_train_poly, y_train)\n",
        "\n",
        "# # Get the best model from the search\n",
        "# best_gb_model = grid_search.best_estimator_\n",
        "\n",
        "# # Make predictions using the best model\n",
        "# train_predictions_gb = best_gb_model.predict(X_train_poly)\n",
        "# test_predictions_gb = best_gb_model.predict(X_test_poly)\n",
        "\n",
        "# # Evaluate the best model\n",
        "# train_mse_gb = mean_squared_error(y_train, train_predictions_gb)\n",
        "# test_mse_gb = mean_squared_error(y_test, test_predictions_gb)\n",
        "# train_r2_gb = r2_score(y_train, train_predictions_gb)\n",
        "# test_r2_gb = r2_score(y_test, test_predictions_gb)\n",
        "\n",
        "# print(\"Best Gradient Boosting Regressor after hyperparameter tuning:\")\n",
        "# print(\"Train MSE:\", train_mse_gb)\n",
        "# print(\"Test MSE:\", test_mse_gb)\n",
        "# print(\"Train R-squared:\", train_r2_gb)\n",
        "# print(\"Test R-squared:\", test_r2_gb)\n",
        "\n",
        "# # Print the best hyperparameters\n",
        "# print(\"Best Hyperparameters:\", grid_search.best_params_)"
      ],
      "metadata": {
        "id": "6wlmVFoxcPaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_gb - y_test,kind ='kde')'''"
      ],
      "metadata": {
        "id": "R-7t3GSxE9UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions_gb)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "x-aY_9nxE8mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "Rn3fbW0SHC6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet provided uses Grid Search with Cross-Validation as the hyperparameter optimization technique. Here's an explanation of why Grid Search with Cross-Validation was used:\n",
        "\n",
        "**Grid Search with Cross-Validation:**\n",
        "- **Technique Used:** Grid Search is a hyperparameter optimization technique that systematically explores a predefined set of hyperparameters to find the combination that produces the best model performance. Cross-validation is a validation strategy that helps assess a model's generalizability.\n",
        "- **Reason for Using Grid Search:** Grid Search is chosen because it offers a structured and exhaustive search of hyperparameters. It explores all possible combinations specified in the parameter grid. Cross-Validation is used to evaluate how well the model performs on unseen data, helping to identify the best hyperparameters that generalize well.\n",
        "- **Benefits:** Grid Search with Cross-Validation helps ensure that the selected hyperparameters are robust and not overfit to the training data. It prevents manual tuning, which can be time-consuming and biased. The combination of Grid Search and Cross-Validation provides a comprehensive approach to hyperparameter tuning, resulting in a well-optimized model.\n",
        "\n",
        "In summary, Grid Search with Cross-Validation is selected as it offers a systematic and data-driven approach to optimize hyperparameters, leading to a more reliable and well-performing machine learning model."
      ],
      "metadata": {
        "id": "fXAsogwHHCtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "E9vB6NbnHCge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"In the interest of project efficiency and to ensure timely progress, I have chosen to temporarily suspend the execution of certain models that were experiencing prolonged delays during the hyperparameter tuning and cross-validation process. While these models are valuable and relevant, their current runtime has exceeded 30 minutes, which is not aligned with the project's timeline. I will revisit these models at a later stage to derive their results when computational resources permit.\""
      ],
      "metadata": {
        "id": "ONGrH8hcHCTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "S69i9WjGHCHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, let's explain the business implications of each evaluation metric in the context of the Gradient Boosting Regressor model:\n",
        "\n",
        "1. **Train Mean Squared Error (MSE):**\n",
        "   - **Indication:** A low training MSE (335.82) suggests that the model fits the training data well, meaning it accurately predicts the target variable for data it has seen during training.\n",
        "   - **Business Impact:** In a business context, a low training MSE is a positive sign, as it implies that the model can make accurate predictions for known data. This can be useful for tasks like demand forecasting, where precise predictions are essential for inventory management and cost optimization.\n",
        "\n",
        "2. **Test Mean Squared Error (MSE):**\n",
        "   - **Indication:** The test MSE (382.84) is slightly higher than the training MSE, indicating that the model might experience a degree of overfitting. Overfitting occurs when the model is too tailored to the training data, and this can lead to less accurate predictions on new, unseen data.\n",
        "   - **Business Impact:** While the test MSE is higher, it's not drastically so. The business impact would depend on the specific application. If the business can tolerate a slight decrease in prediction accuracy in exchange for the model's ability to capture more complex patterns, the impact may be minimal. However, in scenarios where prediction accuracy is critical (e.g., financial risk assessment), further model refinement might be necessary.\n",
        "\n",
        "3. **Train R-squared (R2) Score:**\n",
        "   - **Indication:** A high training R-squared score (0.817) indicates that the model accounts for a substantial proportion of the variance in the training data. In other words, it explains a significant part of the variation in the target variable.\n",
        "   - **Business Impact:** For a business, a high training R-squared score is beneficial, as it means the model is effective at explaining why certain outcomes occur. This can be valuable in scenarios such as customer churn prediction, where understanding the factors contributing to churn is essential for retention strategies.\n",
        "\n",
        "4. **Test R-squared (R2) Score:**\n",
        "   - **Indication:** The test R-squared score (0.794) reflects the model's ability to explain variance in the test data. While slightly lower than the training R-squared, it still suggests a good fit to the data.\n",
        "   - **Business Impact:** The test R-squared score is important for business because it signifies the model's ability to generalize its predictive power to new, unseen data. In applications like pricing optimization, a high test R-squared score is advantageous, as it indicates that the model's predictions are likely to be reliable for future pricing decisions.\n",
        "\n",
        "**Overall Business Implications:**\n",
        "The Gradient Boosting Regressor demonstrates reasonable predictive performance with good explanations of variance in both training and test datasets. While there is slight overfitting, it is not severe. The model's positive indicators, such as low training MSE and high R-squared scores, suggest its potential business value in tasks requiring accurate predictions and insights into underlying data patterns. Further model tuning can be explored to mitigate overfitting and enhance performance. The specific business impact depends on the particular use case and the trade-offs between accuracy and model complexity."
      ],
      "metadata": {
        "id": "ZW-18VKBHB60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 8 - XGBOOST"
      ],
      "metadata": {
        "id": "54RxJBLH9Gq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Create an XGBoost Regressor model\n",
        "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions_xgb = xgb_model.predict(X_train_poly)\n",
        "test_predictions_xgb = xgb_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse_xgb = mean_squared_error(y_train, train_predictions_xgb)\n",
        "test_mse_xgb = mean_squared_error(y_test, test_predictions_xgb)\n",
        "\n",
        "train_r2_xgb = r2_score(y_train, train_predictions_xgb)\n",
        "test_r2_xgb = r2_score(y_test, test_predictions_xgb)\n",
        "\n",
        "print(\"XGBoost Regressor:\")\n",
        "print(\"Train MSE:\", train_mse_xgb)\n",
        "print(\"Test MSE:\", test_mse_xgb)\n",
        "print(\"Train R-squared:\", train_r2_xgb)\n",
        "print(\"Test R-squared:\", test_r2_xgb)"
      ],
      "metadata": {
        "id": "C2xGYM3n9DdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_xgb - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "v9VM8ddonWJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2.\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions_xgb)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xCVT4J1jpBJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JrnlmOb1IA1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The provided code demonstrates the use of an XGBoost Regressor for a regression task. Here's an explanation of the model and its performance using evaluation metrics:\n",
        "\n",
        "**XGBoost Regressor:**\n",
        "- XGBoost is an ensemble learning technique based on gradient boosting, specifically designed for regression and classification tasks.\n",
        "- It builds a decision tree ensemble to make predictions by minimizing the loss function, which is typically a squared error for regression problems.\n",
        "\n",
        "**Performance Evaluation:**\n",
        "- **Train Mean Squared Error (MSE): 86.46:** MSE quantifies the average squared difference between predicted and actual values in the training data. An MSE of 86.46 indicates the average prediction error in the training set.\n",
        "\n",
        "- **Test Mean Squared Error (MSE): 166.14:** Similarly, the test MSE quantifies the average squared difference between predicted and actual values in the test data. A test MSE of 166.14 indicates the average prediction error in the test set.\n",
        "\n",
        "- **Train R-squared (R2) Score: 0.953:** The training R2 score measures the proportion of the variance in the target variable explained by the model. An R2 score of 0.953 suggests that the XGBoost model accounts for approximately 95.3% of the variance in the training data.\n",
        "\n",
        "- **Test R-squared (R2) Score: 0.911:** The test R2 score indicates the proportion of the variance in the test data explained by the model. An R2 score of 0.911 suggests that the model accounts for approximately 91.1% of the variance in the test data.\n",
        "\n",
        "**Evaluation Metric Score Chart:**\n",
        "- The model's training R2 score of 0.953 indicates that it captures a substantial portion of the variance in the training data. This is a positive sign for business applications.\n",
        "\n",
        "- The test R2 score of 0.911 suggests that the model generalizes well to unseen data, as it also explains a significant proportion of the variance in the test data.\n",
        "\n",
        "- The training and test MSE values of 86.46 and 166.14, respectively, represent the model's average prediction error. While these errors may not be negligible, they are within an acceptable range depending on the specific application.\n",
        "\n",
        "**Business Implications:**\n",
        "- The high R2 scores for both training and test data suggest that the XGBoost model is effective in capturing underlying patterns in the data, making it valuable for businesses seeking predictive insights.\n",
        "\n",
        "- The model's ability to generalize well to new data (as indicated by the test R2 score) is crucial for practical business applications.\n",
        "\n",
        "- The moderate test MSE implies that the model's predictions may have some level of error, which businesses should consider when making decisions based on these predictions.\n",
        "\n",
        "Overall, the XGBoost Regressor demonstrates strong performance in explaining variance and generalizing to new data, making it a valuable tool for various business applications. It may be beneficial to further fine-tune the model and explore feature engineering to potentially reduce prediction errors."
      ],
      "metadata": {
        "id": "xb9Pnwm1IB0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "uLwJvLC9FU2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100],  # You can specify other values\n",
        "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate\n",
        "    # Add more hyperparameters as needed\n",
        "}\n",
        "\n",
        "# Create an XGBoost Regressor model\n",
        "xgb_model = xgb.XGBRegressor(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X_train_poly, y_train)\n",
        "\n",
        "# Get the best hyperparameters from the grid search\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Create an XGBoost Regressor model with the best hyperparameters\n",
        "best_xgb_model = xgb.XGBRegressor(**best_params, random_state=42)\n",
        "\n",
        "# Fit the best XGBoost model to the training data\n",
        "best_xgb_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions_xgb = best_xgb_model.predict(X_train_poly)\n",
        "test_predictions_xgb = best_xgb_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse_xgb = mean_squared_error(y_train, train_predictions_xgb)\n",
        "test_mse_xgb = mean_squared_error(y_test, test_predictions_xgb)\n",
        "\n",
        "train_r2_xgb = r2_score(y_train, train_predictions_xgb)\n",
        "test_r2_xgb = r2_score(y_test, test_predictions_xgb)\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"XGBoost Regressor:\")\n",
        "print(\"Train MSE:\", train_mse_xgb)\n",
        "print(\"Test MSE:\", test_mse_xgb)\n",
        "print(\"Train R-squared:\", train_r2_xgb)\n",
        "print(\"Test R-squared:\", test_r2_xgb)\n"
      ],
      "metadata": {
        "id": "DoayzF-vBb9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_xgb - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "CG0tYrDfFbcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions_xgb)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qwJG_laSFhJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "tBvoDNGfGERC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used in the provided code is Grid Search with Cross-Validation. Here's why it was chosen:\n",
        "\n",
        "**Grid Search with Cross-Validation:**\n",
        "- Grid Search is a systematic approach to finding the best combination of hyperparameters for a machine learning model.\n",
        "- Cross-Validation helps in estimating how well a model will perform on unseen data, reducing the risk of overfitting or underfitting.\n",
        "- Grid Search with Cross-Validation is a robust technique for hyperparameter tuning as it systematically explores a predefined parameter grid while using cross-validation to assess the model's performance with different parameter combinations.\n",
        "\n",
        "**Advantages of Grid Search with Cross-Validation:**\n",
        "- It automates the process of hyperparameter tuning, making it less error-prone and more efficient.\n",
        "- It ensures that the best hyperparameters are selected based on how well they perform on multiple cross-validation folds, reducing the risk of overfitting.\n",
        "- It is widely applicable to different machine learning models, including XGBoost.\n",
        "\n",
        "**Why Grid Search with Cross-Validation Was Chosen:**\n",
        "- Grid Search is often preferred when there are a limited number of hyperparameters to explore, and when you want to ensure that you've considered a broad range of hyperparameter combinations.\n",
        "- Cross-Validation provides a reliable estimate of a model's performance on unseen data, which is essential for model generalization and avoiding overfitting.\n",
        "\n",
        "In the provided code, Grid Search with Cross-Validation is used to search for the best combination of hyperparameters for the XGBoost Regressor. The best hyperparameters are selected based on their performance in a cross-validated setting, leading to a well-tuned model that is expected to generalize well to unseen data. This is a sound approach for optimizing the model's predictive capabilities and ensuring it performs at its best."
      ],
      "metadata": {
        "id": "m_JNyJy1GEBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "p-kYelFvGDtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there is a noticeable difference in model performance before and after hyperparameter optimization using Grid Search with Cross-Validation. Let's compare the two sets of results:\n",
        "\n",
        "**Before Hyperparameter Optimization (First Set):**\n",
        "- Train MSE: 86.46\n",
        "- Test MSE: 166.14\n",
        "- Train R-squared: 0.953\n",
        "- Test R-squared: 0.911\n",
        "\n",
        "**After Hyperparameter Optimization (Second Set):**\n",
        "- Train MSE: 39.60\n",
        "- Test MSE: 125.76\n",
        "- Train R-squared: 0.978\n",
        "- Test R-squared: 0.932\n",
        "\n",
        "**Improvements:**\n",
        "1. **Mean Squared Error (MSE):** The MSE on the test data significantly decreased from 166.14 to 125.76, indicating that the model's predictions are closer to the actual values after hyperparameter optimization. Lower MSE values are generally desired, as they indicate a more accurate model.\n",
        "\n",
        "2. **R-squared (R2) Score:** The R-squared score on the test data improved from 0.911 to 0.932 after optimization. R-squared measures how well the model explains the variance in the data, and a higher value is better. This indicates that the optimized model explains more of the variance in the test data.\n",
        "\n",
        "Overall, the second set of results shows a more accurate and better-performing XGBoost Regressor model, which is a clear improvement over the initial model. The hyperparameter optimization process effectively enhanced the model's predictive capabilities and ability to generalize to unseen data."
      ],
      "metadata": {
        "id": "TOcS7imBGDfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "_0Ul6HnpGDGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Here's an explanation of each evaluation metric's indication towards the business and the potential business impact of the ML model used:\n",
        "\n",
        "1. **Mean Squared Error (MSE):**\n",
        "   - **Indication towards Business:** MSE measures the average squared difference between predicted and actual values. In a business context, MSE quantifies the model's prediction accuracy, particularly how close the predicted values are to the actual values.\n",
        "   - **Business Impact:** A lower MSE implies more accurate predictions, which can lead to reduced errors and better decision-making. For example, in financial forecasting, a low MSE indicates better stock price predictions, which can help investors make more profitable decisions.\n",
        "\n",
        "2. **R-squared (R2) Score:**\n",
        "   - **Indication towards Business:** R2 measures the proportion of the variance in the dependent variable (target) that the model can explain. It represents how well the independent variables explain the variability in the dependent variable.\n",
        "   - **Business Impact:** A higher R2 score indicates that the model can better explain and predict the business outcome. For instance, in sales prediction, a high R2 score suggests that the model can accurately capture the factors influencing sales, helping businesses plan inventory and marketing more effectively.\n",
        "\n",
        "3. **Training R-squared (R2) Score:**\n",
        "   - **Indication towards Business:** The training R2 score measures how well the model fits the training data. It's a measure of how well the model learned from the data it was trained on.\n",
        "   - **Business Impact:** While a high training R2 score indicates a good fit, it's important to ensure that the model generalizes well to unseen data. Overfitting (excessive reliance on training data) can lead to poor model performance in real-world business scenarios.\n",
        "\n",
        "4. **Best Hyperparameters (Grid Search with Cross-Validation):**\n",
        "   - **Indication towards Business:** Finding the best hyperparameters is crucial for model performance. It ensures that the model is fine-tuned to produce the best results.\n",
        "   - **Business Impact:** Optimized hyperparameters can significantly enhance the model's accuracy and generalizability. This can lead to better business decisions, such as improved product recommendations, more accurate demand forecasting, and efficient resource allocation.\n",
        "\n",
        "In summary, the choice of evaluation metrics and their values has a direct impact on business outcomes. Lower MSE, higher R2 scores, and well-optimized hyperparameters can lead to more accurate predictions, reduced errors, and better-informed decisions, ultimately improving business processes and profitability."
      ],
      "metadata": {
        "id": "MMGcpC3qGC95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 9 - SUPPORT VECTOR REGRESSOR"
      ],
      "metadata": {
        "id": "LNIFsH1rCnMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already split your data into x_train, x_test, y_train, and y_test\n",
        "# Split the data into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=3)\n",
        "\n",
        "# Create an SVR model\n",
        "svr = SVR(kernel='rbf')  # You can choose the kernel (e.g., 'linear', 'rbf', 'poly')\n",
        "\n",
        "# Fit the SVR model to the training data\n",
        "svr.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = svr.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Calculate the R2 score for the training data\n",
        "training_r2 = svr.score(x_train, y_train)\n",
        "\n",
        "print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared (R2) Score: {r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "gYs9SHe2Hr-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_xgb - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "I6wjGMeLwMBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lGBHa-xXhJu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "PLGD2nqpIE_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we use of a Support Vector Regressor (SVR) model with the radial basis function (RBF) kernel. Here's an explanation of the model and its performance based on the evaluation metric score chart:\n",
        "\n",
        "**Support Vector Regressor (SVR):**\n",
        "- SVR is a supervised machine learning algorithm used for regression tasks.\n",
        "- It works by finding a hyperplane that best fits the data, with the objective of minimizing the margin of error between predicted and actual values while considering a certain margin of tolerance (epsilon).\n",
        "\n",
        "**Performance Evaluation:**\n",
        "- **Training R-squared (R2) Score: 0.67:** The training R2 score measures the proportion of the variance in the target variable (y) explained by the model. An R2 score of 0.67 suggests that the SVR model explains 67% of the variance in the training data. This indicates a moderate level of fit.\n",
        "\n",
        "- **Mean Squared Error (MSE): 592.85:** MSE quantifies the average squared difference between predicted and actual values. A lower MSE is desirable, indicating that the model's predictions are closer to the actual values. In this case, an MSE of 592.85 implies some level of error in the model's predictions.\n",
        "\n",
        "- **R-squared (R2) Score: 0.67:** The R2 score for the test data also indicates that the model explains 67% of the variance in the test data. This is consistent with the training R2 score.\n",
        "\n",
        "**Evaluation Metric Score Chart:**\n",
        "- The R2 score of 0.67 indicates that the SVR model accounts for a moderate proportion of the variance in both the training and test data.\n",
        "- The MSE of 592.85 implies that, on average, the model's predictions deviate from the actual values by a squared error of approximately 592.85.\n",
        "\n",
        "Overall, the SVR model demonstrates a moderate level of fit to the data, as reflected in the R2 scores. However, the relatively high MSE suggests that there is room for improvement in the model's predictive accuracy. Further optimization and hyperparameter tuning may be necessary to enhance the model's performance."
      ],
      "metadata": {
        "id": "sAs-ziplIF74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "8WzZTbavGfLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.svm import SVR\n",
        "# from sklearn.metrics import mean_squared_error, r2_score\n",
        "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "# import numpy as np\n",
        "\n",
        "# # Split the data into training and test sets\n",
        "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=3)\n",
        "\n",
        "# # Create an SVR model\n",
        "# svr = SVR()\n",
        "\n",
        "# # Define a grid of hyperparameters to search\n",
        "# param_grid = {\n",
        "#     'kernel': ['linear', 'rbf', 'poly'],  # You can specify other kernels\n",
        "#     'C': [0.1, 1, 10],  # Regularization parameter\n",
        "#     'epsilon': [0.1, 0.2, 0.3]  # Epsilon parameter\n",
        "# }\n",
        "\n",
        "# # Initialize GridSearchCV with 5-fold cross-validation\n",
        "# grid_search = GridSearchCV(svr, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# # Fit the model to the training data\n",
        "# grid_search.fit(x_train, y_train)\n",
        "\n",
        "# # Get the best hyperparameters from the grid search\n",
        "# best_params = grid_search.best_params_\n",
        "\n",
        "# # Create an SVR model with the best hyperparameters\n",
        "# best_svr = SVR(**best_params)\n",
        "\n",
        "# # Fit the best SVR model to the training data\n",
        "# best_svr.fit(x_train, y_train)\n",
        "\n",
        "# # Predict on the test data\n",
        "# y_pred = best_svr.predict(x_test)\n",
        "\n",
        "# # Evaluate the model\n",
        "# mse = mean_squared_error(y_test, y_pred)\n",
        "# r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# # Calculate the R2 score for the training data\n",
        "# training_r2 = best_svr.score(x_train, y_train)\n",
        "\n",
        "# print(\"Best Hyperparameters:\", best_params)\n",
        "# print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "# print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "# print(f\"R-squared (R2) Score: {r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "SVKB8W3U-bFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Visualizing evaluation Metric Score chart\n",
        "# sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "WOFPeK8lGjO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# plt.figure(figsize=(8,5))\n",
        "# plt.plot(np.array(y_test))\n",
        "# plt.plot(y_pred)\n",
        "# plt.legend([\"Predicted\",\"Actual\"])\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "eKVZ5oi1GmHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "NVDe_rZcGw9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the provided code, the hyperparameter optimization technique used is GridSearchCV. GridSearchCV is employed for hyperparameter tuning, which systematically searches through a predefined grid of hyperparameter values to find the combination that yields the best model performance. The main reason for choosing GridSearchCV is its comprehensiveness and effectiveness in exploring hyperparameter space:\n",
        "\n",
        "1. **Comprehensive Search**: GridSearchCV tests all possible combinations of hyperparameters specified in the grid, ensuring that no suitable set of hyperparameters is missed.\n",
        "\n",
        "2. **Automated and Systematic**: It automates the process of tuning hyperparameters, making it easy to test multiple hyperparameters without manual iteration.\n",
        "\n",
        "3. **Cross-Validation**: It incorporates cross-validation (in this case, 5-fold cross-validation) to evaluate each set of hyperparameters, which provides a robust estimate of model performance and helps prevent overfitting.\n",
        "\n",
        "4. **Scoring Metric**: The choice of using 'neg_mean_squared_error' as the scoring metric indicates that the goal is to minimize the mean squared error (MSE) during hyperparameter tuning.\n",
        "\n",
        "5. **Best Parameters**: GridSearchCV identifies and returns the best hyperparameters that result in the optimal model performance.\n",
        "\n",
        "In summary, GridSearchCV is chosen for its systematic, comprehensive, and automated approach to hyperparameter tuning, which helps fine-tune the Support Vector Regressor (SVR) model for the best predictive performance. The use of cross-validation ensures that the model is evaluated on different subsets of the training data, providing a robust assessment of its generalization capabilities."
      ],
      "metadata": {
        "id": "TarsxO9pGwxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "RSyKNCu6Gwk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"In the interest of project efficiency and to ensure timely progress, I have chosen to temporarily suspend the execution of certain models that were experiencing prolonged delays during the hyperparameter tuning and cross-validation process. While these models are valuable and relevant, their current runtime has exceeded 30 minutes, which is not aligned with the project's timeline. I will revisit these models at a later stage to derive their results when computational resources permit.\""
      ],
      "metadata": {
        "id": "4ZbGhzNvGwZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "GR6zbo21GwIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, let's explain the business implications of the evaluation metrics based on the provided data:\n",
        "\n",
        "1. **Training R-squared (R2) Score: 0.67**\n",
        "   - **Business Implication:** The training R2 score of 0.67 suggests that the SVR model accounts for approximately 67% of the variance in the training data. This means that the model explains a significant portion of the data's variability. However, it also indicates that about 33% of the variance remains unexplained.\n",
        "   - **Business Impact:** The model captures a substantial portion of the underlying patterns in the training data. This can be valuable for businesses seeking to understand and predict trends or patterns within their operations or markets. However, the unexplained variance could represent areas where the model may fail to capture crucial factors affecting the business.\n",
        "\n",
        "2. **Mean Squared Error (MSE): 592.85**\n",
        "   - **Business Implication:** MSE quantifies the average squared difference between the model's predictions and the actual values. A lower MSE indicates that the model's predictions are closer to the actual values. In this case, the MSE is 592.85, implying that, on average, the model's predictions deviate by this amount from the actual values.\n",
        "   - **Business Impact:** The relatively high MSE suggests that the SVR model's predictions have a moderate level of error. For businesses, this means that the model's predictions may not always be highly accurate. Depending on the specific application, the level of error can impact decision-making and potentially lead to suboptimal outcomes.\n",
        "\n",
        "3. **R-squared (R2) Score: 0.67 (Test Data)**\n",
        "   - **Business Implication:** The R2 score for the test data, similar to the training data, is 0.67. This indicates that the model explains about 67% of the variance in the test data. It mirrors the model's performance in the training data.\n",
        "   - **Business Impact:** The consistency between training and test R2 scores suggests that the model generalizes well to new, unseen data. Businesses can have confidence in the model's ability to capture patterns that apply beyond the training dataset. However, it's important to recognize that approximately 33% of variance remains unexplained, and there is room for improvement.\n",
        "\n",
        "In summary, the SVR model's training R2 score and test R2 score indicate its ability to explain a significant portion of the variance in both the training and test data. This is valuable for businesses seeking to understand underlying patterns or relationships in their data. However, the moderate MSE suggests that the model's predictions have a moderate level of error, which may affect the accuracy of business decisions. It's important to consider the specific application and tolerance for prediction errors when using this model in a business context. Further optimization and refinement may be needed to reduce prediction errors and improve the model's overall business impact."
      ],
      "metadata": {
        "id": "RXqPHAc4Gvqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Future Work**"
      ],
      "metadata": {
        "id": "aUzWHSKzNvgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several potential avenues for future work and improvement on your project, depending on your specific goals and the context of your machine learning project. Here are some ideas:\n",
        "\n",
        "1. **Fine-Tuning Hyperparameters**: You can perform more extensive hyperparameter tuning for your models. Adjust parameters such as learning rates, regularization strengths, tree depths, or kernel types to optimize model performance further.\n",
        "\n",
        "2. **Feature Engineering**: Explore additional feature engineering techniques. Create new features, combine existing ones, or perform feature selection to improve the model's ability to capture patterns in the data.\n",
        "\n",
        "3. **Ensemble Methods**: Experiment with ensemble techniques like stacking, where you combine the predictions of multiple models. This can often lead to improved performance.\n",
        "\n",
        "4. **Additional Models**: Try other machine learning algorithms that may be suitable for your problem, such as neural networks, k-Nearest Neighbors, or other regression techniques.\n",
        "\n",
        "5. **Data Preprocessing**: Investigate the data preprocessing pipeline. Ensure data quality, handle missing values, and consider scaling or normalizing features if necessary. Data preprocessing can have a significant impact on model performance.\n",
        "\n",
        "6. **Time Series Analysis**: If your data involves time series, explore time series-specific models and forecasting techniques to account for temporal dependencies.\n",
        "\n",
        "7. **Cross-Validation Strategies**: Experiment with different cross-validation strategies to ensure robust model evaluation. Techniques like k-fold cross-validation, time series cross-validation, or stratified sampling can be beneficial depending on the data type.\n",
        "\n",
        "8. **Feature Importance**: Perform more in-depth feature importance analysis to gain insights into which features are most influential in making predictions. Consider visualizing feature importance scores.\n",
        "\n",
        "9. **Deployment**: If your project aims to deliver real-time predictions, work on deploying your model as a production-ready system, either on a web platform or as an API.\n",
        "\n",
        "10. **Interpretability**: Focus on model interpretability. Use techniques like SHAP (SHapley Additive exPlanations) values or LIME (Local Interpretable Model-Agnostic Explanations) to explain your model's predictions, especially in contexts where interpretability is crucial.\n",
        "\n",
        "11. **Anomaly Detection**: If your project involves anomaly detection, explore specialized techniques for identifying and handling outliers in your data.\n",
        "\n",
        "12. **Scaling and Performance**: Optimize your models for performance. If working with large datasets, consider distributed computing or GPU acceleration for faster training.\n",
        "\n",
        "13. **Monitoring and Maintenance**: Develop a plan for monitoring model performance and retraining the model periodically to account for data drift and concept drift.\n",
        "\n",
        "14. **Data Augmentation**: In cases of limited data, you can explore data augmentation techniques to generate synthetic data points for training.\n",
        "\n",
        "15. **Domain-Specific Knowledge**: Collaborate with domain experts to gain insights and refine the modeling approach. Their expertise can lead to better feature engineering and more informed model choices.\n",
        "\n",
        "16. **Ethical Considerations**: Consider ethical and fairness aspects of your models, especially if they are used to make decisions that impact people's lives. Mitigate biases and ensure fairness in predictions.\n",
        "\n",
        "17. **Scalability**: Assess the scalability of your models to handle larger datasets, as data volume may grow over time.\n",
        "\n",
        "18. **Documentation and Reporting**: Properly document your work, including data sources, methodology, results, and decisions made. Clear reporting is crucial for project transparency and reproducibility.\n",
        "\n",
        "19. **Benchmarking**: Compare your model's performance with existing benchmarks and state-of-the-art solutions in your problem domain.\n",
        "\n",
        "20. **User Feedback and Testing**: If your model is used by end-users, gather feedback and conduct usability testing to improve the user experience.\n",
        "\n",
        "Remember that the choice of future work should align with your project's objectives and constraints. Regularly evaluate and adapt your approach to ensure that your machine learning solution continues to provide value and stays up-to-date with evolving data and requirements."
      ],
      "metadata": {
        "id": "tP7TFd-wNpPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "3iAI3zBHvrpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting the final prediction model depends on your specific use case and the priorities you have. Here are a few considerations based on the provided training and testing accuracy values:\n",
        "\n",
        "1. **Random Forest (CVRndm Forest)**: It has high training and testing accuracy, suggesting good generalization and the potential to avoid overfitting.\n",
        "\n",
        "2. **Gradient Boost (Grdient Boost)**: This model also has high accuracy on both training and testing data. It's well-suited for many tasks due to its robustness.\n",
        "\n",
        "3. **XG Boost (CVXG BOOST)**: Similar to Gradient Boost, XG Boost performs well on both training and testing data. It's known for its performance and speed.\n",
        "\n",
        "4. **Polynomial Regression (CV polynomial LR)**: This model performs well, but it's important to consider that high-degree polynomial regression models can be prone to overfitting. The \"CV\" indicates that it's a cross-validated version, which may help mitigate overfitting.\n",
        "\n",
        "5. **Simple Linear Regression (CV simple LR)**: While it doesn't have the highest accuracy, it's a simple model and can serve as a good baseline. It's also the simplest to interpret.\n",
        "\n",
        "Ultimately, the choice of the final prediction model should consider factors such as:\n",
        "\n",
        "- The nature of your data: Are there nonlinear relationships, interactions, or complex patterns that the chosen model can capture effectively?\n",
        "- Model complexity: Are you looking for a simple and interpretable model, or are you comfortable with more complex models?\n",
        "- Overfitting: Models with high training accuracy but significantly lower testing accuracy might be overfitting, so it's important to evaluate generalization performance.\n",
        "- Computational resources: Some models are computationally intensive, so consider the available resources.\n",
        "- Business requirements: The final choice should align with the specific goals and constraints of your project.\n",
        "\n",
        "It's often a good practice to compare different models through cross-validation, analyze their feature importances, and consider their pros and cons for your particular application before selecting the final prediction model. Additionally, you can perform further evaluations, such as sensitivity analysis and business impact assessment, to make an informed decision."
      ],
      "metadata": {
        "id": "Yc0xx_K6vuru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the model names (you can adjust these as needed)\n",
        "model_names = [\"simple LR \", \"CV simple LR\", \"Polynomial LR\", \"CVpolynomial LR\", \"Ridge LR\", \"CVRidge LR\",\"Lasoo\",\"CVLasoo\",\" Elastic Net\", \"CV elastic net\",\n",
        "               \"Rndm Forrest\",\"CVRndm Forest\",\"Grdient Boost\", \"XG Boost\", \" CVXG BOOST \", \"SVR\"]\n",
        "\n",
        "# Define the training and testing accuracy values for each\n",
        "training_accuracy =[0.67, 0.67, 0.78, 0.78, 0.68, 0.78, 0.76, 0.78, 0.69, 0.77, 0.99, 0.91, 0.82, 0.96, 0.98 , 0.67]\n",
        "testing_accuracy = [0.68, 0.68, 0.78, 0.78, 0.68, 0.78, 0.75, 0.65, 0.68, 0.78, 0.92, 0.92, 0.79, 0.91, 0.93 ,0.67]\n",
        "\n",
        "# Set the width of the bars and their positions\n",
        "width = 0.35\n",
        "x = range(len(model_names))\n",
        "\n",
        "# Create the bar plot\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.bar(x, training_accuracy, width, label='Training Accuracy')\n",
        "plt.bar([i + width for i in x], testing_accuracy, width, label='Testing Accuracy')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Testing Accuracy of Different Models')\n",
        "plt.xticks([i + width / 2 for i in x], model_names)\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wKkTtUX_8_NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above data of machine learning models and their corresponding training and testing accuracy values, we can draw the following conclusions:\n",
        "\n",
        "1. **Simple Linear Regression (LR)**: Both the training and testing accuracies are around 0.67, indicating moderate predictive performance.\n",
        "\n",
        "2. **Cross-Validated (CV) Simple Linear Regression**: The addition of cross-validation did not significantly improve accuracy, with both training and testing accuracies remaining at approximately 0.67.\n",
        "\n",
        "3. **Polynomial Linear Regression**: Polynomial regression with a degree of 2 achieved a high training accuracy of 0.78, indicating a good fit to the training data. The testing accuracy also reached 0.78, suggesting the model generalizes well.\n",
        "\n",
        "4. **Cross-Validated Polynomial Linear Regression**: The cross-validated polynomial regression maintained the high training and testing accuracies of 0.78, confirming good performance on both sets.\n",
        "\n",
        "5. **Ridge Linear Regression**: Ridge regression achieved moderate accuracy with a training and testing accuracy of around 0.68.\n",
        "\n",
        "6. **Cross-Validated Ridge Linear Regression**: Adding cross-validation to Ridge regression led to a slightly higher testing accuracy of 0.78, suggesting better generalization.\n",
        "\n",
        "7. **Lasso Regression**: Lasso regression achieved moderate testing accuracy of 0.75 and training accuracy of 0.76.\n",
        "\n",
        "8. **Cross-Validated Lasso Regression**: Cross-validated Lasso regression resulted in a lower testing accuracy of 0.65, which may indicate overfitting.\n",
        "\n",
        "9. **Elastic Net**: Elastic Net achieved moderate testing accuracy of 0.68 and training accuracy of 0.69.\n",
        "\n",
        "10. **Cross-Validated Elastic Net**: The cross-validated Elastic Net model maintained a testing accuracy of 0.78, suggesting good generalization.\n",
        "\n",
        "11. **Random Forest**: Random Forest achieved high accuracy with a training accuracy of 0.99 and a testing accuracy of 0.92, indicating an excellent fit to the training data and good generalization.\n",
        "\n",
        "12. **Cross-Validated Random Forest**: Cross-validated Random Forest maintained a high testing accuracy of 0.92, suggesting strong generalization.\n",
        "\n",
        "13. **Gradient Boosting**: Gradient Boosting achieved a training accuracy of 0.82 and a testing accuracy of 0.79, indicating good performance.\n",
        "\n",
        "14. **XG Boost**: XG Boost achieved high accuracy with a training accuracy of 0.96 and a testing accuracy of 0.91, suggesting excellent fit and generalization.\n",
        "\n",
        "15. **Cross-Validated XG Boost**: The cross-validated XG Boost maintained a high testing accuracy of 0.93, indicating robust generalization.\n",
        "\n",
        "16. **Support Vector Regression (SVR)**: SVR achieved moderate accuracy with a training and testing accuracy of approximately 0.67.\n",
        "\n",
        "In summary, it appears that ensemble models like Random Forest, XG Boost, and Cross-Validated XG Boost outperform the other models in terms of testing accuracy, indicating strong predictive performance. Polynomial Linear Regression and Cross-Validated Polynomial Linear Regression also perform well. It's important to consider not only accuracy but also other evaluation metrics and the specific context of your problem when selecting the most suitable model for your application. Additionally, further analysis and hyperparameter tuning may lead to improved results for some models."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}