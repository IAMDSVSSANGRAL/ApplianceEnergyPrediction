{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IAMDSVSSANGRAL/applianceenergyprediction/blob/main/Appliance_energy_prediction_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 -Samadhan Tangde**\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective:\n",
        "The objective of this project is to develop a regression model that accurately predicts the energy consumption of household appliances based on various input features. The model aims to provide insights into energy usage patterns and facilitate energy efficiency improvements in residential settings.\n",
        "\n",
        "Data:\n",
        "The project utilizes a dataset that contains information on household appliance energy consumption along with several relevant input features. The dataset includes variables such as temperature, humidity, time of day, and various appliance power readings. The data is collected over a specific time period and is representative of real-world residential energy usage scenarios.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "Exploratory Data Analysis (EDA):\n",
        "\n",
        "Perform a thorough analysis of the dataset to understand the distribution, statistics, and relationships among variables.\n",
        "Identify any missing values, outliers, or data quality issues that need to be addressed.\n",
        "Visualize the data using appropriate charts and graphs to gain insights into the patterns and trends.\n",
        "Data Preprocessing:\n",
        "\n",
        "Handle missing values by applying suitable imputation techniques or deciding on appropriate strategies for dealing with them.\n",
        "Address outliers and anomalies by considering various methods such as removal, transformation, or capping.\n",
        "Normalize or scale the data if necessary to ensure all features are on a similar scale.\n",
        "Feature Engineering:\n",
        "\n",
        "Explore the relationships between the input features and the target variable (appliance energy consumption) to identify potential feature engineering opportunities.\n",
        "Create new features, derive meaningful variables, or transform existing variables to capture important patterns or interactions in the data.\n",
        "Model Development:\n",
        "\n",
        "Split the dataset into training and testing sets for model development and evaluation.\n",
        "Select an appropriate regression algorithm (e.g., linear regression, decision tree regression, random forest regression) based on the project requirements and characteristics of the data.\n",
        "Train the model using the training data and tune hyperparameters to optimize performance.\n",
        "Evaluate the model's performance using various metrics such as mean squared error (MSE), mean absolute error (MAE), and R-squared.\n",
        "Model Evaluation and Interpretation:\n",
        "\n",
        "Assess the model's performance on the testing data to measure its ability to generalize to unseen data.\n",
        "Interpret the model's coefficients or feature importance to gain insights into the factors that have the most significant impact on appliance energy consumption.\n",
        "Validate the model's predictions against domain knowledge or external benchmarks to ensure its reliability and usefulness.\n",
        "Model Deployment and Recommendations:\n",
        "\n",
        "Deploy the trained model into a production environment or create a user-friendly interface for stakeholders to interact with the model.\n",
        "Provide recommendations based on the model's predictions and insights to improve energy efficiency, optimize appliance usage, or suggest modifications in residential settings.\n",
        "Conclusion:\n",
        "The Appliance Energy Prediction regression project aims to develop a robust regression model to accurately predict household appliance energy consumption. By analyzing and understanding the data, performing feature engineering, and building an effective regression model, the project provides valuable insights and recommendations for optimizing energy usage and promoting energy-efficient practices in residential settings.\n",
        "\n",
        "Note: This project summary provides a general outline and can be tailored based on specific requirements, dataset characteristics, and project goals."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/IAMDSVSSANGRAL/applianceenergyprediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, here is the problem statement broken down into bullet points using different phrases:\n",
        "\n",
        "- **Data Source**: The dataset spans approximately 4.5 months and includes information collected at 10-minute intervals. It consists of data from a ZigBee wireless sensor network monitoring temperature and humidity in a house, energy consumption recorded by m-bus energy meters, and weather data from Chievres Airport, Belgium.\n",
        "\n",
        "- **Data Averaging**: The wireless sensor network reports temperature and humidity every 3.3 minutes, but the data is averaged over 10-minute periods.\n",
        "\n",
        "- **Objective**: The primary goal is to develop a machine learning model capable of accurately predicting energy usage based on the provided features.\n",
        "\n",
        "- **Utility**: This predictive model has potential applications for building managers, energy companies, and policymakers. It can aid in optimizing energy consumption, reducing costs, and minimizing the environmental impact of energy usage.\n",
        "\n",
        "- **Influence Factors**: The model aims to consider a range of influencing factors, including temperature, humidity, illumination, and time of day, all of which can impact energy consumption in a building.\n",
        "\n",
        "- **Pattern and Trend Identification**: Building managers and energy firms can benefit from this model by identifying patterns and trends in energy consumption. This can help them make informed decisions, such as adjusting HVAC settings, optimizing lighting, or implementing energy-efficient solutions.\n",
        "\n",
        "- **Policymaker Applications**: Policymakers can also leverage the insights from this model to develop regulations and incentives that promote energy efficiency and sustainability.\n",
        "\n",
        "- **Random Variables**: The dataset includes random variables designed for testing regression models and filtering out non-predictive features.\n",
        "\n",
        "- **Integration of External Data**: External weather data from Chievres Airport, Belgium, was integrated into the dataset using date and time columns, enhancing the model's ability to make energy usage forecasts.\n",
        "\n",
        "- **Environmental Impact**: One of the broader goals is to contribute to reducing the environmental impact of energy usage through better management and decision-making."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -"
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "\n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "\n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "\n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "\n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime as dt\n",
        "\n",
        "# Import Data Visualisation Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly as pl\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "from pandas.plotting import scatter_matrix\n",
        "%matplotlib inline\n",
        "\n",
        "# Set the plot style and display options\n",
        "plt.style.use('ggplot')\n",
        "sns.set()\n",
        "\n",
        "# To display all the columns in Dataframe\n",
        "pd.set_option('display.max_columns', None)\n",
        "# Import Library to visualise missing data\n",
        "import missingno as mno\n",
        "\n",
        "# Import and Ignore warnings for better code readability,\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the data set\n",
        "data_raw = pd.read_csv('/content/drive/MyDrive/Santa/Regression capstone/data_application_energy.csv')"
      ],
      "metadata": {
        "id": "TmkdQzZojsHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a copy of data set\n",
        "data = data_raw.copy()"
      ],
      "metadata": {
        "id": "HJ8f2WTWL-YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "num_rows, num_cols = data.shape\n",
        "\n",
        "print(\"Number of rows:\", num_rows)\n",
        "print(\"Number of columns:\", num_cols)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your date column is named \"date_column\"\n",
        "data['date'] = pd.to_datetime(data['date'])"
      ],
      "metadata": {
        "id": "76kdxS8WdrbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting date as the index:\n",
        "data.set_index('date', inplace=True)"
      ],
      "metadata": {
        "id": "5kPxyGdERI8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count assinged a dataframe name 'df'\n",
        "df = data[data.duplicated()]"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#There is no duplicate rows in the data\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bVN4H8UU09Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is in the form of a Pandas DataFrame with 29 columns and 19,735 rows. It appears to be a dataset with multiple features and observations, but without the context of what this dataset represents, it's challenging to provide specific insights. However, I can offer some general insights you can gain from this data:\n",
        "\n",
        "1. **Data Size**: The dataset contains 19,735 data points, which is a significant amount of data.\n",
        "\n",
        "2. **Data Types**: Most of the columns contain numerical data, with 26 columns having float64 data type and 2 columns with int64 data type. The 'date' column seems to contain date values as objects.\n",
        "\n",
        "3. **Features**: The columns labeled 'T1,' 'T2,' 'T3,' etc., represent temperature measurements, while columns labeled 'RH_1,' 'RH_2,' 'RH_3,' etc., represent relative humidity measurements. 'Appliances' and 'lights' are integer columns, which might be related to energy consumption and lighting. Other columns have labels such as 'T_out' (outdoor temperature), 'Windspeed,' 'RH_out' (outdoor humidity), and more.\n",
        "\n",
        "4. **Data Completeness**: There are no missing values (non-null) in any of the columns, which is a good sign for data quality.\n",
        "\n",
        "5. **Memory Usage**: The dataset consumes 4.4+ MB of memory, which might be relevant for memory-constrained analyses.\n",
        "\n",
        "6. **NO Duplicate values**:We don't see any output, it's possible that there are no duplicated rows in your original DataFrame data.\n",
        "\n",
        "To gain more meaningful insights from this data, you'll need to have a clear understanding of what the dataset represents and what kind of analysis you want to perform. Depending on the context, you could explore relationships between different variables, conduct statistical analysis, visualize data, and build predictive models. Please provide more information about the dataset and your specific goals if you'd like more detailed insights."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "data.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The observation data consists of the following variables:**\n",
        "\n",
        "\n",
        "datetime year-month-day hour : minute:second\n",
        "\n",
        "Appliances: energy use in Wh [TARGETED]\n",
        "\n",
        "lights: energy use of light fixtures in the house in Wh\n",
        "\n",
        "T1: Temperature in kitchen area, in Celsius\n",
        "\n",
        "RH_1: Humidity in kitchen area, in %\n",
        "\n",
        "T2: Temperature in living room area, in Celsius\n",
        "\n",
        "RH_2:Humidity in living room area, in %\n",
        "\n",
        "T3:Temperature in laundry room area\n",
        "\n",
        "RH_3:Humidity in laundry room area, in %\n",
        "\n",
        "T4:Temperature in office room, in Celsius\n",
        "\n",
        "RH_4:Humidity in office room, in %\n",
        "\n",
        "T5:Temperature in bathroom, in Celsius\n",
        "\n",
        "RH_5:Humidity in bathroom, in %\n",
        "\n",
        "T6:Temperature outside the building (north side), in Celsius\n",
        "\n",
        "RH_6:Humidity outside the building (north side), in %\n",
        "\n",
        "T7:Temperature in ironing room , in Celsius\n",
        "\n",
        "RH_7:Humidity in ironing room, in %\n",
        "\n",
        "T8:Temperature in teenager room 2, in Celsius\n",
        "\n",
        "RH_8:Humidity in teenager room 2, in %\n",
        "\n",
        "T9:Temperature in parents room, in Celsius\n",
        "\n",
        "RH_9:Humidity in parents room, in %\n",
        "\n",
        "T_out:Temperature outside (from Chièvres weather station), in Celsius\n",
        "\n",
        "Press_mm_hg: (from Chièvres weather station), in mm Hg\n",
        "\n",
        "RH_out: Humidity outside (from Chièvres weather station), in %\n",
        "\n",
        "Windspeed: (from Chièvres weather station), in m/s\n",
        "\n",
        "Visibility: (from Chièvres weather station), in km\n",
        "\n",
        "Tdewpoint: (from Chièvres weather station), °C\n",
        "\n",
        "rv1: Random variable 1, nondimensional\n",
        "\n",
        "rv2: Rnadom variable 2, nondimensional"
      ],
      "metadata": {
        "id": "PYzlcej3OU5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Unique Values count for each variable.\n",
        "for i in data.columns.tolist():\n",
        "  print(\"The unique values in\",i, \"is\",data[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "LGAz2eAYwhmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Round the unique values to two decimal places\n",
        "rounded_unique_values = data.apply(lambda x: set(round(val, 2) for val in x))\n",
        "\n",
        "# Print the unique values for each feature\n",
        "for feature, unique in rounded_unique_values.items():\n",
        "    print(f'{feature}: {unique}')"
      ],
      "metadata": {
        "id": "_6QYnjVk4wNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating columns:\n",
        "temperature_column = [i for i in data.columns if \"T\" in i]\n",
        "humidity_column = [i for i in data.columns if \"RH\" in i]\n",
        "other = [i for i in data.columns if (\"T\" not in i)&(\"RH\" not in i)]"
      ],
      "metadata": {
        "id": "Kst3KQHVRrVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[temperature_column].describe(include='all')"
      ],
      "metadata": {
        "id": "DzXNlgYLRtMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can derive several insights:\n",
        "\n",
        "1. **Count**:\n",
        "   - There are 19,735 data points for each of the temperature-related variables (T1, T2, T3, T4, T5, T6, T7, T8, T9, T_out, and Tdewpoint). This indicates that there are no missing values in these columns.\n",
        "\n",
        "2. **Mean (Average)**:\n",
        "   - The mean values for the temperature-related variables are in the range of 16.79°C to 26.26°C. The \"T3\" variable has the highest mean at approximately 22.27°C, while \"T5\" has the lowest mean at about 19.59°C.\n",
        "\n",
        "3. **Standard Deviation (std)**:\n",
        "   - The standard deviations for the temperature-related variables range from approximately 1.61°C to 2.20°C. Variables like \"T3\" and \"T4\" have relatively low variability, while \"T9\" has slightly higher variability.\n",
        "\n",
        "4. **Minimum (min)**:\n",
        "   - The minimum values for the temperature-related variables range from 15.10°C to 29.24°C. These values indicate the lower bounds of the temperature measurements.\n",
        "\n",
        "5. **25th Percentile (25%)**:\n",
        "   - The 25th percentile values represent the lower quartile of the data. For example, the 25th percentile of \"T2\" is approximately 18.79°C.\n",
        "\n",
        "6. **Median (50%)**:\n",
        "   - The median values (50th percentile) represent the middle values of the dataset. For instance, the median temperature \"T7\" is approximately 20.03°C.\n",
        "\n",
        "7. **75th Percentile (75%)**:\n",
        "   - The 75th percentile values represent the upper quartile of the data. The 75th percentile of \"T6\" is approximately 11.26°C.\n",
        "\n",
        "8. **Maximum (max)**:\n",
        "   - The maximum values represent the upper bounds of the temperature measurements. \"T4\" has the highest maximum value at approximately 26.20°C, while \"T5\" has the lowest maximum at about 25.79°C."
      ],
      "metadata": {
        "id": "u2zLtBh8XctQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[humidity_column].describe()"
      ],
      "metadata": {
        "id": "w_j4XCxISHY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e can derive several insights regarding the relative humidity (RH) variables:\n",
        "\n",
        "1. **Count**:\n",
        "   - There are 19,735 data points for each of the RH-related variables (RH_1, RH_2, RH_3, RH_4, RH_5, RH_6, RH_7, RH_8, RH_9, and RH_out). This indicates that there are no missing values in these columns.\n",
        "\n",
        "2. **Mean (Average)**:\n",
        "   - The mean values for the relative humidity variables vary across the columns. For example, \"RH_5\" has the highest mean at approximately 50.95%, while \"RH_7\" has the lowest mean at around 35.39%. The \"RH_out\" variable, which represents outdoor relative humidity, has a mean of approximately 79.75%.\n",
        "\n",
        "3. **Standard Deviation (std)**:\n",
        "   - The standard deviations for the relative humidity variables also vary. \"RH_5\" has a standard deviation of approximately 9.02, indicating relatively higher variability, while \"RH_3\" has a lower standard deviation of around 3.25.\n",
        "\n",
        "4. **Minimum (min)**:\n",
        "   - The minimum values for the relative humidity variables indicate the lower bounds of the humidity measurements. For example, \"RH_6\" has a minimum of approximately 1.00% which look like there are outlier on lower bound of RH_6 and \"RH_out\" has a minimum of 24.00%.\n",
        "\n",
        "5. **25th Percentile (25%)**:\n",
        "   - The 25th percentile values represent the lower quartile of the data. \"RH_7\" has a 25th percentile value of approximately 31.50%.\n",
        "\n",
        "6. **Median (50%)**:\n",
        "   - The median values (50th percentile) represent the middle values of the dataset. \"RH_9\" has a median relative humidity of approximately 40.90%.\n",
        "\n",
        "7. **75th Percentile (75%)**:\n",
        "   - The 75th percentile values represent the upper quartile of the data. \"RH_4\" has a 75th percentile value of approximately 42.16%.\n",
        "\n",
        "8. **Maximum (max)**:\n",
        "   - The maximum values represent the upper bounds of the relative humidity measurements. \"RH_1\" has the highest maximum value at approximately 63.36%, and \"RH_out\" has the lowest maximum value at 100.00%.\n",
        "\n"
      ],
      "metadata": {
        "id": "f0n4GXA2YRTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[other].describe()"
      ],
      "metadata": {
        "id": "StZKZkcwSQpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can derive several insights regarding the variables Appliances, lights, Press_mm_hg, Windspeed, Visibility, rv1, and rv2:\n",
        "\n",
        "1. **Appliances**:\n",
        "   - The \"Appliances\" variable represents energy consumption related to appliances. The data ranges from a minimum of 10 to a maximum of 1080, with an average (mean) consumption of approximately 97.69. **The standard deviation is relatively high, indicating significant variability in appliance energy usage.**\n",
        "\n",
        "2. **Lights**:\n",
        "   - The \"lights\" variable shows energy consumption related to lighting. It varies from 0 to 70, with an average of approximately 3.80. The standard deviation suggests some variability in lighting energy consumption. **upto 75 percent of value have 0 values which is slightly ODD.**\n",
        "\n",
        "3. **Press_mm_hg**:\n",
        "   - \"Press_mm_hg\" represents atmospheric pressure. The pressure varies from 729.30 to 772.30, with an average of approximately 755.52. The data has relatively low variability.\n",
        "\n",
        "4. **Windspeed**:\n",
        "   - The \"Windspeed\" variable indicates wind speed and varies from 0 to 14. The average wind speed is about 4.04. The standard deviation suggests some variation in wind speed. **Maximum value is 14 which is very far from 75% of values that is 5.50**\n",
        "\n",
        "5. **Visibility**:\n",
        "   - \"Visibility\" represents the visibility in meters. It ranges from 1 to 66, with an average of approximately 38.33. The data exhibits relatively **high variability**.\n",
        "\n",
        "6. **rv1 and rv2**:\n",
        "   - The columns \"rv1\" and \"rv2\" have identical statistics, suggesting that they are likely **highly correlated or identical features**. They have a minimum value of approximately 0.0053 and a maximum value of around 49.9965."
      ],
      "metadata": {
        "id": "fg_iUznxVuXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to map current column names to new column names\n",
        "column_mapping = {'T1': 'KITCHEN_TEMP',\n",
        "    'RH_1': 'KITCHEN_HUM',\n",
        "    'T2': 'LIVING_TEMP',\n",
        "    'RH_2' :'LIVING_HUM',\n",
        "    'T3': 'BEDROOM_TEMP',\n",
        "    'RH_3':'BEDROOM_HUM',\n",
        "    'T4' : 'OFFICE_TEMP',\n",
        "    'RH_4' : 'OFFICE_HUM',\n",
        "    'T5' : 'BATHROOM_TEMP',\n",
        "    'RH_5': 'BATHROOM_HUM',\n",
        "    'T6':'OUTSIDE_TEMP_build',\n",
        "    'RH_6': 'OUTSIDE_HUM_build',\n",
        "    'T7': 'IRONING_ROOM_TEMP',\n",
        "    'RH_7' : 'IRONING_ROOM_HUM',\n",
        "    'T8' :'TEEN_ROOM_2_TEMP',\n",
        "    'RH_8' : 'TEEN_ROOM_HUM',\n",
        "    'T9': 'PARENTS_ROOM_TEMP',\n",
        "    'RH_9': 'PARENTS_ROOM_HUM',\n",
        "    'T_out' :'OUTSIDE_TEMP_wstn',\n",
        "    'RH_out' :'OUTSIDE_HUM_wstn'}\n",
        "\n",
        "# Rename the columns using the mapping\n",
        "data.rename(columns=column_mapping, inplace=True)"
      ],
      "metadata": {
        "id": "xE1jxXD7iQ5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "ZW2VQUgF9wi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new features\n",
        "data['month'] = data.index.month\n",
        "data['weekday'] = data.index.weekday\n",
        "data['hour'] = data.index.hour\n",
        "data['week'] = data.index.week\n",
        "data['day'] = data.index.day\n",
        "data['day_of_week'] = data.index.dayofweek"
      ],
      "metadata": {
        "id": "UGcDgyyT3nUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(2)"
      ],
      "metadata": {
        "id": "lXzEM5GB3sMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting values of the \"lights\" column:\n",
        "data['lights'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "4WZ0JUpVUwPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "77% value of lights column are 0 and it is not relevant for prediction. so we are going to drop this column"
      ],
      "metadata": {
        "id": "TBGBIpteVhXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the lights column:\n",
        "data.drop(columns='lights', inplace=True)"
      ],
      "metadata": {
        "id": "VJbGWhxOVcbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reorder the data for clear vision\n",
        "desired_order = ['KITCHEN_TEMP','LIVING_TEMP','BEDROOM_TEMP','OFFICE_TEMP','BATHROOM_TEMP','OUTSIDE_TEMP_build','IRONING_ROOM_TEMP','TEEN_ROOM_2_TEMP','PARENTS_ROOM_TEMP','OUTSIDE_TEMP_wstn',\n",
        "                 'KITCHEN_HUM','LIVING_HUM','BEDROOM_HUM','OFFICE_HUM','BATHROOM_HUM','OUTSIDE_HUM_build','IRONING_ROOM_HUM','TEEN_ROOM_HUM','PARENTS_ROOM_HUM','OUTSIDE_HUM_wstn',\n",
        "                 \"Tdewpoint\",\"Press_mm_hg\",\"Windspeed\",\"Visibility\",\"rv1\", \"rv2\",'month','weekday','hour','week','day','day_of_week',\"Appliances\"]\n",
        "#assinging new_data as new name of dataframe\n",
        "data = data.reindex(columns=desired_order)"
      ],
      "metadata": {
        "id": "6Xw2W7igdBfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail(2)"
      ],
      "metadata": {
        "id": "vdToJwPyiufd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AUTOEDA\n",
        "!pip install sweetviz\n",
        "import sweetviz as sv\n",
        "sweet_report = sv.analyze(data)\n",
        "sweet_report.show_html('sweet_report.html')"
      ],
      "metadata": {
        "id": "4hKGGwU4HsqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pivot table to aggregate the daily energy consumption\n",
        "daily_energy = data.pivot_table(values='Appliances', index='day', columns='month', aggfunc = 'mean')\n",
        "\n",
        "# Create a heatmap using the pivot table\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Daily Energy Consumption')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Day')\n",
        "plt.imshow(daily_energy, cmap='YlGnBu', aspect='auto')\n",
        "plt.colorbar(label='Energy Consumption')\n",
        "plt.xticks(range(0,5), ['Jan', 'Feb', 'Mar', 'Apr', 'May'])\n",
        "plt.yticks(range(1, 32))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xSZHw-DN80Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I choose this chart to identify the distribution of each variable in the data.\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the day of the week values to their respective names\n",
        "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "data['day_of_week'] = data['day_of_week'].map(lambda x: day_names[x])\n",
        "\n",
        "# Create a box plot or violin plot to compare energy consumption across different days of the week\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='day_of_week', y='Appliances', data=data, order=day_names)  # or sns.violinplot()\n",
        "plt.title('Appliance Energy Consumption by Day of the Week')\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Energy Consumption')"
      ],
      "metadata": {
        "id": "vFnJADwHDwAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a line plot to show the trend of energy consumption over time\n",
        "import plotly.express as px\n",
        "\n",
        "# Assuming you have a DataFrame 'data' with a datetime index\n",
        "fig = px.line(data, x=data.index, y='Appliances', title='Energy Consumption of Appliances Over Time')\n",
        "fig.update_xaxes(title_text='Date', tickangle=-45)\n",
        "fig.update_yaxes(title_text='Energy Consumption')\n",
        "\n",
        "# Show the Plotly figure\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "BLX7c5AF-_yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the lights column:\n",
        "data.drop(columns='day_of_week', inplace=True)"
      ],
      "metadata": {
        "id": "AWWj0N4zlMRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Examining the outlier in the dataset\n",
        "# Assuming 'data' is your DataFrame\n",
        "num_columns = len(data.columns)\n",
        "fig, axes = plt.subplots(nrows=num_columns, figsize=(8, num_columns*6))\n",
        "\n",
        "for i, column in enumerate(data.columns):\n",
        "    # Exclude 'day_of_week' from the visualization\n",
        "    if column != 'day_of_week':\n",
        "        data.boxplot(column=column, ax=axes[i])\n",
        "        axes[i].set_title(f'Box Plot for {column}')\n",
        "        axes[i].set_xlabel('Column')\n",
        "        axes[i].set_ylabel('Values')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "#close look on four columns\n",
        "fig_sub = make_subplots(rows=1, cols=4, shared_yaxes=False)\n",
        "\n",
        "fig_sub.add_trace(go.Box(y=data['Appliances'].values,name='Appliances'),row=1, col=1)\n",
        "fig_sub.add_trace(go.Box(y=data['Windspeed'].values,name='Windspeed'),row=1, col=2)\n",
        "fig_sub.add_trace(go.Box(y=data['Visibility'].values,name='Visibility'),row=1, col=3)\n",
        "fig_sub.add_trace(go.Box(y=data['Press_mm_hg'].values,name='Press_mm_hg'),row=1, col=4)\n",
        "\n",
        "fig_sub.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame with the energy consumption data\n",
        "# You can group the data by hour and calculate the mean energy consumption for each hour\n",
        "hourly_energy = data.groupby('hour')['Appliances'].mean()\n",
        "\n",
        "# Create a line chart to visualize the hourly energy consumption patterns\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(hourly_energy.index, hourly_energy.values, marker='o', linestyle='-')\n",
        "plt.title('Hourly Energy Consumption Patterns')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Energy Consumption (mean)')\n",
        "plt.xticks(range(24))\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame with the relevant columns (e.g., 'KITCHEN_TEMP', 'OUTSIDE_TEMP_build', and 'Appliances')\n",
        "# You can create a scatter plot with a regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='KITCHEN_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can create a scatter plot with a regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='LIVING_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can create a scatter plot with a regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='BEDROOM_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can create a scatter plot with a regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='OFFICE_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can create a scatter plot with a regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='BATHROOM_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can create a scatter plot with a regression line for outdoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='OUTSIDE_TEMP_build', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can also create a similar scatter plot and regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='IRONING_ROOM_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Outdoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Outdoor Temperature (OUTSIDE_TEMP_build)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can also create a similar scatter plot and regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='TEEN_ROOM_2_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Outdoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Outdoor Temperature (OUTSIDE_TEMP_build)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can also create a similar scatter plot and regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='PARENTS_ROOM_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Outdoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Outdoor Temperature (OUTSIDE_TEMP_build)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can also create a similar scatter plot and regression line for outdoor temperature data from weather station vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='OUTSIDE_TEMP_wstn', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Outdoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Outdoor Temperature (OUTSIDE_TEMP_build)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame with the relevant columns (e.g., 'hour' and 'Appliances')\n",
        "# You can create a line chart to show energy consumption throughout the day\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Group the data by hour and calculate the mean energy consumption for each hour\n",
        "hourly_energy = data.groupby('hour')['Appliances'].mean()\n",
        "\n",
        "# Split the data into daytime (6:00 AM to 6:00 PM) and nighttime (6:00 PM to 6:00 AM)\n",
        "daytime_energy = hourly_energy[6:18]\n",
        "\n",
        "nighttime_energy= hourly_energy[0:6].append(hourly_energy[18:24])\n",
        "\n",
        "# Plot the daytime and nighttime energy consumption\n",
        "plt.plot(daytime_energy.index, daytime_energy.values, label='Daytime', marker='o',color = 'r')\n",
        "plt.plot(nighttime_energy.index, nighttime_energy.values, label='Nighttime', marker='o',color = 'b')\n",
        "\n",
        "plt.title('Energy Consumption Throughout the Day')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Mean Energy Consumption')\n",
        "plt.xticks(range(24))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame with relevant columns (e.g., 'weekday' and 'Appliances')\n",
        "# You can create a line chart to compare energy consumption on weekdays vs. weekends\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Group the data by 'weekday' and calculate the mean energy consumption for weekdays and weekends\n",
        "weekday_energy = data[data['weekday'] < 5].groupby('hour')['Appliances'].mean()\n",
        "weekend_energy = data[data['weekday'] >= 5].groupby('hour')['Appliances'].mean()\n",
        "\n",
        "# Plot energy consumption for weekdays and weekends\n",
        "plt.plot(weekday_energy.index, weekday_energy.values, label='Weekdays', marker='o')\n",
        "plt.plot(weekend_energy.index, weekend_energy.values, label='Weekends', marker='o')\n",
        "\n",
        "plt.title('Energy Consumption on Weekdays vs. Weekends')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Mean Energy Consumption')\n",
        "plt.xticks(range(24))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame with relevant columns (e.g., 'Appliances', 'T_out', 'RH_out', 'Windspeed')\n",
        "# You can create scatter plots to explore these relationships\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "\n",
        "# Scatter plot of energy consumption vs. TDewPoint\n",
        "sns.scatterplot(data=data, x='Tdewpoint', y='Appliances', alpha=0.5, label='Outdoor Temperature')\n",
        "\n",
        "# Scatter plot of energy consumption vs. Press_mm_hg\n",
        "sns.scatterplot(data=data, x='Press_mm_hg', y='Appliances', alpha=0.5, label='Outdoor Humidity')\n",
        "\n",
        "# Scatter plot of energy consumption vs. wind speed\n",
        "sns.scatterplot(data=data, x='Windspeed', y='Appliances', alpha=0.5, label='Wind Speed')\n",
        "\n",
        "# Scatter plot of energy consumption vs. OUTSIDE_TEMP_wstn\n",
        "sns.scatterplot(data=data, x='OUTSIDE_TEMP_wstn', y='Appliances', alpha=0.5, label='OUTSIDE_TEMP_wstn')\n",
        "\n",
        "# Scatter plot of energy consumption vs. OUTSIDE_HUM_wstn\n",
        "sns.scatterplot(data=data, x='OUTSIDE_HUM_wstn', y='Appliances', alpha=0.5, label='OUTSIDE_HUM_wstn')\n",
        "\n",
        "plt.title('Energy Consumption vs. Weather Variables')\n",
        "plt.xlabel('Weather Variables')\n",
        "plt.ylabel('Energy Consumption')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing distributions using Histograms:\n",
        "data.hist(figsize=(17, 20), grid=True);"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "correlation_matrix = data.corr()\n",
        "plt.figure(figsize=(21, 18))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"RdYlGn\")\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OtkZZyU1fiOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of column names in your dataset\n",
        "columns = data.columns\n",
        "\n",
        "# Determine the number of rows and columns for subplots\n",
        "num_rows = len(columns)\n",
        "num_cols = 1\n",
        "\n",
        "# Create subplots with specified number of rows and columns\n",
        "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 80))\n",
        "\n",
        "# Iterate over each column (excluding \"Appliances\") and create pair plot\n",
        "for i, column in enumerate(columns):\n",
        "    #if column != \"Appliances\":\n",
        "        sns.scatterplot(data=data, x=\"Appliances\", y=column, ax=axes[i])\n",
        "        axes[i].set_xlabel(\"Appliances\")\n",
        "        axes[i].set_ylabel(column)\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pzVpJ9VlZ_qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-MMp3DFQLEqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is huge prescece of heteroscedasticity and we usually do log tranformation to solve this error."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no significant linear relationship between the independent variables and the appliance energy consumption.\n",
        "\n",
        "Alternative Hypothesis (H1): There is a significant linear relationship between the independent variables and the appliance energy consumption."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "OBBrJOQ5cjCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Extract the two continuous variables you want to test\n",
        "column_to_drop = ['Appliances']\n",
        "independent_variables = data.drop(column_to_drop, axis = 1)\n",
        "dependent_variable = data['Appliances']\n",
        "\n",
        "# Step 2: Perform the Correlation Test (Pearson correlation)\n",
        "correlation_coefficients, p_values = [], []\n",
        "for feature in independent_variables.columns:\n",
        "    correlation_coefficient, p_value = pearsonr(independent_variables[feature], dependent_variable)\n",
        "    correlation_coefficients.append(correlation_coefficient)\n",
        "    p_values.append(p_value)\n",
        "\n",
        "# Step 3: Interpret the Results for each feature\n",
        "alpha = 0.05  # Significance level (commonly set to 0.05)\n",
        "for i, feature in enumerate(independent_variables.columns):\n",
        "    print(f\"Correlation Coefficient for '{feature}': {correlation_coefficients[i]:.4f}\")\n",
        "    print(f\"P-value for '{feature}': {p_values[i]:.4f}\")\n",
        "\n",
        "    if p_values[i] < alpha:\n",
        "        print(\"Result: The correlation is statistically significant (reject H0).\\n\")\n",
        "    else:\n",
        "        print(\"Result: There is no significant correlation (fail to reject H0).\\n\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the practical implementation provided earlier, the statistical test used to obtain the p-value is the Pearson correlation coefficient test. The Pearson correlation coefficient, also known as Pearson's r or simply r, is a measure of the linear relationship between two continuous variables."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The p-value obtained from the test indicates the probability of observing the calculated correlation coefficient (or a more extreme value) if the null hypothesis is true. The null hypothesis (H0) in this context states that there is no significant linear relationship between the two variables.\n",
        "\n",
        "By comparing the p-value to a chosen significance level (alpha), commonly set to 0.05 (5%), we can determine whether to reject or fail to reject the null hypothesis. If the p-value is less than alpha, we reject the null hypothesis, suggesting a statistically significant correlation. If the p-value is greater than alpha, we fail to reject the null hypothesis, indicating no significant correlation.\n",
        "\n",
        "This test is appropriate when you want to assess the strength and direction of the linear relationship between two continuous variables. It is commonly used to explore the association between variables in correlation analysis and is widely used in various fields of research and data analysis."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import missingno as msno\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the null matrix\n",
        "msno.matrix(data)\n",
        "\n",
        "# Customizing the plot\n",
        "plt.title('Null Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Luckily there are no missing value in our dataset. No missing values imputation required**"
      ],
      "metadata": {
        "id": "qOnQ9TPjpJTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "df= data.copy()\n",
        "col_list = list(df.describe().columns)\n",
        "\n",
        "#find the outliers using boxplot\n",
        "plt.figure(figsize=(25, 20))\n",
        "plt.suptitle(\"Box Plot\", fontsize=18, y=0.95)\n",
        "\n",
        "for n, ticker in enumerate(col_list):\n",
        "\n",
        "    ax = plt.subplot(8, 4, n + 1)\n",
        "\n",
        "    plt.subplots_adjust(hspace=0.5, wspace=0.2)\n",
        "\n",
        "    sns.boxplot(x=df[ticker],color='pink', ax = ax)\n",
        "\n",
        "    # chart formatting\n",
        "    ax.set_title(ticker.upper())\n"
      ],
      "metadata": {
        "id": "DGTCoxgkaWIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def find_outliers_iqr(data):\n",
        "    # Calculate the first quartile (Q1) and third quartile (Q3) for each column\n",
        "    q1 = data.quantile(0.25)\n",
        "    q3 = data.quantile(0.75)\n",
        "\n",
        "    # Calculate the interquartile range (IQR) for each column\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    # Calculate the lower and upper bounds for outliers for each column\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "    # Check for outliers in each column and count the number of outliers\n",
        "    outliers_count = (data < lower_bound) | (data > upper_bound)\n",
        "    num_outliers = outliers_count.sum()\n",
        "\n",
        "    return num_outliers\n",
        "\n",
        "\n",
        "outliers_per_column = find_outliers_iqr(data)\n",
        "print(\"Number of outliers per column:\")\n",
        "print(outliers_per_column.sort_values(ascending = False))\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "for ftr in col_list:\n",
        "  print(ftr,'\\n')\n",
        "  q_25= np.percentile(df[ftr], 25)\n",
        "  q_75 = np.percentile(df[ftr], 75)\n",
        "  iqr = q_75 - q_25\n",
        "  print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q_25, q_75, iqr))\n",
        "  # calculate the outlier cutoff\n",
        "  cut_off = iqr * 1.5\n",
        "  lower = q_25 - cut_off\n",
        "  upper = q_75 + cut_off\n",
        "  print(f\"\\nlower = {lower} and upper = {upper} \\n \")\n",
        "  # identify outliers\n",
        "  outliers = [x for x in df[ftr] if x < lower or x > upper]\n",
        "  print('Identified outliers: %d' % len(outliers))\n",
        "  #removing outliers\n",
        "  if len(outliers)!=0:\n",
        "\n",
        "    def bin(row):\n",
        "      if row[ftr]> upper:\n",
        "        return upper\n",
        "      if row[ftr] < lower:\n",
        "        return lower\n",
        "      else:\n",
        "        return row[ftr]\n",
        "\n",
        "\n",
        "\n",
        "    data[ftr] =  df.apply (lambda row: bin(row), axis=1)\n",
        "    print(f\"{ftr} Outliers Removed\")\n",
        "  print(\"\\n-------\\n\")"
      ],
      "metadata": {
        "id": "gpoGcJ-laLK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25, 20))\n",
        "plt.suptitle(\"Box Plot without Outliers\", fontsize=18, y=0.95)\n",
        "#plot the all figures in loop with boxplot\n",
        "for n, ticker in enumerate(col_list):\n",
        "\n",
        "    ax = plt.subplot(8, 4, n + 1)\n",
        "\n",
        "    plt.subplots_adjust(hspace=0.5, wspace=0.2)\n",
        "\n",
        "    sns.boxplot(x=data[ticker],color='g' ,ax = ax)\n",
        "\n",
        "    # chart formatting\n",
        "    ax.set_title(ticker.upper())\n",
        ""
      ],
      "metadata": {
        "id": "ZrdPC2Srdk2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the shape after\n",
        "data.shape"
      ],
      "metadata": {
        "id": "iFTzKc6UcU8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# create new features\n",
        "# create a column average building temperature based on all temperature\n",
        "data['Average_building_Temperature']=data[['KITCHEN_TEMP','LIVING_TEMP','BEDROOM_TEMP','OFFICE_TEMP','BATHROOM_TEMP','IRONING_ROOM_TEMP','TEEN_ROOM_2_TEMP','PARENTS_ROOM_TEMP']].mean(axis=1)\n",
        "#create a column of difference between outside and inside temperature\n",
        "data['Temperature_difference']=abs(data['Average_building_Temperature']-data['OUTSIDE_TEMP_build'])\n",
        "\n",
        "#create a column average building humidity\n",
        "data['Average_building_humidity']=data[['KITCHEN_HUM','LIVING_HUM','BEDROOM_HUM', 'OFFICE_HUM','BATHROOM_HUM','IRONING_ROOM_HUM','TEEN_ROOM_HUM','PARENTS_ROOM_HUM']].mean(axis=1)\n",
        "#create a column of difference between outside and inside building humidity\n",
        "data['Humidity_difference']=abs(data['OUTSIDE_HUM_build']-data['Average_building_humidity'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "THq_WDcy0cvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop random variables as it does not look like that much important while predicting the output\n",
        "columns_to_drop = ['rv1','rv2']\n",
        "data.drop(columns_to_drop, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "C_kdVVTSKQRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "WD_vkrWMV8EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finding the skewed and symmetrical data"
      ],
      "metadata": {
        "id": "8N_Na8hJcJcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the skewness in the dataset to check the distribution\n",
        "skewness = data.skew()\n",
        "\n",
        "#ginding the absolute value\n",
        "abs(skewness)\n",
        "\n",
        "# setting up the threshold\n",
        "skewness_threshold = 0.5\n",
        "\n",
        "# Separate features into symmetrical and skewed based on skewness threshold\n",
        "symmetrical_features = skewness[abs(skewness) < skewness_threshold].index\n",
        "skewed_features = skewness[abs(skewness) >= skewness_threshold].index\n",
        "\n",
        "# Create new DataFrames for symmetrical and skewed features\n",
        "print('FEATURES FOLLOWED SYMMETRICAL DISTRIBUTION :')\n",
        "symmetrical_data = data[symmetrical_features]\n",
        "print(symmetrical_features)\n",
        "\n",
        "print('FEATURES FOLLOWED SKEWED DISTRIBUTION :')\n",
        "skewed_data = data[skewed_features]\n",
        "print(skewed_features)\n"
      ],
      "metadata": {
        "id": "1fSKxWkHCi-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the skewed data\n",
        "skewed_data"
      ],
      "metadata": {
        "id": "9f_lt6SWHEGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the liabrary\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# Initialize the PowerTransformer\n",
        "power_transformer = PowerTransformer()\n",
        "\n",
        "# Fit and transform the data using the PowerTransformer\n",
        "power_transformed = pd.DataFrame(power_transformer.fit_transform(skewed_data))\n",
        "power_transformed.columns = skewed_data.columns\n"
      ],
      "metadata": {
        "id": "XYFTzJnZHRA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the power transformed data\n",
        "power_transformed"
      ],
      "metadata": {
        "id": "_lWwTodiIVeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index to the default integer index\n",
        "symmetrical_data.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "wIsZBYKLKSxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the symmetrical data\n",
        "symmetrical_data"
      ],
      "metadata": {
        "id": "FqbmBhCgOSwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate horizontally (along columns)\n",
        "tranformed_data = pd.concat([symmetrical_data, power_transformed], axis=1)"
      ],
      "metadata": {
        "id": "-axrMYS-Ju78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the transformed data\n",
        "tranformed_data"
      ],
      "metadata": {
        "id": "XYxVg0eOJ7De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Yes My data needs transformation specially skewed data , i used power transformaiton to solve this concern"
      ],
      "metadata": {
        "id": "dMFDTrcIcfEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Scaling the DATA set"
      ],
      "metadata": {
        "id": "yxxTpRupBq2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the desired liabrary\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = pd.DataFrame(scaler.fit_transform(tranformed_data))\n",
        "scaled_data.columns = tranformed_data.columns\n",
        "scaled_data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction is a crucial technique in machine learning and data analysis, and it is often needed for several reasons:\n",
        "\n",
        "1. **Curse of Dimensionality:** As the number of features (dimensions) in your dataset increases, the volume of the data space grows exponentially. This can lead to sparsity, making it difficult to collect sufficient data to model relationships effectively. Dimensionality reduction can help mitigate this problem by reducing the number of features while retaining essential information.\n",
        "\n",
        "2. **Overfitting:** Models trained on high-dimensional data are more likely to overfit the training data, meaning they perform well on training data but poorly on unseen data. By reducing dimensionality, you can reduce the complexity of the model and enhance its generalization capabilities.\n",
        "\n",
        "3. **Computational Efficiency:** High-dimensional data can strain computational resources and increase the time required for training and prediction. Dimensionality reduction can make algorithms more efficient.\n",
        "\n",
        "4. **Visualization:** It's challenging to visualize and interpret data in high-dimensional spaces. Reducing dimensionality allows for more accessible data exploration and visualization.\n",
        "\n",
        "5. **Feature Engineering:** Some features may be redundant or irrelevant, and dimensionality reduction helps in identifying and removing them. This can improve model performance and understanding of data.\n",
        "\n",
        "6. **Collinearity:** High-dimensional data often contains multicollinearity, where features are highly correlated. Dimensionality reduction can alleviate this issue and help in extracting meaningful and uncorrelated features.\n",
        "\n",
        "Common techniques for dimensionality reduction include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and feature selection methods. The choice of technique depends on the specific dataset and the goals of the analysis. In summary, dimensionality reduction is needed to simplify high-dimensional data, improve model performance, and enhance the interpretability and efficiency of data analysis."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize a PCA instance without specifying the number of components\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA model to your standardized data\n",
        "pca.fit(scaled_data)\n",
        "\n",
        "# Calculate the cumulative explained variance\n",
        "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Create an elbow plot to visualize the explained variance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Elbow Plot')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Create a PCA instance and specify the number of components you want to retain\n",
        "# For example, if you want to retain 10 components, set n_components=10\n",
        "n_components = 10\n",
        "pca = PCA(n_components=n_components)\n",
        "\n",
        "# Fit the PCA model to your standardized data and transform it\n",
        "transformed_data_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "# The variable 'transformed_data_pca' now contains your data in the reduced-dimensional space with 'n_components' principal components.\n",
        "\n",
        "# You can also access explained variance to see how much variance is explained by each component\n",
        "explained_variance = pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "sHTojg5sUyJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the variances of the pca that we extract and there importance in predicting the output\n",
        "explained_variance"
      ],
      "metadata": {
        "id": "MtWCswByX8B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating the total of  explained_variance  which needs to be more than 90%\n",
        "explained_variance.sum()"
      ],
      "metadata": {
        "id": "dLNiowtirSjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize a PCA instance without specifying the number of components\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA model to your standardized data\n",
        "pca.fit(scaled_data)\n",
        "\n",
        "# Calculate the explained variance for each component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Create a scree plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance')\n",
        "plt.title('Scree Plot for PCA')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "u_lp1LmNxB5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the shape after pca\n",
        "transformed_data_pca.shape"
      ],
      "metadata": {
        "id": "e5IshvswYBBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_data_pca"
      ],
      "metadata": {
        "id": "vJSrc4FGVPKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#assinign the independent and dependent feature\n",
        "x = transformed_data_pca\n",
        "y = data['Appliances']"
      ],
      "metadata": {
        "id": "bNJtj0r0CDnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the data into 80/20 ration\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=3)"
      ],
      "metadata": {
        "id": "as7VuE-HPHsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 - Simple Linear Regression Model"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the mdoel\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#defining the object\n",
        "reg = LinearRegression()\n",
        "reg.fit(x_train, y_train)\n",
        "\n",
        "#training dataset score\n",
        "training_score = reg.score(x_train, y_train)\n",
        "\n",
        "#predicting the value\n",
        "y_pred = reg.predict(x_test)\n",
        "\n",
        "#calculating the training accuracy\n",
        "print(\"Train score:\" ,training_score)\n",
        "\n",
        "#calculating the MSE\n",
        "MSE  = mean_squared_error((y_test),(y_pred))\n",
        "print(\"Test MSE :\" , MSE)\n",
        "\n",
        "#calculating the testing accuracy\n",
        "r2 = r2_score((y_test),(y_pred))\n",
        "print(\"Test R2 :\" ,r2)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot to compare the predicted values against the actual values.\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3UPe56CAxrZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "o8NLZeV1a2uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model used is a Linear Regression model, a simple and interpretable machine learning algorithm. Linear Regression is used for predicting a continuous target variable based on one or more input features. It models the relationship between the input features and the target variable as a linear equation.\n",
        "\n",
        "Performance Evaluation:\n",
        "1. **Train Score (R-squared)**: The train score of approximately 0.679 indicates that the model explains about 67.9% of the variance in the training data. A higher R-squared is generally better, and this suggests that the model captures a significant portion of the variation in the data.\n",
        "\n",
        "2. **Test Mean Squared Error (MSE)**: The test MSE of approximately 568.88 measures the average squared difference between predicted and actual values. Lower MSE is desirable, and this value suggests that, on average, the model's predictions have a moderate error in the test data.\n",
        "\n",
        "3. **Test R-squared (R2)**: The test R2 score of about 0.684 implies that the model accounts for roughly 68.4% of the variance in the test data. A higher R2 score indicates a better fit to the test data.\n",
        "\n",
        "In summary, the Linear Regression model has a moderate level of predictive power. It explains a significant portion of the variance in both the training and test data. The model's performance is reasonable, but there may still be room for improvement in reducing the mean squared error for more accurate predictions. Further evaluation with additional metrics and potentially exploring more complex models may be considered for fine-tuning."
      ],
      "metadata": {
        "id": "jiC0wGYOed97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "\n",
        "# Create a Linear Regression model (you can replace this with any other regression model)\n",
        "model = LinearRegression()\n",
        "\n",
        "# Define hyperparameter search space (you can customize this based on your model)\n",
        "param_dist = {'fit_intercept': [True, False],\n",
        "              'copy_X': [True, False],\n",
        "              'positive':[True, False]}\n",
        "\n",
        "# Perform RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,\n",
        "                                   n_iter=10, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the RandomizedSearchCV to find the best hyperparameters\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Train the best model with the entire training dataset\n",
        "best_model.fit(x_train, y_train)\n",
        "\n",
        "training_score_val = best_model.score(x_train, y_train)\n",
        "# Evaluate the best model on the test set\n",
        "test_predictions = best_model.predict(x_test)\n",
        "\n",
        "# Calculate evaluation metrics for the test predictions (e.g., mean squared error)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(y_test, test_predictions)\n",
        "r2 = r2_score((y_test),(test_predictions))\n",
        "\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "\n",
        "#visual of training score\n",
        "print(\"Train score:\" ,training_score_val)\n",
        "print(\"Test MSE:\", mse)\n",
        "print(\"Test R2:\", r2)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.displot(test_predictions - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "SkDXF8syg0iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zUw-OVnIgpoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used in this case is \"RandomizedSearchCV.\"\n",
        "\n",
        "RandomizedSearchCV is chosen for several reasons:\n",
        "1. **Efficiency**: Compared to GridSearchCV, RandomizedSearchCV explores a random subset of hyperparameter combinations, making it more efficient when there are a large number of possible hyperparameter settings.\n",
        "\n",
        "2. **Exploration**: It provides a balance between random exploration and an exhaustive search. It randomly samples hyperparameters within specified ranges, which can be beneficial in discovering hidden, effective configurations.\n",
        "\n",
        "3. **Parallelization**: It allows parallel processing, using multiple CPU cores (specified by `n_jobs=-1`), which speeds up the search process.\n",
        "\n",
        "4. **Scoring**: The choice of 'neg_mean_squared_error' as the scoring metric indicates that the search aims to minimize the mean squared error, a common metric for regression tasks.\n",
        "\n",
        "RandomizedSearchCV efficiently explores a range of hyperparameter settings, leading to the discovery of a set of hyperparameters that perform well. In this case, it led to a model with the same training score as the initial model, indicating that the optimal hyperparameters did not significantly improve the model's performance. However, it's a valuable technique to systematically search for optimal hyperparameters and can lead to substantial performance improvements in other cases."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon performing hyperparameter optimization using RandomizedSearchCV, there doesn't seem to be a significant improvement in model performance compared to the initial model. The training score remains the same, and the evaluation metrics on the test data also show similar values. The Mean Squared Error (MSE) and R-squared (R2) values remain approximately unchanged. This suggests that the initial model's hyperparameters were already reasonably effective, and the hyperparameter search did not lead to noticeable enhancements in this particular case. Further exploration or considering different models may be necessary to achieve substantial improvements."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "4DGpWFhQja7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each evaluation metric provides valuable insights into the model's performance and its potential impact on the business:\n",
        "\n",
        "1. **Training Score (R-squared)**:\n",
        "   - **Indication**: R-squared measures the proportion of variance in the target variable explained by the model. A high R-squared indicates a good fit to the training data.\n",
        "   - **Business Implication**: A high training R-squared suggests that the model captures a significant portion of the variance in the training data. This can be beneficial for understanding the relationships between input features and the target variable within the business context. However, a very high R-squared could indicate overfitting, which may not generalize well to new data.\n",
        "\n",
        "2. **Test Mean Squared Error (MSE)**:\n",
        "   - **Indication**: MSE quantifies the average squared difference between the model's predictions and actual values in the test data. Lower MSE values are desirable.\n",
        "   - **Business Implication**: A lower test MSE indicates that the model's predictions are closer to the actual values on average. This suggests that the model's predictions have a smaller error, making it more reliable for business applications. Reduced errors can lead to cost savings, improved decision-making, and better resource allocation.\n",
        "\n",
        "3. **Test R-squared (R2)**:\n",
        "   - **Indication**: R2 on the test data measures how well the model explains the variance in new, unseen data. A higher R2 indicates better predictive power.\n",
        "   - **Business Implication**: A high test R2 suggests that the model generalizes well to new data, making it valuable for making predictions in a business context. It signifies that the model maintains its predictive performance beyond the training data, which can lead to more accurate forecasting, better resource planning, and improved business outcomes.\n",
        "\n",
        "In summary, while the model shows good performance on the training data, indicating a strong understanding of relationships within that dataset, it's equally important to evaluate its performance on test data. The low test MSE and high test R2 suggest that the model is reasonably accurate and generalizes well. The business impact includes improved decision-making, better resource allocation, and potentially cost savings, as the model's predictions align closely with the actual values, benefiting various business applications. However, continued monitoring and potential model refinement may be necessary to ensure long-term business success."
      ],
      "metadata": {
        "id": "OrGhlayXjnJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - Polynomial Regression model\n"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already split your data into x_train, x_test, y_train, and y_test\n",
        "\n",
        "# Choose the degree of the polynomial (e.g., 2 for quadratic)\n",
        "degree = 2\n",
        "\n",
        "# Create a Polynomial Regression model using a pipeline\n",
        "polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "\n",
        "# Fit the model to the training data\n",
        "polyreg.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = polyreg.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Calculate the R2 score for the training data\n",
        "training_r2 = polyreg.score(x_train, y_train)\n",
        "\n",
        "print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared (R2) Score: {r2:.2f}\")"
      ],
      "metadata": {
        "id": "ymH6IB2GjXg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "kmERsQaXYTc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xUrAeYExZ_Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "WVTu0r-3tWqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used in this scenario is a Polynomial Regression model with a degree of 2. Polynomial Regression is a type of linear regression where polynomial features are generated from the original features to capture more complex relationships between the independent and dependent variables.\n",
        "\n",
        "Performance Evaluation:\n",
        "\n",
        "1. **Training R-squared (R2) Score:** The training R2 score of approximately 0.68 indicates that the model explains about 68% of the variance in the training data, suggesting a moderate fit.\n",
        "\n",
        "2. **Mean Squared Error (MSE):** The MSE of approximately 568.88 represents the average squared difference between the predicted and actual values. Lower MSE values are better, indicating that the model's predictions are relatively close to the actual values.\n",
        "\n",
        "3. **Test R-squared (R2) Score:** The test R2 score of approximately 0.68 is consistent with the training R2 score, suggesting that the model generalizes well to unseen data.\n",
        "\n",
        "In summary, the Polynomial Regression model performs moderately well, explaining a significant portion of the variance in both the training and test datasets. The low MSE indicates relatively accurate predictions. The consistent training and test R2 scores suggest that the model is not overfitting and can make reliable predictions on new data."
      ],
      "metadata": {
        "id": "zxinTGqUtbuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "KcEF3md1kE4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "\n",
        "# Create a Polynomial Regression model without specifying the degree\n",
        "polyreg = make_pipeline(PolynomialFeatures(), LinearRegression())\n",
        "\n",
        "# Define a range of polynomial degrees to be tested\n",
        "param_grid = {'polynomialfeatures__degree': range(1, 3)}\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(polyreg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best polynomial degree\n",
        "best_degree = grid_search.best_params_['polynomialfeatures__degree']\n",
        "\n",
        "# Create a Polynomial Regression model with the best degree\n",
        "best_polyreg = make_pipeline(PolynomialFeatures(degree=best_degree), LinearRegression())\n",
        "\n",
        "# Perform cross-validation to evaluate the model\n",
        "cv_scores = cross_val_score(best_polyreg, x_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "cv_r2_scores = cross_val_score(best_polyreg, x_train, y_train, cv=5, scoring='r2')\n",
        "\n",
        "# Calculate the mean squared error and R2 score\n",
        "mse_cv = -cv_scores.mean()\n",
        "r2_cv = cv_r2_scores.mean()\n",
        "\n",
        "# Fit the best model to the training data\n",
        "best_polyreg.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = best_polyreg.predict(x_test)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Polynomial Degree: {best_degree}\")\n",
        "print(f\"Cross-Validation Mean Squared Error: {mse_cv:.2f}\")\n",
        "print(f\"Cross-Validation R-squared (R2) Score: {r2_cv:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared (R2) Score: {r2:.2f}\")"
      ],
      "metadata": {
        "id": "dyEaycxPXV0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "tb8NDSy7jq3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KT0g7g0si6jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "7Kmb0xk8jvH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we used the GridSearchCV technique for hyperparameter optimization. Here's why it was chosen:\n",
        "\n",
        "GridSearchCV systematically searches through a predefined hyperparameter grid, in this case, the polynomial degree of a Polynomial Regression model, to find the best combination that minimizes the chosen scoring metric (in this case, negative mean squared error). This technique was chosen for several reasons:\n",
        "\n",
        "1. **Exhaustive Search:** GridSearchCV explores all possible combinations of hyperparameters within the specified grid, ensuring that no promising settings are missed. This is particularly valuable when you don't have prior knowledge about the best hyperparameter values.\n",
        "\n",
        "2. **Automated Hyperparameter Tuning:** It automates the process of hyperparameter tuning, saving time and reducing the risk of manual errors.\n",
        "\n",
        "3. **Cross-Validation:** GridSearchCV uses cross-validation to estimate the model's performance with different hyperparameter settings, providing a more reliable assessment of how the model is likely to perform on unseen data.\n",
        "\n",
        "4. **Objective Optimization:** It optimizes hyperparameters based on a specified scoring metric, which allows you to tailor the model to achieve your specific goals. In this case, it minimizes mean squared error, which is a common choice for regression problems.\n",
        "\n",
        "5. **Ease of Use:** GridSearchCV is easy to implement and integrate into your machine learning workflow using Scikit-Learn.\n",
        "\n",
        "In summary, GridSearchCV is a systematic and effective technique for finding the best hyperparameters for your model, ensuring that it performs well on both the training data and unseen data. It's widely used in machine learning because of its simplicity and robustness in hyperparameter optimization."
      ],
      "metadata": {
        "id": "VdZDbferjx5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this scenario, you applied cross-validation to a Polynomial Regression model with a degree of 2, and here's the comparison between the original model and the cross-validated model:\n",
        "\n",
        "Original Model:\n",
        "- Training R-squared (R2) Score: 0.78\n",
        "- Mean Squared Error: 404.51\n",
        "- Test R-squared (R2) Score: 0.78\n",
        "\n",
        "Cross-Validated Model:\n",
        "- Cross-Validation Mean Squared Error: 415.34\n",
        "- Cross-Validation R-squared (R2) Score: 0.78\n",
        "- Mean Squared Error: 404.51\n",
        "- Test R-squared (R2) Score: 0.78\n",
        "\n",
        "It appears that the cross-validated model has a slightly higher Cross-Validation Mean Squared Error (415.34) compared to the original model's Mean Squared Error (404.51). However, the R-squared (R2) scores for both the original model and the cross-validated model are the same (0.78).\n",
        "\n",
        "In this case, the cross-validated model doesn't show a significant improvement in terms of Cross-Validation Mean Squared Error or R-squared (R2) score compared to the original model. The values are quite similar. Cross-validation is typically used to provide a more reliable estimate of a model's performance and assess its generalization to unseen data. In this instance, it confirms that the original model's performance is consistent across different folds of the data."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "i2pN9ajttoKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's analyze each of the evaluation metrics in the context of a business problem and discuss their potential business impact based on the Polynomial Regression model with a degree of 2:\n",
        "\n",
        "1. **Training R-squared (R2) Score (0.78):**\n",
        "   - **Indication:** R-squared measures the proportion of the variance in the dependent variable that is explained by the independent variables. An R2 score of 0.78 indicates that approximately 78% of the variance in the target variable is accounted for by the model.\n",
        "   - **Business Impact:** This high R-squared score suggests that the model captures a significant portion of the underlying patterns in the data. The business impact is that the model is relatively effective in explaining and predicting the target variable, which can be valuable for decision-making and forecasting.\n",
        "\n",
        "2. **Mean Squared Error (MSE) (404.51):**\n",
        "   - **Indication:** MSE quantifies the average squared difference between predicted and actual values. A lower MSE is desirable as it indicates that the model's predictions are closer to the actual values.\n",
        "   - **Business Impact:** An MSE of 404.51 means that, on average, the model's predictions have a squared error of this value. Lower MSE implies that the model is making more accurate predictions, which can lead to cost savings, better resource allocation, and improved decision-making in various business applications.\n",
        "\n",
        "3. **R-squared (R2) Score (0.78):**\n",
        "   - **Indication:** The R-squared score on the test data confirms the model's ability to explain the variance in unseen data, similar to the training R2.\n",
        "   - **Business Impact:** Consistency between training and test R2 scores (0.78) indicates that the model generalizes well and is not overfitting. This means that the model can make reliable predictions on new, unseen data, which is crucial for making informed business decisions.\n",
        "\n",
        "In summary, the evaluated Polynomial Regression model (degree = 2) exhibits strong performance metrics, suggesting that it effectively captures the underlying relationships in the data. The business impact of this model includes the ability to make accurate predictions, explain variance in the target variable, and generalize well to new data. This can lead to improved decision-making, better resource allocation, and potentially cost savings in various business applications, such as sales forecasting, risk assessment, or quality control."
      ],
      "metadata": {
        "id": "PYDbdWHWtpt2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 - RIDGE Regression Model"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already created the 'x_train', 'x_test', 'y_train', and 'y_test' datasets\n",
        "# 'x_train' and 'x_test' are the results of polynomial regression on PCA-transformed data\n",
        "\n",
        "# Create a PolynomialFeatures instance (with degree=2 for quadratic features)\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Transform the data to include polynomial features\n",
        "x_train_poly = poly_features.fit_transform(x_train)\n",
        "x_test_poly = poly_features.transform(x_test)\n",
        "\n",
        "# Create a Ridge regression model\n",
        "ridge_reg = Ridge(alpha=1.0)  # You can adjust the alpha parameter (regularization strength)\n",
        "\n",
        "# Fit the Ridge model to the training data\n",
        "ridge_reg.fit(x_train_poly, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = ridge_reg.predict(x_test_poly)\n",
        "\n",
        "# Calculate R-squared (R2) for the test data\n",
        "test_r2 = ridge_reg.score(x_test_poly, y_test)\n",
        "\n",
        "# Calculate R-squared (R2) for the training data\n",
        "training_r2 = ridge_reg.score(x_train_poly, y_train)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for the test data\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(f\"Test R-squared (R2) Score: {test_r2:.2f}\")\n",
        "print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")"
      ],
      "metadata": {
        "id": "gxks1j-natPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "JlupRzxBAIqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V0hHNR6qAQp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TRu5wzPguD_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used is Ridge Regression with polynomial features (degree=2). Here's the explanation and performance summary based on the provided evaluation metrics:\n",
        "\n",
        "**Model Explanation:**\n",
        "Ridge Regression is a linear regression technique that includes L2 regularization to prevent overfitting. It's combined with Polynomial Features, which generates new features by considering interactions between variables. The degree=2 indicates quadratic features.\n",
        "\n",
        "**Performance Evaluation:**\n",
        "1. **Test R-squared (R2) Score: 0.78**\n",
        "   - The R2 score measures the proportion of variance in the target variable explained by the model.\n",
        "   - A score of 0.78 indicates that approximately 78% of the variance is accounted for, suggesting the model's effectiveness in explaining test data.\n",
        "\n",
        "2. **Training R-squared (R2) Score: 0.78**\n",
        "   - Consistency between training and test R2 scores (0.78) suggests that the model generalizes well to new data and is not overfitting.\n",
        "\n",
        "3. **Mean Squared Error (MSE): 404.51**\n",
        "   - MSE quantifies the average squared difference between predicted and actual values.\n",
        "   - A lower MSE (404.51) implies accurate predictions, with predictions on average being close to actual values.\n",
        "\n",
        "**Model Performance:**\n",
        "- The model exhibits strong predictive capabilities, explaining 78% of the variance in the test data.\n",
        "- Consistency between training and test R2 scores indicates reliable generalization.\n",
        "- The low MSE demonstrates accurate predictions, which can lead to improved decision-making and resource allocation.\n",
        "\n",
        "Overall, this Ridge Regression model with polynomial features (degree=2) performs well and can be valuable in applications like predictive modeling, where understanding relationships between variables is crucial for informed decision-making."
      ],
      "metadata": {
        "id": "FzWbYwX0uH3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n"
      ],
      "metadata": {
        "id": "aSkQDwTWxvud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have 'x' and 'y' as your data and target variable\n",
        "\n",
        "# Create a PolynomialFeatures instance (with degree=3 for cubic features)\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Create a Ridge regression model\n",
        "ridge_reg = Ridge()\n",
        "\n",
        "# Create a pipeline with the polynomial features and Ridge regression\n",
        "pipeline = Pipeline([\n",
        "    ('polynomial_features', poly_features),\n",
        "    ('ridge_regression', ridge_reg)\n",
        "])\n",
        "\n",
        "# Define hyperparameters and values to search\n",
        "param_grid = {\n",
        "    'ridge_regression__alpha': [0.001, 0.01, 0.1, 1]  # You can adjust the alpha values\n",
        "}\n",
        "\n",
        "# Perform Grid Search with Cross-Validation\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(x, y)  # Use the full dataset for cross-validation\n",
        "\n",
        "# Get the best hyperparameters from the grid search\n",
        "best_alpha = grid_search.best_params_['ridge_regression__alpha']\n",
        "\n",
        "# Create a Ridge regression model with the best hyperparameters\n",
        "best_ridge_reg = Ridge(alpha=best_alpha)\n",
        "\n",
        "# Fit the Ridge model to the training data\n",
        "best_ridge_reg.fit(x_train, y_train)\n",
        "\n",
        "# Calculate cross-validated R-squared (R2) scores\n",
        "cv_scores = cross_val_score(best_ridge_reg, x_train, y_train, cv=5, scoring='r2')\n",
        "\n",
        "# Calculate R-squared (R2) score on the test data\n",
        "test_r2 = best_ridge_reg.score(x_test, y_test)\n",
        "\n",
        "print(f\"Best Alpha: {best_alpha}\")\n",
        "print(f\"Cross-Validated R-squared (R2) Scores: {cv_scores}\")\n",
        "print(f\"Mean R-squared (R2) Score: {np.mean(cv_scores):.2f}\")\n",
        "print(f\"Training R-squared (R2) Score: {best_ridge_reg.score(x_train, y_train):.2f}\")\n",
        "print(f\"Test R-squared (R2) Score: {test_r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "sxNXg6XccnYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "z-qh-ma1p6yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dF-mi86ygzrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Which hyperparameter optimization technique have you used and why?\n"
      ],
      "metadata": {
        "id": "YKyOrusqoWV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the **GridSearchCV** technique for hyperparameter optimization. Here's why it was chosen:\n",
        "\n",
        "1. **GridSearchCV Systematic Search:** GridSearchCV exhaustively explores a predefined hyperparameter grid, systematically testing all possible combinations of hyperparameters. In this case, it optimizes the alpha hyperparameter for Ridge regression.\n",
        "\n",
        "2. **Cross-Validation:** GridSearchCV employs cross-validation to estimate a model's performance with different hyperparameter settings. This provides a more reliable assessment of how the model is likely to perform on unseen data, helping to avoid overfitting.\n",
        "\n",
        "3. **Objective Optimization:** GridSearchCV optimizes hyperparameters based on a specified scoring metric (in this case, negative mean squared error), allowing you to fine-tune the model for specific objectives.\n",
        "\n",
        "4. **Automation:** GridSearchCV automates the process of hyperparameter tuning, saving time and reducing the risk of manual errors. It's a convenient and widely-used tool in machine learning workflows.\n",
        "\n",
        "In summary, GridSearchCV was chosen for its systematic search, cross-validation, and objective optimization capabilities. It helps find the best hyperparameters for the Ridge regression model, ensuring optimal performance and generalization to unseen data."
      ],
      "metadata": {
        "id": "i78QWwwWoewU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "GJgYOywluOR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two models being compared are Polynomial Ridge Regression (degree=2) and Ridge Regression with different hyperparameters. Let's analyze the improvement:\n",
        "\n",
        "**Polynomial Ridge Regression (degree=2):**\n",
        "- **Cross-Validated R-squared (R2) Scores**: The model achieves a mean R2 score of 0.68 across cross-validation folds. This indicates its ability to explain about 68% of the variance in the data.\n",
        "- **Training R-squared (R2) Score**: The consistency between training and cross-validation R2 scores (both 0.68) suggests the model generalizes well.\n",
        "- **Test R-squared (R2) Score**: The test R2 score is also 0.68, indicating the model's capability to predict unseen data effectively.\n",
        "- **Cross-Validation Mean Squared Error (415.34)**: The model exhibits a reasonable performance with an average MSE, signifying the average squared difference between predictions and actual values.\n",
        "- **R-squared (R2) Score (0.78)**: This score of 0.78 in the original model showcases a higher ability to explain variance, suggesting a relatively strong model.\n",
        "\n",
        "**Comparison:**\n",
        "- The Polynomial Ridge model maintains consistent R2 scores between training, cross-validation, and test datasets, indicating good generalization.\n",
        "- However, the original model with Ridge Regression, without polynomial features, exhibits a higher cross-validation R2 score (0.78), indicating better explanation of variance. This might be due to its flexibility in capturing complex relationships with the right hyperparameters.\n",
        "\n",
        "**Summary:**\n",
        "- The Polynomial Ridge model offers decent performance, but it doesn't surpass the original Ridge Regression model in terms of cross-validated R2 scores.\n",
        "- The original model, with careful hyperparameter tuning, shows a better ability to explain variance and generalizes well to unseen data.\n",
        "- Therefore, the original Ridge Regression model, with appropriate hyperparameters, appears to be the stronger choice for this particular task.\n",
        "\n"
      ],
      "metadata": {
        "id": "rOe2CVTauPe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "-JOW0zRat3TE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The significance of each evaluation metric in the context of a business problem and the potential business impact of the Ridge Regression model with Polynomial Features (degree=2):\n",
        "\n",
        "1. **Cross-Validated R-squared (R2) Scores (0.68)**:\n",
        "   - **Indication:** R-squared measures the proportion of variance in the target variable explained by the model. A score of 0.68 suggests that the model can explain about 68% of the variance in the data.\n",
        "   - **Business Impact:** This level of explanation of variance can be valuable for businesses in decision-making. It means the model captures a significant portion of the underlying patterns, aiding in understanding and forecasting.\n",
        "\n",
        "2. **Cross-Validation Mean Squared Error (415.34)**:\n",
        "   - **Indication:** MSE quantifies the average squared difference between predicted and actual values. Lower MSE is desirable as it indicates accurate predictions.\n",
        "   - **Business Impact:** The model's ability to achieve a lower MSE (415.34) implies accurate predictions, which can lead to cost savings, resource allocation efficiency, and reduced errors in various business processes.\n",
        "\n",
        "3. **R-squared (R2) Score (0.78)**:\n",
        "   - **Indication:** The R2 score on the original model indicates that approximately 78% of the variance in the target variable is explained. It represents a strong level of explanation.\n",
        "   - **Business Impact:** A high R2 score means the model provides valuable insights for businesses. It aids in making informed decisions, optimizing operations, and forecasting outcomes.\n",
        "\n",
        "**Overall Business Impact:**\n",
        "- The Ridge Regression model with Polynomial Features (degree=2) demonstrates its value in business applications by effectively explaining variance and providing accurate predictions.\n",
        "- The model's ability to generalize well ensures it can perform reliably on unseen data, a crucial aspect in practical business scenarios.\n",
        "- By utilizing this model, businesses can make data-driven decisions, improve resource allocation, reduce costs, and enhance their overall efficiency and effectiveness.\n",
        "\n",
        "In summary, the Ridge Regression model offers businesses the potential for improved decision-making, more accurate forecasting, and better utilization of resources, ultimately leading to enhanced operational performance and competitiveness.\n"
      ],
      "metadata": {
        "id": "ZSR2IjzGt4sL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4 - Lasso Regression Model"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already created the 'x_train', 'x_test', 'y_train', and 'y_test' datasets\n",
        "# 'x_train' and 'x_test' are the results of polynomial regression on PCA-transformed data\n",
        "\n",
        "# Create a PolynomialFeatures instance (with degree=3 for cubic features)\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Transform the data to include polynomial features\n",
        "x_train_poly = poly_features.fit_transform(x_train)\n",
        "x_test_poly = poly_features.transform(x_test)\n",
        "\n",
        "# Create a Lasso regression model\n",
        "lasso_reg = Lasso(alpha=1.0)  # You can adjust the alpha parameter (regularization strength)\n",
        "\n",
        "# Fit the Lasso model to the training data\n",
        "lasso_reg.fit(x_train_poly, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = lasso_reg.predict(x_test_poly)\n",
        "\n",
        "# Calculate R-squared (R2) for the test data\n",
        "test_r2 = lasso_reg.score(x_test_poly, y_test)\n",
        "\n",
        "# Calculate R-squared (R2) for the training data\n",
        "training_r2 = lasso_reg.score(x_train_poly, y_train)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for the test data\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"Test R-squared (R2) Score: {test_r2:.2f}\")\n",
        "print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "vGUJFicTB4Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t724bczkg1YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Tbrj6Z5UHvJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_WfirAn8HwZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "FV4bbI3rBx1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have 'x' and 'y' as your data and target variable\n",
        "\n",
        "# Split the data into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=3)\n",
        "\n",
        "# Create a PolynomialFeatures instance (with degree=3 for cubic features)\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Transform the data to include polynomial features\n",
        "x_train_poly = poly_features.fit_transform(x_train)\n",
        "x_test_poly = poly_features.transform(x_test)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "x_train_poly = scaler.fit_transform(x_train_poly)\n",
        "x_test_poly = scaler.transform(x_test_poly)\n",
        "\n",
        "# Create a Lasso regression model\n",
        "lasso_reg = Lasso(max_iter=10000)  # Increase max_iter and adjust the alpha parameter if needed\n",
        "\n",
        "# Define hyperparameters and values to search\n",
        "param_grid = {\n",
        "    'alpha': [0.001, 0.01, 0.1, 1, 10]  # You can adjust the alpha values\n",
        "}\n",
        "\n",
        "# Perform Grid Search with Cross-Validation\n",
        "grid_search = GridSearchCV(lasso_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(x_train_poly, y_train)  # Use the training data for cross-validation\n",
        "\n",
        "# Get the best hyperparameters from the grid search\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "\n",
        "# Create a Lasso regression model with the best hyperparameters\n",
        "best_lasso_reg = Lasso(alpha=best_alpha, max_iter=10000)\n",
        "\n",
        "# Fit the Lasso model to the training data\n",
        "best_lasso_reg.fit(x_train_poly, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = best_lasso_reg.predict(x_test_poly)\n",
        "\n",
        "# Calculate R-squared (R2) for the test data\n",
        "test_r2 = best_lasso_reg.score(x_test_poly, y_test)\n",
        "\n",
        "# Calculate R-squared (R2) for the training data\n",
        "training_r2 = best_lasso_reg.score(x_train_poly, y_train)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for the test data\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for the training data\n",
        "training_mse = mean_squared_error(y_train, best_lasso_reg.predict(x_train_poly))\n",
        "\n",
        "# Calculate cross-validated R-squared (R2) scores\n",
        "cv_scores = cross_val_score(best_lasso_reg, x, y, cv=5, scoring='r2')\n",
        "\n",
        "print(f\"Best Alpha: {best_alpha}\")\n",
        "print(f\"Test R-squared (R2) Score: {test_r2:.2f}\")\n",
        "print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Training Mean Squared Error (MSE): {training_mse:.2f}\")\n",
        "print(f\"Cross-Validated R-squared (R2) Scores: {cv_scores}\")\n",
        "print(f\"Mean R-squared (R2) Score: {np.mean(cv_scores):.2f}\")\n"
      ],
      "metadata": {
        "id": "HefHlXLpIh0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "j_DT8rtIB_KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hdY1YGc2g2uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "pgZGPjukCDkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8ginfcseCEaq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "IWkQP2VfCFHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6pNe9vzvCF5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "2WKputEICGu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aVMr4s3eCHd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5 - elastic net Regression Model"
      ],
      "metadata": {
        "id": "SAu0wU95I9lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import  ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
        "\n",
        "# Specify the degree of polynomial (you can change this based on your data)\n",
        "degree = 2\n",
        "\n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=degree)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "X_test_poly = poly_features.transform(X_test)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "ElasticNet_model = ElasticNet(alpha=1.0)\n",
        "\n",
        "# Train the model using the polynomial features\n",
        "ElasticNet_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = ElasticNet_model.predict(X_train_poly)\n",
        "test_predictions = ElasticNet_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Polynomial Regression (Degree {}):\".format(degree))\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)"
      ],
      "metadata": {
        "id": "A7aJUHSjJEc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "p8bCsNqFCoMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1LjuNiUfg39K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "FCZ-Wy25Hz_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JBumnRdzH1OQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "Tyd3H1_2C6Nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create a Ridge Regression model\n",
        "ElasticNet_model = ElasticNet()\n",
        "\n",
        "# Perform Cross-Validation and Hyperparameter Tuning\n",
        "param_grid = {'alpha': [0.1, 1.0, 10.0]}  # Define the hyperparameter grid\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=ElasticNet_model, param_grid=param_grid,\n",
        "                           scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV to find the best degree and alpha\n",
        "grid_search.fit(X_train_poly, y_train)\n",
        "\n",
        "# Get the best degree and alpha from the GridSearchCV results\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = best_model.predict(X_train_poly)\n",
        "test_predictions = best_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Best Alpha:\", best_alpha)\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)"
      ],
      "metadata": {
        "id": "X57hdecTOHvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "mg0xbnKJC-x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IViw6ojTg48L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "ZxM-XvGaDFSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UGZLq4sSDGyU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "CFKCCe3HDI4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7h8OkPT9DL50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "x06JEx1rDOR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DLBIqZe3DP38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 6 - Ranfom Forest Regressor"
      ],
      "metadata": {
        "id": "iPocHRDIgkRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Create a Random Forest Regressor model\n",
        "rf_model = RandomForestRegressor(n_estimators=20, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions_rf = rf_model.predict(X_train_poly)\n",
        "test_predictions_rf = rf_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse_rf = mean_squared_error(y_train, train_predictions_rf)\n",
        "test_mse_rf = mean_squared_error(y_test, test_predictions_rf)\n",
        "\n",
        "train_r2_rf = r2_score(y_train, train_predictions_rf)\n",
        "test_r2_rf = r2_score(y_test, test_predictions_rf)\n",
        "\n",
        "print(\"Random Forest Regressor:\")\n",
        "print(\"Train MSE:\", train_mse_rf)\n",
        "print(\"Test MSE:\", test_mse_rf)\n",
        "print(\"Train R-squared:\", train_r2_rf)\n",
        "print(\"Test R-squared:\", test_r2_rf)"
      ],
      "metadata": {
        "id": "CvU_uznThPU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_rf - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "BQakyycMEBL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions_rf)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YFoh7Fe4g54R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "e9JNG_aFH4P0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XQ8oo4OSH5Z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "dvH1l01QEPfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Create a Random Forest Regressor model\n",
        "rf_model = RandomForestRegressor(n_estimators=20, random_state=42)\n",
        "\n",
        "# Define scoring functions\n",
        "scoring = {\n",
        "    'mse': make_scorer(mean_squared_error),\n",
        "    'r2': make_scorer(r2_score)}\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "test_predictions_rf = rf_model.predict(X_test_poly)\n",
        "\n",
        "# Calculate Test MSE and Test R-squared\n",
        "test_mse_rf = mean_squared_error(y_test, test_predictions_rf)\n",
        "test_r2_rf = r2_score(y_test, test_predictions_rf)\n",
        "\n",
        "# Perform cross-validation\n",
        "k = 5  # Number of folds (you can adjust this as needed)\n",
        "mse_scores = -cross_val_score(rf_model, X_train_poly, y_train, cv=k, scoring=scoring['mse'])\n",
        "r2_scores = cross_val_score(rf_model, X_train_poly, y_train, cv=k, scoring=scoring['r2'])\n",
        "\n",
        "# Calculate the mean and standard deviation of MSE and R-squared\n",
        "mean_mse = np.mean(mse_scores)\n",
        "mean_r2 = np.mean(r2_scores)\n",
        "\n",
        "# Print the cross-validation results\n",
        "print(\"Cross-Validation Results for Random Forest Regressor:\")\n",
        "print(f\"Train MSE: {mean_mse:.2f} \")\n",
        "print(f\"Train R-squared: {mean_r2:.2f} \")\n",
        "print(f\"Test MSE: {test_mse_rf:.2f}\")\n",
        "print(f\"Test R-squared: {test_r2_rf:.2f}\")\n"
      ],
      "metadata": {
        "id": "CElotrAA91r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_rf - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "KvAFuKIuoTQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions_rf)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IvQHTP0zg7IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "NpHoaRqNEZvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2pIFBm-GEZqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ucueqBQuEZa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v8LXqxCOEZL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "cH7-EUflEY2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Vtk-F3vYEYdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 7 - GRADIENT BOOSTING"
      ],
      "metadata": {
        "id": "cv3Xato39JSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Create a Gradient Boosting Regressor model\n",
        "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gb_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions_gb = gb_model.predict(X_train_poly)\n",
        "test_predictions_gb = gb_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse_gb = mean_squared_error(y_train, train_predictions_gb)\n",
        "test_mse_gb = mean_squared_error(y_test, test_predictions_gb)\n",
        "\n",
        "train_r2_gb = r2_score(y_train, train_predictions_gb)\n",
        "test_r2_gb = r2_score(y_test, test_predictions_gb)\n",
        "\n",
        "print(\"Gradient Boosting Regressor:\")\n",
        "print(\"Train MSE:\", train_mse_gb)\n",
        "print(\"Test MSE:\", test_mse_gb)\n",
        "print(\"Train R-squared:\", train_r2_gb)\n",
        "print(\"Test R-squared:\", test_r2_gb)"
      ],
      "metadata": {
        "id": "EhZ-3mgo84eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_gb - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "qord_ocNoMbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions_gb)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Eq7bV1jOhELY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "V97fnq-UH9TM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u1po2DT1H-fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "c3p3tc8oE6hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # We can use this code snippet for cross validation\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from sklearn.ensemble import GradientBoostingRegressor\n",
        "# import numpy as np\n",
        "\n",
        "# # Create a Gradient Boosting Regressor\n",
        "# gb_model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# # Define a parameter grid for hyperparameter tuning\n",
        "# param_grid = {\n",
        "#     'n_estimators': [50, 100],\n",
        "#     'learning_rate': [0.01, 0.1],\n",
        "#     'max_depth': [3, 4]\n",
        "# }\n",
        "\n",
        "# # Create a GridSearchCV object with 5-fold cross-validation\n",
        "# grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# # Fit the GridSearchCV object on your data\n",
        "# grid_search.fit(X_train_poly, y_train)\n",
        "\n",
        "# # Get the best model from the search\n",
        "# best_gb_model = grid_search.best_estimator_\n",
        "\n",
        "# # Make predictions using the best model\n",
        "# train_predictions_gb = best_gb_model.predict(X_train_poly)\n",
        "# test_predictions_gb = best_gb_model.predict(X_test_poly)\n",
        "\n",
        "# # Evaluate the best model\n",
        "# train_mse_gb = mean_squared_error(y_train, train_predictions_gb)\n",
        "# test_mse_gb = mean_squared_error(y_test, test_predictions_gb)\n",
        "# train_r2_gb = r2_score(y_train, train_predictions_gb)\n",
        "# test_r2_gb = r2_score(y_test, test_predictions_gb)\n",
        "\n",
        "# print(\"Best Gradient Boosting Regressor after hyperparameter tuning:\")\n",
        "# print(\"Train MSE:\", train_mse_gb)\n",
        "# print(\"Test MSE:\", test_mse_gb)\n",
        "# print(\"Train R-squared:\", train_r2_gb)\n",
        "# print(\"Test R-squared:\", test_r2_gb)\n",
        "\n",
        "# # Print the best hyperparameters\n",
        "# print(\"Best Hyperparameters:\", grid_search.best_params_)"
      ],
      "metadata": {
        "id": "6wlmVFoxcPaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_gb - y_test,kind ='kde')'''"
      ],
      "metadata": {
        "id": "R-7t3GSxE9UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions_gb)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "x-aY_9nxE8mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "Rn3fbW0SHC6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fXAsogwHHCtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "E9vB6NbnHCge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ONGrH8hcHCTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "S69i9WjGHCHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZW-18VKBHB60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 8 - XGBOOST"
      ],
      "metadata": {
        "id": "54RxJBLH9Gq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Create an XGBoost Regressor model\n",
        "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions_xgb = xgb_model.predict(X_train_poly)\n",
        "test_predictions_xgb = xgb_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse_xgb = mean_squared_error(y_train, train_predictions_xgb)\n",
        "test_mse_xgb = mean_squared_error(y_test, test_predictions_xgb)\n",
        "\n",
        "train_r2_xgb = r2_score(y_train, train_predictions_xgb)\n",
        "test_r2_xgb = r2_score(y_test, test_predictions_xgb)\n",
        "\n",
        "print(\"XGBoost Regressor:\")\n",
        "print(\"Train MSE:\", train_mse_xgb)\n",
        "print(\"Test MSE:\", test_mse_xgb)\n",
        "print(\"Train R-squared:\", train_r2_xgb)\n",
        "print(\"Test R-squared:\", test_r2_xgb)\n",
        "\n"
      ],
      "metadata": {
        "id": "C2xGYM3n9DdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_xgb - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "v9VM8ddonWJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2.\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions_xgb)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xCVT4J1jpBJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JrnlmOb1IA1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xb9Pnwm1IB0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "uLwJvLC9FU2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [100],  # You can specify other values\n",
        "    'learning_rate': [0.01, 0.1, 0.2],  # Learning rate\n",
        "    # Add more hyperparameters as needed\n",
        "}\n",
        "\n",
        "# Create an XGBoost Regressor model\n",
        "xgb_model = xgb.XGBRegressor(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X_train_poly, y_train)\n",
        "\n",
        "# Get the best hyperparameters from the grid search\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Create an XGBoost Regressor model with the best hyperparameters\n",
        "best_xgb_model = xgb.XGBRegressor(**best_params, random_state=42)\n",
        "\n",
        "# Fit the best XGBoost model to the training data\n",
        "best_xgb_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions_xgb = best_xgb_model.predict(X_train_poly)\n",
        "test_predictions_xgb = best_xgb_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse_xgb = mean_squared_error(y_train, train_predictions_xgb)\n",
        "test_mse_xgb = mean_squared_error(y_test, test_predictions_xgb)\n",
        "\n",
        "train_r2_xgb = r2_score(y_train, train_predictions_xgb)\n",
        "test_r2_xgb = r2_score(y_test, test_predictions_xgb)\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"XGBoost Regressor:\")\n",
        "print(\"Train MSE:\", train_mse_xgb)\n",
        "print(\"Test MSE:\", test_mse_xgb)\n",
        "print(\"Train R-squared:\", train_r2_xgb)\n",
        "print(\"Test R-squared:\", test_r2_xgb)\n"
      ],
      "metadata": {
        "id": "DoayzF-vBb9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_xgb - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "CG0tYrDfFbcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions_xgb)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qwJG_laSFhJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "tBvoDNGfGERC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m_JNyJy1GEBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "p-kYelFvGDtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TOcS7imBGDfQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "_0Ul6HnpGDGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MMGcpC3qGC95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 9 - SUPPORT VECTOR REGRESSOR"
      ],
      "metadata": {
        "id": "LNIFsH1rCnMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already split your data into x_train, x_test, y_train, and y_test\n",
        "# Split the data into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=3)\n",
        "\n",
        "# Create an SVR model\n",
        "svr = SVR(kernel='rbf')  # You can choose the kernel (e.g., 'linear', 'rbf', 'poly')\n",
        "\n",
        "# Fit the SVR model to the training data\n",
        "svr.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = svr.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Calculate the R2 score for the training data\n",
        "training_r2 = svr.score(x_train, y_train)\n",
        "\n",
        "print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared (R2) Score: {r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "gYs9SHe2Hr-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_xgb - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "I6wjGMeLwMBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lGBHa-xXhJu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "PLGD2nqpIE_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sAs-ziplIF74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "8WzZTbavGfLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.svm import SVR\n",
        "# from sklearn.metrics import mean_squared_error, r2_score\n",
        "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "# import numpy as np\n",
        "\n",
        "# # Split the data into training and test sets\n",
        "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=3)\n",
        "\n",
        "# # Create an SVR model\n",
        "# svr = SVR()\n",
        "\n",
        "# # Define a grid of hyperparameters to search\n",
        "# param_grid = {\n",
        "#     'kernel': ['linear', 'rbf', 'poly'],  # You can specify other kernels\n",
        "#     'C': [0.1, 1, 10],  # Regularization parameter\n",
        "#     'epsilon': [0.1, 0.2, 0.3]  # Epsilon parameter\n",
        "# }\n",
        "\n",
        "# # Initialize GridSearchCV with 5-fold cross-validation\n",
        "# grid_search = GridSearchCV(svr, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# # Fit the model to the training data\n",
        "# grid_search.fit(x_train, y_train)\n",
        "\n",
        "# # Get the best hyperparameters from the grid search\n",
        "# best_params = grid_search.best_params_\n",
        "\n",
        "# # Create an SVR model with the best hyperparameters\n",
        "# best_svr = SVR(**best_params)\n",
        "\n",
        "# # Fit the best SVR model to the training data\n",
        "# best_svr.fit(x_train, y_train)\n",
        "\n",
        "# # Predict on the test data\n",
        "# y_pred = best_svr.predict(x_test)\n",
        "\n",
        "# # Evaluate the model\n",
        "# mse = mean_squared_error(y_test, y_pred)\n",
        "# r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# # Calculate the R2 score for the training data\n",
        "# training_r2 = best_svr.score(x_train, y_train)\n",
        "\n",
        "# print(\"Best Hyperparameters:\", best_params)\n",
        "# print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "# print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "# print(f\"R-squared (R2) Score: {r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "SVKB8W3U-bFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Visualizing evaluation Metric Score chart\n",
        "# sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "WOFPeK8lGjO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# plt.figure(figsize=(8,5))\n",
        "# plt.plot(np.array(y_test))\n",
        "# plt.plot(y_pred)\n",
        "# plt.legend([\"Predicted\",\"Actual\"])\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "eKVZ5oi1GmHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "NVDe_rZcGw9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the provided code, the hyperparameter optimization technique used is GridSearchCV. GridSearchCV is employed for hyperparameter tuning, which systematically searches through a predefined grid of hyperparameter values to find the combination that yields the best model performance. The main reason for choosing GridSearchCV is its comprehensiveness and effectiveness in exploring hyperparameter space:\n",
        "\n",
        "1. **Comprehensive Search**: GridSearchCV tests all possible combinations of hyperparameters specified in the grid, ensuring that no suitable set of hyperparameters is missed.\n",
        "\n",
        "2. **Automated and Systematic**: It automates the process of tuning hyperparameters, making it easy to test multiple hyperparameters without manual iteration.\n",
        "\n",
        "3. **Cross-Validation**: It incorporates cross-validation (in this case, 5-fold cross-validation) to evaluate each set of hyperparameters, which provides a robust estimate of model performance and helps prevent overfitting.\n",
        "\n",
        "4. **Scoring Metric**: The choice of using 'neg_mean_squared_error' as the scoring metric indicates that the goal is to minimize the mean squared error (MSE) during hyperparameter tuning.\n",
        "\n",
        "5. **Best Parameters**: GridSearchCV identifies and returns the best hyperparameters that result in the optimal model performance.\n",
        "\n",
        "In summary, GridSearchCV is chosen for its systematic, comprehensive, and automated approach to hyperparameter tuning, which helps fine-tune the Support Vector Regressor (SVR) model for the best predictive performance. The use of cross-validation ensures that the model is evaluated on different subsets of the training data, providing a robust assessment of its generalization capabilities."
      ],
      "metadata": {
        "id": "TarsxO9pGwxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "RSyKNCu6Gwk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"In the interest of project efficiency and to ensure timely progress, I have chosen to temporarily suspend the execution of certain models that were experiencing prolonged delays during the hyperparameter tuning and cross-validation process. While these models are valuable and relevant, their current runtime has exceeded 30 minutes, which is not aligned with the project's timeline. I will revisit these models at a later stage to derive their results when computational resources permit.\""
      ],
      "metadata": {
        "id": "4ZbGhzNvGwZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "GR6zbo21GwIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RXqPHAc4Gvqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Future Work**"
      ],
      "metadata": {
        "id": "aUzWHSKzNvgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several potential avenues for future work and improvement on your project, depending on your specific goals and the context of your machine learning project. Here are some ideas:\n",
        "\n",
        "1. **Fine-Tuning Hyperparameters**: You can perform more extensive hyperparameter tuning for your models. Adjust parameters such as learning rates, regularization strengths, tree depths, or kernel types to optimize model performance further.\n",
        "\n",
        "2. **Feature Engineering**: Explore additional feature engineering techniques. Create new features, combine existing ones, or perform feature selection to improve the model's ability to capture patterns in the data.\n",
        "\n",
        "3. **Ensemble Methods**: Experiment with ensemble techniques like stacking, where you combine the predictions of multiple models. This can often lead to improved performance.\n",
        "\n",
        "4. **Additional Models**: Try other machine learning algorithms that may be suitable for your problem, such as neural networks, k-Nearest Neighbors, or other regression techniques.\n",
        "\n",
        "5. **Data Preprocessing**: Investigate the data preprocessing pipeline. Ensure data quality, handle missing values, and consider scaling or normalizing features if necessary. Data preprocessing can have a significant impact on model performance.\n",
        "\n",
        "6. **Time Series Analysis**: If your data involves time series, explore time series-specific models and forecasting techniques to account for temporal dependencies.\n",
        "\n",
        "7. **Cross-Validation Strategies**: Experiment with different cross-validation strategies to ensure robust model evaluation. Techniques like k-fold cross-validation, time series cross-validation, or stratified sampling can be beneficial depending on the data type.\n",
        "\n",
        "8. **Feature Importance**: Perform more in-depth feature importance analysis to gain insights into which features are most influential in making predictions. Consider visualizing feature importance scores.\n",
        "\n",
        "9. **Deployment**: If your project aims to deliver real-time predictions, work on deploying your model as a production-ready system, either on a web platform or as an API.\n",
        "\n",
        "10. **Interpretability**: Focus on model interpretability. Use techniques like SHAP (SHapley Additive exPlanations) values or LIME (Local Interpretable Model-Agnostic Explanations) to explain your model's predictions, especially in contexts where interpretability is crucial.\n",
        "\n",
        "11. **Anomaly Detection**: If your project involves anomaly detection, explore specialized techniques for identifying and handling outliers in your data.\n",
        "\n",
        "12. **Scaling and Performance**: Optimize your models for performance. If working with large datasets, consider distributed computing or GPU acceleration for faster training.\n",
        "\n",
        "13. **Monitoring and Maintenance**: Develop a plan for monitoring model performance and retraining the model periodically to account for data drift and concept drift.\n",
        "\n",
        "14. **Data Augmentation**: In cases of limited data, you can explore data augmentation techniques to generate synthetic data points for training.\n",
        "\n",
        "15. **Domain-Specific Knowledge**: Collaborate with domain experts to gain insights and refine the modeling approach. Their expertise can lead to better feature engineering and more informed model choices.\n",
        "\n",
        "16. **Ethical Considerations**: Consider ethical and fairness aspects of your models, especially if they are used to make decisions that impact people's lives. Mitigate biases and ensure fairness in predictions.\n",
        "\n",
        "17. **Scalability**: Assess the scalability of your models to handle larger datasets, as data volume may grow over time.\n",
        "\n",
        "18. **Documentation and Reporting**: Properly document your work, including data sources, methodology, results, and decisions made. Clear reporting is crucial for project transparency and reproducibility.\n",
        "\n",
        "19. **Benchmarking**: Compare your model's performance with existing benchmarks and state-of-the-art solutions in your problem domain.\n",
        "\n",
        "20. **User Feedback and Testing**: If your model is used by end-users, gather feedback and conduct usability testing to improve the user experience.\n",
        "\n",
        "Remember that the choice of future work should align with your project's objectives and constraints. Regularly evaluate and adapt your approach to ensure that your machine learning solution continues to provide value and stays up-to-date with evolving data and requirements."
      ],
      "metadata": {
        "id": "tP7TFd-wNpPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "3iAI3zBHvrpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selecting the final prediction model depends on your specific use case and the priorities you have. Here are a few considerations based on the provided training and testing accuracy values:\n",
        "\n",
        "1. **Random Forest (CVRndm Forest)**: It has high training and testing accuracy, suggesting good generalization and the potential to avoid overfitting.\n",
        "\n",
        "2. **Gradient Boost (Grdient Boost)**: This model also has high accuracy on both training and testing data. It's well-suited for many tasks due to its robustness.\n",
        "\n",
        "3. **XG Boost (CVXG BOOST)**: Similar to Gradient Boost, XG Boost performs well on both training and testing data. It's known for its performance and speed.\n",
        "\n",
        "4. **Polynomial Regression (CV polynomial LR)**: This model performs well, but it's important to consider that high-degree polynomial regression models can be prone to overfitting. The \"CV\" indicates that it's a cross-validated version, which may help mitigate overfitting.\n",
        "\n",
        "5. **Simple Linear Regression (CV simple LR)**: While it doesn't have the highest accuracy, it's a simple model and can serve as a good baseline. It's also the simplest to interpret.\n",
        "\n",
        "Ultimately, the choice of the final prediction model should consider factors such as:\n",
        "\n",
        "- The nature of your data: Are there nonlinear relationships, interactions, or complex patterns that the chosen model can capture effectively?\n",
        "- Model complexity: Are you looking for a simple and interpretable model, or are you comfortable with more complex models?\n",
        "- Overfitting: Models with high training accuracy but significantly lower testing accuracy might be overfitting, so it's important to evaluate generalization performance.\n",
        "- Computational resources: Some models are computationally intensive, so consider the available resources.\n",
        "- Business requirements: The final choice should align with the specific goals and constraints of your project.\n",
        "\n",
        "It's often a good practice to compare different models through cross-validation, analyze their feature importances, and consider their pros and cons for your particular application before selecting the final prediction model. Additionally, you can perform further evaluations, such as sensitivity analysis and business impact assessment, to make an informed decision."
      ],
      "metadata": {
        "id": "Yc0xx_K6vuru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the model names (you can adjust these as needed)\n",
        "model_names = [\"simple LR \", \"CV simple LR\", \"Polynomial LR\", \"CVpolynomial LR\", \"Ridge LR\", \"CVRidge LR\",\"Lasoo\",\"CVLasoo\",\" Elastic Net\", \"CV elastic net\",\n",
        "               \"Rndm Forrest\",\"CVRndm Forest\",\"Grdient Boost\", \"XG Boost\", \" CVXG BOOST \", \"SVR\"]\n",
        "\n",
        "# Define the training and testing accuracy values for each\n",
        "training_accuracy =[0.67, 0.67, 0.78, 0.78, 0.68, 0.78, 0.76, 0.78, 0.69, 0.77, 0.99, 0.91, 0.82, 0.96, 0.98 , 0.67]\n",
        "testing_accuracy = [0.68, 0.68, 0.78, 0.78, 0.68, 0.78, 0.75, 0.65, 0.68, 0.78, 0.92, 0.92, 0.79, 0.91, 0.93 ,0.67]\n",
        "\n",
        "# Set the width of the bars and their positions\n",
        "width = 0.35\n",
        "x = range(len(model_names))\n",
        "\n",
        "# Create the bar plot\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.bar(x, training_accuracy, width, label='Training Accuracy')\n",
        "plt.bar([i + width for i in x], testing_accuracy, width, label='Testing Accuracy')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Models')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Testing Accuracy of Different Models')\n",
        "plt.xticks([i + width / 2 for i in x], model_names)\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wKkTtUX_8_NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above data of machine learning models and their corresponding training and testing accuracy values, we can draw the following conclusions:\n",
        "\n",
        "1. **Simple Linear Regression (LR)**: Both the training and testing accuracies are around 0.67, indicating moderate predictive performance.\n",
        "\n",
        "2. **Cross-Validated (CV) Simple Linear Regression**: The addition of cross-validation did not significantly improve accuracy, with both training and testing accuracies remaining at approximately 0.67.\n",
        "\n",
        "3. **Polynomial Linear Regression**: Polynomial regression with a degree of 2 achieved a high training accuracy of 0.78, indicating a good fit to the training data. The testing accuracy also reached 0.78, suggesting the model generalizes well.\n",
        "\n",
        "4. **Cross-Validated Polynomial Linear Regression**: The cross-validated polynomial regression maintained the high training and testing accuracies of 0.78, confirming good performance on both sets.\n",
        "\n",
        "5. **Ridge Linear Regression**: Ridge regression achieved moderate accuracy with a training and testing accuracy of around 0.68.\n",
        "\n",
        "6. **Cross-Validated Ridge Linear Regression**: Adding cross-validation to Ridge regression led to a slightly higher testing accuracy of 0.78, suggesting better generalization.\n",
        "\n",
        "7. **Lasso Regression**: Lasso regression achieved moderate testing accuracy of 0.75 and training accuracy of 0.76.\n",
        "\n",
        "8. **Cross-Validated Lasso Regression**: Cross-validated Lasso regression resulted in a lower testing accuracy of 0.65, which may indicate overfitting.\n",
        "\n",
        "9. **Elastic Net**: Elastic Net achieved moderate testing accuracy of 0.68 and training accuracy of 0.69.\n",
        "\n",
        "10. **Cross-Validated Elastic Net**: The cross-validated Elastic Net model maintained a testing accuracy of 0.78, suggesting good generalization.\n",
        "\n",
        "11. **Random Forest**: Random Forest achieved high accuracy with a training accuracy of 0.99 and a testing accuracy of 0.92, indicating an excellent fit to the training data and good generalization.\n",
        "\n",
        "12. **Cross-Validated Random Forest**: Cross-validated Random Forest maintained a high testing accuracy of 0.92, suggesting strong generalization.\n",
        "\n",
        "13. **Gradient Boosting**: Gradient Boosting achieved a training accuracy of 0.82 and a testing accuracy of 0.79, indicating good performance.\n",
        "\n",
        "14. **XG Boost**: XG Boost achieved high accuracy with a training accuracy of 0.96 and a testing accuracy of 0.91, suggesting excellent fit and generalization.\n",
        "\n",
        "15. **Cross-Validated XG Boost**: The cross-validated XG Boost maintained a high testing accuracy of 0.93, indicating robust generalization.\n",
        "\n",
        "16. **Support Vector Regression (SVR)**: SVR achieved moderate accuracy with a training and testing accuracy of approximately 0.67.\n",
        "\n",
        "In summary, it appears that ensemble models like Random Forest, XG Boost, and Cross-Validated XG Boost outperform the other models in terms of testing accuracy, indicating strong predictive performance. Polynomial Linear Regression and Cross-Validated Polynomial Linear Regression also perform well. It's important to consider not only accuracy but also other evaluation metrics and the specific context of your problem when selecting the most suitable model for your application. Additionally, further analysis and hyperparameter tuning may lead to improved results for some models."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}