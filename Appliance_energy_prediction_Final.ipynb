{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IAMDSVSSANGRAL/applianceenergyprediction/blob/main/Appliance_energy_prediction_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 -Samadhan**\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective:\n",
        "The objective of this project is to develop a regression model that accurately predicts the energy consumption of household appliances based on various input features. The model aims to provide insights into energy usage patterns and facilitate energy efficiency improvements in residential settings.\n",
        "\n",
        "Data:\n",
        "The project utilizes a dataset that contains information on household appliance energy consumption along with several relevant input features. The dataset includes variables such as temperature, humidity, time of day, and various appliance power readings. The data is collected over a specific time period and is representative of real-world residential energy usage scenarios.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "Exploratory Data Analysis (EDA):\n",
        "\n",
        "Perform a thorough analysis of the dataset to understand the distribution, statistics, and relationships among variables.\n",
        "Identify any missing values, outliers, or data quality issues that need to be addressed.\n",
        "Visualize the data using appropriate charts and graphs to gain insights into the patterns and trends.\n",
        "Data Preprocessing:\n",
        "\n",
        "Handle missing values by applying suitable imputation techniques or deciding on appropriate strategies for dealing with them.\n",
        "Address outliers and anomalies by considering various methods such as removal, transformation, or capping.\n",
        "Normalize or scale the data if necessary to ensure all features are on a similar scale.\n",
        "Feature Engineering:\n",
        "\n",
        "Explore the relationships between the input features and the target variable (appliance energy consumption) to identify potential feature engineering opportunities.\n",
        "Create new features, derive meaningful variables, or transform existing variables to capture important patterns or interactions in the data.\n",
        "Model Development:\n",
        "\n",
        "Split the dataset into training and testing sets for model development and evaluation.\n",
        "Select an appropriate regression algorithm (e.g., linear regression, decision tree regression, random forest regression) based on the project requirements and characteristics of the data.\n",
        "Train the model using the training data and tune hyperparameters to optimize performance.\n",
        "Evaluate the model's performance using various metrics such as mean squared error (MSE), mean absolute error (MAE), and R-squared.\n",
        "Model Evaluation and Interpretation:\n",
        "\n",
        "Assess the model's performance on the testing data to measure its ability to generalize to unseen data.\n",
        "Interpret the model's coefficients or feature importance to gain insights into the factors that have the most significant impact on appliance energy consumption.\n",
        "Validate the model's predictions against domain knowledge or external benchmarks to ensure its reliability and usefulness.\n",
        "Model Deployment and Recommendations:\n",
        "\n",
        "Deploy the trained model into a production environment or create a user-friendly interface for stakeholders to interact with the model.\n",
        "Provide recommendations based on the model's predictions and insights to improve energy efficiency, optimize appliance usage, or suggest modifications in residential settings.\n",
        "Conclusion:\n",
        "The Appliance Energy Prediction regression project aims to develop a robust regression model to accurately predict household appliance energy consumption. By analyzing and understanding the data, performing feature engineering, and building an effective regression model, the project provides valuable insights and recommendations for optimizing energy usage and promoting energy-efficient practices in residential settings.\n",
        "\n",
        "Note: This project summary provides a general outline and can be tailored based on specific requirements, dataset characteristics, and project goals."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/IAMDSVSSANGRAL/applianceenergyprediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, here is the problem statement broken down into bullet points using different phrases:\n",
        "\n",
        "- **Data Source**: The dataset spans approximately 4.5 months and includes information collected at 10-minute intervals. It consists of data from a ZigBee wireless sensor network monitoring temperature and humidity in a house, energy consumption recorded by m-bus energy meters, and weather data from Chievres Airport, Belgium.\n",
        "\n",
        "- **Data Averaging**: The wireless sensor network reports temperature and humidity every 3.3 minutes, but the data is averaged over 10-minute periods.\n",
        "\n",
        "- **Objective**: The primary goal is to develop a machine learning model capable of accurately predicting energy usage based on the provided features.\n",
        "\n",
        "- **Utility**: This predictive model has potential applications for building managers, energy companies, and policymakers. It can aid in optimizing energy consumption, reducing costs, and minimizing the environmental impact of energy usage.\n",
        "\n",
        "- **Influence Factors**: The model aims to consider a range of influencing factors, including temperature, humidity, illumination, and time of day, all of which can impact energy consumption in a building.\n",
        "\n",
        "- **Pattern and Trend Identification**: Building managers and energy firms can benefit from this model by identifying patterns and trends in energy consumption. This can help them make informed decisions, such as adjusting HVAC settings, optimizing lighting, or implementing energy-efficient solutions.\n",
        "\n",
        "- **Policymaker Applications**: Policymakers can also leverage the insights from this model to develop regulations and incentives that promote energy efficiency and sustainability.\n",
        "\n",
        "- **Random Variables**: The dataset includes random variables designed for testing regression models and filtering out non-predictive features.\n",
        "\n",
        "- **Integration of External Data**: External weather data from Chievres Airport, Belgium, was integrated into the dataset using date and time columns, enhancing the model's ability to make energy usage forecasts.\n",
        "\n",
        "- **Environmental Impact**: One of the broader goals is to contribute to reducing the environmental impact of energy usage through better management and decision-making."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -"
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "\n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "\n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "\n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "\n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime as dt\n",
        "\n",
        "# Import Data Visualisation Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly as pl\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "from pandas.plotting import scatter_matrix\n",
        "%matplotlib inline\n",
        "\n",
        "# Set the plot style and display options\n",
        "plt.style.use('ggplot')\n",
        "sns.set()\n",
        "\n",
        "# To display all the columns in Dataframe\n",
        "pd.set_option('display.max_columns', None)\n",
        "# Import Library to visualise missing data\n",
        "import missingno as mno\n",
        "\n",
        "# Import and Ignore warnings for better code readability,\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the data set\n",
        "data_raw = pd.read_csv('/content/drive/MyDrive/Santa/Regression capstone/data_application_energy.csv')"
      ],
      "metadata": {
        "id": "TmkdQzZojsHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a copy of data set\n",
        "data = data_raw.copy()"
      ],
      "metadata": {
        "id": "HJ8f2WTWL-YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "num_rows, num_cols = data.shape\n",
        "\n",
        "print(\"Number of rows:\", num_rows)\n",
        "print(\"Number of columns:\", num_cols)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your date column is named \"date_column\"\n",
        "data['date'] = pd.to_datetime(data['date'])"
      ],
      "metadata": {
        "id": "76kdxS8WdrbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting date as the index:\n",
        "data.set_index('date', inplace=True)"
      ],
      "metadata": {
        "id": "5kPxyGdERI8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count assinged a dataframe name 'df'\n",
        "df = data[data.duplicated()]"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#There is no duplicate rows in the data\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bVN4H8UU09Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import missingno as msno\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the null matrix\n",
        "msno.matrix(data)\n",
        "\n",
        "# Customizing the plot\n",
        "plt.title('Null Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is in the form of a Pandas DataFrame with 29 columns and 19,735 rows. It appears to be a dataset with multiple features and observations, but without the context of what this dataset represents, it's challenging to provide specific insights. However, I can offer some general insights you can gain from this data:\n",
        "\n",
        "1. **Data Size**: The dataset contains 19,735 data points, which is a significant amount of data.\n",
        "\n",
        "2. **Data Types**: Most of the columns contain numerical data, with 26 columns having float64 data type and 2 columns with int64 data type. The 'date' column seems to contain date values as objects.\n",
        "\n",
        "3. **Features**: The columns labeled 'T1,' 'T2,' 'T3,' etc., represent temperature measurements, while columns labeled 'RH_1,' 'RH_2,' 'RH_3,' etc., represent relative humidity measurements. 'Appliances' and 'lights' are integer columns, which might be related to energy consumption and lighting. Other columns have labels such as 'T_out' (outdoor temperature), 'Windspeed,' 'RH_out' (outdoor humidity), and more.\n",
        "\n",
        "4. **Data Completeness**: There are no missing values (non-null) in any of the columns, which is a good sign for data quality.\n",
        "\n",
        "5. **Memory Usage**: The dataset consumes 4.4+ MB of memory, which might be relevant for memory-constrained analyses.\n",
        "\n",
        "6. **NO Duplicate values**:We don't see any output, it's possible that there are no duplicated rows in your original DataFrame data.\n",
        "\n",
        "To gain more meaningful insights from this data, you'll need to have a clear understanding of what the dataset represents and what kind of analysis you want to perform. Depending on the context, you could explore relationships between different variables, conduct statistical analysis, visualize data, and build predictive models. Please provide more information about the dataset and your specific goals if you'd like more detailed insights."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "data.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The observation data consists of the following variables:**\n",
        "\n",
        "\n",
        "datetime year-month-day hour : minute:second\n",
        "\n",
        "Appliances: energy use in Wh [TARGETED]\n",
        "\n",
        "lights: energy use of light fixtures in the house in Wh\n",
        "\n",
        "T1: Temperature in kitchen area, in Celsius\n",
        "\n",
        "RH_1: Humidity in kitchen area, in %\n",
        "\n",
        "T2: Temperature in living room area, in Celsius\n",
        "\n",
        "RH_2:Humidity in living room area, in %\n",
        "\n",
        "T3:Temperature in laundry room area\n",
        "\n",
        "RH_3:Humidity in laundry room area, in %\n",
        "\n",
        "T4:Temperature in office room, in Celsius\n",
        "\n",
        "RH_4:Humidity in office room, in %\n",
        "\n",
        "T5:Temperature in bathroom, in Celsius\n",
        "\n",
        "RH_5:Humidity in bathroom, in %\n",
        "\n",
        "T6:Temperature outside the building (north side), in Celsius\n",
        "\n",
        "RH_6:Humidity outside the building (north side), in %\n",
        "\n",
        "T7:Temperature in ironing room , in Celsius\n",
        "\n",
        "RH_7:Humidity in ironing room, in %\n",
        "\n",
        "T8:Temperature in teenager room 2, in Celsius\n",
        "\n",
        "RH_8:Humidity in teenager room 2, in %\n",
        "\n",
        "T9:Temperature in parents room, in Celsius\n",
        "\n",
        "RH_9:Humidity in parents room, in %\n",
        "\n",
        "T_out:Temperature outside (from Chièvres weather station), in Celsius\n",
        "\n",
        "Press_mm_hg: (from Chièvres weather station), in mm Hg\n",
        "\n",
        "RH_out: Humidity outside (from Chièvres weather station), in %\n",
        "\n",
        "Windspeed: (from Chièvres weather station), in m/s\n",
        "\n",
        "Visibility: (from Chièvres weather station), in km\n",
        "\n",
        "Tdewpoint: (from Chièvres weather station), °C\n",
        "\n",
        "rv1: Random variable 1, nondimensional\n",
        "\n",
        "rv2: Rnadom variable 2, nondimensional"
      ],
      "metadata": {
        "id": "PYzlcej3OU5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Unique Values count for each variable.\n",
        "for i in data.columns.tolist():\n",
        "  print(\"The unique values in\",i, \"is\",data[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "LGAz2eAYwhmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Round the unique values to two decimal places\n",
        "rounded_unique_values = data.apply(lambda x: set(round(val, 2) for val in x))\n",
        "\n",
        "# Print the unique values for each feature\n",
        "for feature, unique in rounded_unique_values.items():\n",
        "    print(f'{feature}: {unique}')"
      ],
      "metadata": {
        "id": "_6QYnjVk4wNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating columns:\n",
        "temperature_column = [i for i in data.columns if \"T\" in i]\n",
        "humidity_column = [i for i in data.columns if \"RH\" in i]\n",
        "other = [i for i in data.columns if (\"T\" not in i)&(\"RH\" not in i)]"
      ],
      "metadata": {
        "id": "Kst3KQHVRrVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[temperature_column].describe(include='all')"
      ],
      "metadata": {
        "id": "DzXNlgYLRtMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can derive several insights:\n",
        "\n",
        "1. **Count**:\n",
        "   - There are 19,735 data points for each of the temperature-related variables (T1, T2, T3, T4, T5, T6, T7, T8, T9, T_out, and Tdewpoint). This indicates that there are no missing values in these columns.\n",
        "\n",
        "2. **Mean (Average)**:\n",
        "   - The mean values for the temperature-related variables are in the range of 16.79°C to 26.26°C. The \"T3\" variable has the highest mean at approximately 22.27°C, while \"T5\" has the lowest mean at about 19.59°C.\n",
        "\n",
        "3. **Standard Deviation (std)**:\n",
        "   - The standard deviations for the temperature-related variables range from approximately 1.61°C to 2.20°C. Variables like \"T3\" and \"T4\" have relatively low variability, while \"T9\" has slightly higher variability.\n",
        "\n",
        "4. **Minimum (min)**:\n",
        "   - The minimum values for the temperature-related variables range from 15.10°C to 29.24°C. These values indicate the lower bounds of the temperature measurements.\n",
        "\n",
        "5. **25th Percentile (25%)**:\n",
        "   - The 25th percentile values represent the lower quartile of the data. For example, the 25th percentile of \"T2\" is approximately 18.79°C.\n",
        "\n",
        "6. **Median (50%)**:\n",
        "   - The median values (50th percentile) represent the middle values of the dataset. For instance, the median temperature \"T7\" is approximately 20.03°C.\n",
        "\n",
        "7. **75th Percentile (75%)**:\n",
        "   - The 75th percentile values represent the upper quartile of the data. The 75th percentile of \"T6\" is approximately 11.26°C.\n",
        "\n",
        "8. **Maximum (max)**:\n",
        "   - The maximum values represent the upper bounds of the temperature measurements. \"T4\" has the highest maximum value at approximately 26.20°C, while \"T5\" has the lowest maximum at about 25.79°C."
      ],
      "metadata": {
        "id": "u2zLtBh8XctQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[humidity_column].describe()"
      ],
      "metadata": {
        "id": "w_j4XCxISHY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e can derive several insights regarding the relative humidity (RH) variables:\n",
        "\n",
        "1. **Count**:\n",
        "   - There are 19,735 data points for each of the RH-related variables (RH_1, RH_2, RH_3, RH_4, RH_5, RH_6, RH_7, RH_8, RH_9, and RH_out). This indicates that there are no missing values in these columns.\n",
        "\n",
        "2. **Mean (Average)**:\n",
        "   - The mean values for the relative humidity variables vary across the columns. For example, \"RH_5\" has the highest mean at approximately 50.95%, while \"RH_7\" has the lowest mean at around 35.39%. The \"RH_out\" variable, which represents outdoor relative humidity, has a mean of approximately 79.75%.\n",
        "\n",
        "3. **Standard Deviation (std)**:\n",
        "   - The standard deviations for the relative humidity variables also vary. \"RH_5\" has a standard deviation of approximately 9.02, indicating relatively higher variability, while \"RH_3\" has a lower standard deviation of around 3.25.\n",
        "\n",
        "4. **Minimum (min)**:\n",
        "   - The minimum values for the relative humidity variables indicate the lower bounds of the humidity measurements. For example, \"RH_6\" has a minimum of approximately 1.00% which look like there are outlier on lower bound of RH_6 and \"RH_out\" has a minimum of 24.00%.\n",
        "\n",
        "5. **25th Percentile (25%)**:\n",
        "   - The 25th percentile values represent the lower quartile of the data. \"RH_7\" has a 25th percentile value of approximately 31.50%.\n",
        "\n",
        "6. **Median (50%)**:\n",
        "   - The median values (50th percentile) represent the middle values of the dataset. \"RH_9\" has a median relative humidity of approximately 40.90%.\n",
        "\n",
        "7. **75th Percentile (75%)**:\n",
        "   - The 75th percentile values represent the upper quartile of the data. \"RH_4\" has a 75th percentile value of approximately 42.16%.\n",
        "\n",
        "8. **Maximum (max)**:\n",
        "   - The maximum values represent the upper bounds of the relative humidity measurements. \"RH_1\" has the highest maximum value at approximately 63.36%, and \"RH_out\" has the lowest maximum value at 100.00%.\n",
        "\n"
      ],
      "metadata": {
        "id": "f0n4GXA2YRTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[other].describe()"
      ],
      "metadata": {
        "id": "StZKZkcwSQpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can derive several insights regarding the variables Appliances, lights, Press_mm_hg, Windspeed, Visibility, rv1, and rv2:\n",
        "\n",
        "1. **Appliances**:\n",
        "   - The \"Appliances\" variable represents energy consumption related to appliances. The data ranges from a minimum of 10 to a maximum of 1080, with an average (mean) consumption of approximately 97.69. **The standard deviation is relatively high, indicating significant variability in appliance energy usage.**\n",
        "\n",
        "2. **Lights**:\n",
        "   - The \"lights\" variable shows energy consumption related to lighting. It varies from 0 to 70, with an average of approximately 3.80. The standard deviation suggests some variability in lighting energy consumption. **upto 75 percent of value have 0 values which is slightly ODD.**\n",
        "\n",
        "3. **Press_mm_hg**:\n",
        "   - \"Press_mm_hg\" represents atmospheric pressure. The pressure varies from 729.30 to 772.30, with an average of approximately 755.52. The data has relatively low variability.\n",
        "\n",
        "4. **Windspeed**:\n",
        "   - The \"Windspeed\" variable indicates wind speed and varies from 0 to 14. The average wind speed is about 4.04. The standard deviation suggests some variation in wind speed. **Maximum value is 14 which is very far from 75% of values that is 5.50**\n",
        "\n",
        "5. **Visibility**:\n",
        "   - \"Visibility\" represents the visibility in meters. It ranges from 1 to 66, with an average of approximately 38.33. The data exhibits relatively **high variability**.\n",
        "\n",
        "6. **rv1 and rv2**:\n",
        "   - The columns \"rv1\" and \"rv2\" have identical statistics, suggesting that they are likely **highly correlated or identical features**. They have a minimum value of approximately 0.0053 and a maximum value of around 49.9965."
      ],
      "metadata": {
        "id": "fg_iUznxVuXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to map current column names to new column names\n",
        "column_mapping = {'T1': 'KITCHEN_TEMP',\n",
        "    'RH_1': 'KITCHEN_HUM',\n",
        "    'T2': 'LIVING_TEMP',\n",
        "    'RH_2' :'LIVING_HUM',\n",
        "    'T3': 'BEDROOM_TEMP',\n",
        "    'RH_3':'BEDROOM_HUM',\n",
        "    'T4' : 'OFFICE_TEMP',\n",
        "    'RH_4' : 'OFFICE_HUM',\n",
        "    'T5' : 'BATHROOM_TEMP',\n",
        "    'RH_5': 'BATHROOM_HUM',\n",
        "    'T6':'OUTSIDE_TEMP_build',\n",
        "    'RH_6': 'OUTSIDE_HUM_build',\n",
        "    'T7': 'IRONING_ROOM_TEMP',\n",
        "    'RH_7' : 'IRONING_ROOM_HUM',\n",
        "    'T8' :'TEEN_ROOM_2_TEMP',\n",
        "    'RH_8' : 'TEEN_ROOM_HUM',\n",
        "    'T9': 'PARENTS_ROOM_TEMP',\n",
        "    'RH_9': 'PARENTS_ROOM_HUM',\n",
        "    'T_out' :'OUTSIDE_TEMP_wstn',\n",
        "    'RH_out' :'OUTSIDE_HUM_wstn'}\n",
        "\n",
        "# Rename the columns using the mapping\n",
        "data.rename(columns=column_mapping, inplace=True)"
      ],
      "metadata": {
        "id": "xE1jxXD7iQ5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "ZW2VQUgF9wi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new features\n",
        "data['month'] = data.index.month\n",
        "data['weekday'] = data.index.weekday\n",
        "data['hour'] = data.index.hour\n",
        "data['week'] = data.index.week\n",
        "data['day'] = data.index.day\n",
        "data['day_of_week'] = data.index.dayofweek"
      ],
      "metadata": {
        "id": "UGcDgyyT3nUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(2)"
      ],
      "metadata": {
        "id": "lXzEM5GB3sMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting values of the \"lights\" column:\n",
        "data['lights'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "4WZ0JUpVUwPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "77% value of lights column are 0 and it is not relevant for prediction. so we are going to drop this column"
      ],
      "metadata": {
        "id": "TBGBIpteVhXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the lights column:\n",
        "data.drop(columns='lights', inplace=True)"
      ],
      "metadata": {
        "id": "VJbGWhxOVcbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reorder the data for clear vision\n",
        "desired_order = ['KITCHEN_TEMP','LIVING_TEMP','BEDROOM_TEMP','OFFICE_TEMP','BATHROOM_TEMP','OUTSIDE_TEMP_build','IRONING_ROOM_TEMP','TEEN_ROOM_2_TEMP','PARENTS_ROOM_TEMP','OUTSIDE_TEMP_wstn',\n",
        "                 'KITCHEN_HUM','LIVING_HUM','BEDROOM_HUM','OFFICE_HUM','BATHROOM_HUM','OUTSIDE_HUM_build','IRONING_ROOM_HUM','TEEN_ROOM_HUM','PARENTS_ROOM_HUM','OUTSIDE_HUM_wstn',\n",
        "                 \"Tdewpoint\",\"Press_mm_hg\",\"Windspeed\",\"Visibility\",\"rv1\", \"rv2\",'month','weekday','hour','week','day','day_of_week',\"Appliances\"]\n",
        "#assinging new_data as new name of dataframe\n",
        "data = data.reindex(columns=desired_order)"
      ],
      "metadata": {
        "id": "6Xw2W7igdBfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail(2)"
      ],
      "metadata": {
        "id": "vdToJwPyiufd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AUTOEDA\n",
        "!pip install sweetviz\n",
        "import sweetviz as sv\n",
        "sweet_report = sv.analyze(data)\n",
        "sweet_report.show_html('sweet_report.html')"
      ],
      "metadata": {
        "id": "4hKGGwU4HsqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pivot table to aggregate the daily energy consumption\n",
        "daily_energy = data.pivot_table(values='Appliances', index='day', columns='month', aggfunc = 'mean')\n",
        "\n",
        "# Create a heatmap using the pivot table\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Daily Energy Consumption')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Day')\n",
        "plt.imshow(daily_energy, cmap='YlGnBu', aspect='auto')\n",
        "plt.colorbar(label='Energy Consumption')\n",
        "plt.xticks(range(0,5), ['Jan', 'Feb', 'Mar', 'Apr', 'May'])\n",
        "plt.yticks(range(1, 32))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xSZHw-DN80Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I choose this chart to identify the distribution of each variable in the data.\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the day of the week values to their respective names\n",
        "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "data['day_of_week'] = data['day_of_week'].map(lambda x: day_names[x])\n",
        "\n",
        "# Create a box plot or violin plot to compare energy consumption across different days of the week\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='day_of_week', y='Appliances', data=data, order=day_names)  # or sns.violinplot()\n",
        "plt.title('Appliance Energy Consumption by Day of the Week')\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Energy Consumption')"
      ],
      "metadata": {
        "id": "vFnJADwHDwAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a line plot to show the trend of energy consumption over time\n",
        "import plotly.express as px\n",
        "\n",
        "# Assuming you have a DataFrame 'data' with a datetime index\n",
        "fig = px.line(data, x=data.index, y='Appliances', title='Energy Consumption of Appliances Over Time')\n",
        "fig.update_xaxes(title_text='Date', tickangle=-45)\n",
        "fig.update_yaxes(title_text='Energy Consumption')\n",
        "\n",
        "# Show the Plotly figure\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "BLX7c5AF-_yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the lights column:\n",
        "data.drop(columns='day_of_week', inplace=True)"
      ],
      "metadata": {
        "id": "AWWj0N4zlMRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Examining the outlier in the dataset\n",
        "# Assuming 'data' is your DataFrame\n",
        "num_columns = len(data.columns)\n",
        "fig, axes = plt.subplots(nrows=num_columns, figsize=(8, num_columns*6))\n",
        "\n",
        "for i, column in enumerate(data.columns):\n",
        "    # Exclude 'day_of_week' from the visualization\n",
        "    if column != 'day_of_week':\n",
        "        data.boxplot(column=column, ax=axes[i])\n",
        "        axes[i].set_title(f'Box Plot for {column}')\n",
        "        axes[i].set_xlabel('Column')\n",
        "        axes[i].set_ylabel('Values')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "#close look on four columns\n",
        "fig_sub = make_subplots(rows=1, cols=4, shared_yaxes=False)\n",
        "\n",
        "fig_sub.add_trace(go.Box(y=data['Appliances'].values,name='Appliances'),row=1, col=1)\n",
        "fig_sub.add_trace(go.Box(y=data['Windspeed'].values,name='Windspeed'),row=1, col=2)\n",
        "fig_sub.add_trace(go.Box(y=data['Visibility'].values,name='Visibility'),row=1, col=3)\n",
        "fig_sub.add_trace(go.Box(y=data['Press_mm_hg'].values,name='Press_mm_hg'),row=1, col=4)\n",
        "\n",
        "fig_sub.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame with the energy consumption data\n",
        "# You can group the data by hour and calculate the mean energy consumption for each hour\n",
        "hourly_energy = data.groupby('hour')['Appliances'].mean()\n",
        "\n",
        "# Create a line chart to visualize the hourly energy consumption patterns\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(hourly_energy.index, hourly_energy.values, marker='o', linestyle='-')\n",
        "plt.title('Hourly Energy Consumption Patterns')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Energy Consumption (mean)')\n",
        "plt.xticks(range(24))\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame with the relevant columns (e.g., 'KITCHEN_TEMP', 'OUTSIDE_TEMP_build', and 'Appliances')\n",
        "# You can create a scatter plot with a regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='KITCHEN_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can create a scatter plot with a regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='LIVING_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can create a scatter plot with a regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='BEDROOM_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can create a scatter plot with a regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='OFFICE_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can create a scatter plot with a regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='BATHROOM_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can create a scatter plot with a regression line for outdoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='OUTSIDE_TEMP_build', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Indoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Indoor Temperature (KITCHEN_TEMP)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can also create a similar scatter plot and regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='IRONING_ROOM_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Outdoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Outdoor Temperature (OUTSIDE_TEMP_build)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can also create a similar scatter plot and regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='TEEN_ROOM_2_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Outdoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Outdoor Temperature (OUTSIDE_TEMP_build)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can also create a similar scatter plot and regression line for indoor temperature vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='PARENTS_ROOM_TEMP', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Outdoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Outdoor Temperature (OUTSIDE_TEMP_build)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "# You can also create a similar scatter plot and regression line for outdoor temperature data from weather station vs. energy consumption\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.regplot(x='OUTSIDE_TEMP_wstn', y='Appliances', data=data, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n",
        "plt.title('Scatter Plot and Regression Line for Outdoor Temperature vs. Energy Consumption')\n",
        "plt.xlabel('Outdoor Temperature (OUTSIDE_TEMP_build)')\n",
        "plt.ylabel('Energy Consumption (Appliances)')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame with the relevant columns (e.g., 'hour' and 'Appliances')\n",
        "# You can create a line chart to show energy consumption throughout the day\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Group the data by hour and calculate the mean energy consumption for each hour\n",
        "hourly_energy = data.groupby('hour')['Appliances'].mean()\n",
        "\n",
        "# Split the data into daytime (6:00 AM to 6:00 PM) and nighttime (6:00 PM to 6:00 AM)\n",
        "daytime_energy = hourly_energy[6:18]\n",
        "\n",
        "nighttime_energy= hourly_energy[0:6].append(hourly_energy[18:24])\n",
        "\n",
        "# Plot the daytime and nighttime energy consumption\n",
        "plt.plot(daytime_energy.index, daytime_energy.values, label='Daytime', marker='o',color = 'r')\n",
        "plt.plot(nighttime_energy.index, nighttime_energy.values, label='Nighttime', marker='o',color = 'b')\n",
        "\n",
        "plt.title('Energy Consumption Throughout the Day')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Mean Energy Consumption')\n",
        "plt.xticks(range(24))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame with relevant columns (e.g., 'weekday' and 'Appliances')\n",
        "# You can create a line chart to compare energy consumption on weekdays vs. weekends\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Group the data by 'weekday' and calculate the mean energy consumption for weekdays and weekends\n",
        "weekday_energy = data[data['weekday'] < 5].groupby('hour')['Appliances'].mean()\n",
        "weekend_energy = data[data['weekday'] >= 5].groupby('hour')['Appliances'].mean()\n",
        "\n",
        "# Plot energy consumption for weekdays and weekends\n",
        "plt.plot(weekday_energy.index, weekday_energy.values, label='Weekdays', marker='o')\n",
        "plt.plot(weekend_energy.index, weekend_energy.values, label='Weekends', marker='o')\n",
        "\n",
        "plt.title('Energy Consumption on Weekdays vs. Weekends')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Mean Energy Consumption')\n",
        "plt.xticks(range(24))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'data' is your DataFrame with relevant columns (e.g., 'Appliances', 'T_out', 'RH_out', 'Windspeed')\n",
        "# You can create scatter plots to explore these relationships\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "\n",
        "# Scatter plot of energy consumption vs. TDewPoint\n",
        "sns.scatterplot(data=data, x='Tdewpoint', y='Appliances', alpha=0.5, label='Outdoor Temperature')\n",
        "\n",
        "# Scatter plot of energy consumption vs. Press_mm_hg\n",
        "sns.scatterplot(data=data, x='Press_mm_hg', y='Appliances', alpha=0.5, label='Outdoor Humidity')\n",
        "\n",
        "# Scatter plot of energy consumption vs. wind speed\n",
        "sns.scatterplot(data=data, x='Windspeed', y='Appliances', alpha=0.5, label='Wind Speed')\n",
        "\n",
        "# Scatter plot of energy consumption vs. OUTSIDE_TEMP_wstn\n",
        "sns.scatterplot(data=data, x='OUTSIDE_TEMP_wstn', y='Appliances', alpha=0.5, label='OUTSIDE_TEMP_wstn')\n",
        "\n",
        "# Scatter plot of energy consumption vs. OUTSIDE_HUM_wstn\n",
        "sns.scatterplot(data=data, x='OUTSIDE_HUM_wstn', y='Appliances', alpha=0.5, label='OUTSIDE_HUM_wstn')\n",
        "\n",
        "plt.title('Energy Consumption vs. Weather Variables')\n",
        "plt.xlabel('Weather Variables')\n",
        "plt.ylabel('Energy Consumption')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing distributions using Histograms:\n",
        "data.hist(figsize=(17, 20), grid=True);"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "correlation_matrix = data.corr()\n",
        "plt.figure(figsize=(21, 18))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"RdYlGn\")\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OtkZZyU1fiOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of column names in your dataset\n",
        "columns = data.columns\n",
        "\n",
        "# Determine the number of rows and columns for subplots\n",
        "num_rows = len(columns)\n",
        "num_cols = 1\n",
        "\n",
        "# Create subplots with specified number of rows and columns\n",
        "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 80))\n",
        "\n",
        "# Iterate over each column (excluding \"Appliances\") and create pair plot\n",
        "for i, column in enumerate(columns):\n",
        "    #if column != \"Appliances\":\n",
        "        sns.scatterplot(data=data, x=\"Appliances\", y=column, ax=axes[i])\n",
        "        axes[i].set_xlabel(\"Appliances\")\n",
        "        axes[i].set_ylabel(column)\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pzVpJ9VlZ_qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-MMp3DFQLEqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is huge prescece of heteroscedasticity and we usually do log tranformation to solve this error."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no significant linear relationship between the independent variables and the appliance energy consumption.\n",
        "\n",
        "Alternative Hypothesis (H1): There is a significant linear relationship between the independent variables and the appliance energy consumption."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "OBBrJOQ5cjCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Extract the two continuous variables you want to test\n",
        "column_to_drop = ['Appliances']\n",
        "independent_variables = data.drop(column_to_drop, axis = 1)\n",
        "dependent_variable = data['Appliances']\n",
        "\n",
        "# Step 2: Perform the Correlation Test (Pearson correlation)\n",
        "correlation_coefficients, p_values = [], []\n",
        "for feature in independent_variables.columns:\n",
        "    correlation_coefficient, p_value = pearsonr(independent_variables[feature], dependent_variable)\n",
        "    correlation_coefficients.append(correlation_coefficient)\n",
        "    p_values.append(p_value)\n",
        "\n",
        "# Step 3: Interpret the Results for each feature\n",
        "alpha = 0.05  # Significance level (commonly set to 0.05)\n",
        "for i, feature in enumerate(independent_variables.columns):\n",
        "    print(f\"Correlation Coefficient for '{feature}': {correlation_coefficients[i]:.4f}\")\n",
        "    print(f\"P-value for '{feature}': {p_values[i]:.4f}\")\n",
        "\n",
        "    if p_values[i] < alpha:\n",
        "        print(\"Result: The correlation is statistically significant (reject H0).\\n\")\n",
        "    else:\n",
        "        print(\"Result: There is no significant correlation (fail to reject H0).\\n\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the practical implementation provided earlier, the statistical test used to obtain the p-value is the Pearson correlation coefficient test. The Pearson correlation coefficient, also known as Pearson's r or simply r, is a measure of the linear relationship between two continuous variables."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The p-value obtained from the test indicates the probability of observing the calculated correlation coefficient (or a more extreme value) if the null hypothesis is true. The null hypothesis (H0) in this context states that there is no significant linear relationship between the two variables.\n",
        "\n",
        "By comparing the p-value to a chosen significance level (alpha), commonly set to 0.05 (5%), we can determine whether to reject or fail to reject the null hypothesis. If the p-value is less than alpha, we reject the null hypothesis, suggesting a statistically significant correlation. If the p-value is greater than alpha, we fail to reject the null hypothesis, indicating no significant correlation.\n",
        "\n",
        "This test is appropriate when you want to assess the strength and direction of the linear relationship between two continuous variables. It is commonly used to explore the association between variables in correlation analysis and is widely used in various fields of research and data analysis."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thankfully there is no missing value in out dataset"
      ],
      "metadata": {
        "id": "qOnQ9TPjpJTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have not used any missing values handling technique as there are no Nan Values in the data set"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "df= data.copy()\n",
        "col_list = list(df.describe().columns)\n",
        "\n",
        "#find the outliers using boxplot\n",
        "plt.figure(figsize=(25, 20))\n",
        "plt.suptitle(\"Box Plot\", fontsize=18, y=0.95)\n",
        "\n",
        "for n, ticker in enumerate(col_list):\n",
        "\n",
        "    ax = plt.subplot(8, 4, n + 1)\n",
        "\n",
        "    plt.subplots_adjust(hspace=0.5, wspace=0.2)\n",
        "\n",
        "    sns.boxplot(x=df[ticker],color='pink', ax = ax)\n",
        "\n",
        "    # chart formatting\n",
        "    ax.set_title(ticker.upper())\n"
      ],
      "metadata": {
        "id": "DGTCoxgkaWIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def find_outliers_iqr(data):\n",
        "    # Calculate the first quartile (Q1) and third quartile (Q3) for each column\n",
        "    q1 = data.quantile(0.25)\n",
        "    q3 = data.quantile(0.75)\n",
        "\n",
        "    # Calculate the interquartile range (IQR) for each column\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    # Calculate the lower and upper bounds for outliers for each column\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "    # Check for outliers in each column and count the number of outliers\n",
        "    outliers_count = (data < lower_bound) | (data > upper_bound)\n",
        "    num_outliers = outliers_count.sum()\n",
        "\n",
        "    return num_outliers\n",
        "\n",
        "\n",
        "outliers_per_column = find_outliers_iqr(data)\n",
        "print(\"Number of outliers per column:\")\n",
        "print(outliers_per_column.sort_values(ascending = False))\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "for ftr in col_list:\n",
        "  print(ftr,'\\n')\n",
        "  q_25= np.percentile(df[ftr], 25)\n",
        "  q_75 = np.percentile(df[ftr], 75)\n",
        "  iqr = q_75 - q_25\n",
        "  print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q_25, q_75, iqr))\n",
        "  # calculate the outlier cutoff\n",
        "  cut_off = iqr * 1.5\n",
        "  lower = q_25 - cut_off\n",
        "  upper = q_75 + cut_off\n",
        "  print(f\"\\nlower = {lower} and upper = {upper} \\n \")\n",
        "  # identify outliers\n",
        "  outliers = [x for x in df[ftr] if x < lower or x > upper]\n",
        "  print('Identified outliers: %d' % len(outliers))\n",
        "  #removing outliers\n",
        "  if len(outliers)!=0:\n",
        "\n",
        "    def bin(row):\n",
        "      if row[ftr]> upper:\n",
        "        return upper\n",
        "      if row[ftr] < lower:\n",
        "        return lower\n",
        "      else:\n",
        "        return row[ftr]\n",
        "\n",
        "\n",
        "\n",
        "    data[ftr] =  df.apply (lambda row: bin(row), axis=1)\n",
        "    print(f\"{ftr} Outliers Removed\")\n",
        "  print(\"\\n-------\\n\")"
      ],
      "metadata": {
        "id": "gpoGcJ-laLK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25, 20))\n",
        "plt.suptitle(\"Box Plot without Outliers\", fontsize=18, y=0.95)\n",
        "#plot the all figures in loop with boxplot\n",
        "for n, ticker in enumerate(col_list):\n",
        "\n",
        "    ax = plt.subplot(8, 4, n + 1)\n",
        "\n",
        "    plt.subplots_adjust(hspace=0.5, wspace=0.2)\n",
        "\n",
        "    sns.boxplot(x=data[ticker],color='g' ,ax = ax)\n",
        "\n",
        "    # chart formatting\n",
        "    ax.set_title(ticker.upper())\n",
        ""
      ],
      "metadata": {
        "id": "ZrdPC2Srdk2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "iFTzKc6UcU8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# create new features\n",
        "# create a column average building temperature based on all temperature\n",
        "data['Average_building_Temperature']=data[['KITCHEN_TEMP','LIVING_TEMP','BEDROOM_TEMP','OFFICE_TEMP','BATHROOM_TEMP','IRONING_ROOM_TEMP','TEEN_ROOM_2_TEMP','PARENTS_ROOM_TEMP']].mean(axis=1)\n",
        "#create a column of difference between outside and inside temperature\n",
        "data['Temperature_difference']=abs(data['Average_building_Temperature']-data['OUTSIDE_TEMP_build'])\n",
        "\n",
        "#create a column average building humidity\n",
        "data['Average_building_humidity']=data[['KITCHEN_HUM','LIVING_HUM','BEDROOM_HUM', 'OFFICE_HUM','BATHROOM_HUM','IRONING_ROOM_HUM','TEEN_ROOM_HUM','PARENTS_ROOM_HUM']].mean(axis=1)\n",
        "#create a column of difference between outside and inside building humidity\n",
        "data['Humidity_difference']=abs(data['OUTSIDE_HUM_build']-data['Average_building_humidity'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "THq_WDcy0cvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#do not remove hour\n",
        "columns_to_drop = [\n",
        "'rv1',\n",
        "'rv2']\n",
        "data.drop(columns_to_drop, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "C_kdVVTSKQRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "WD_vkrWMV8EI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###finding the skewed and symmetrical data"
      ],
      "metadata": {
        "id": "8N_Na8hJcJcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the skewness in the dataset to check the distribution\n",
        "skewness = data.skew()\n",
        "\n",
        "#ginding the absolute value\n",
        "abs(skewness)\n",
        "\n",
        "# setting up the threshold\n",
        "skewness_threshold = 0.5\n",
        "\n",
        "# Separate features into symmetrical and skewed based on skewness threshold\n",
        "symmetrical_features = skewness[abs(skewness) < skewness_threshold].index\n",
        "skewed_features = skewness[abs(skewness) >= skewness_threshold].index\n",
        "\n",
        "# Create new DataFrames for symmetrical and skewed features\n",
        "print('FEATURES FOLLOWED SYMMETRICAL DISTRIBUTION :')\n",
        "symmetrical_data = data[symmetrical_features]\n",
        "print(symmetrical_features)\n",
        "\n",
        "print('FEATURES FOLLOWED SKEWED DISTRIBUTION :')\n",
        "skewed_data = data[skewed_features]\n",
        "print(skewed_features)\n"
      ],
      "metadata": {
        "id": "1fSKxWkHCi-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''skewed_data.drop('Appliances',axis = 1,inplace = True)'''"
      ],
      "metadata": {
        "id": "dIlgOh5tGoRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skewed_data"
      ],
      "metadata": {
        "id": "9f_lt6SWHEGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the liabrary\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# Initialize the PowerTransformer\n",
        "power_transformer = PowerTransformer()\n",
        "\n",
        "# Fit and transform the data using the PowerTransformer\n",
        "power_transformed = pd.DataFrame(power_transformer.fit_transform(skewed_data))\n",
        "power_transformed.columns = skewed_data.columns\n"
      ],
      "metadata": {
        "id": "XYFTzJnZHRA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "power_transformed"
      ],
      "metadata": {
        "id": "_lWwTodiIVeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index to the default integer index\n",
        "symmetrical_data.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "wIsZBYKLKSxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symmetrical_data"
      ],
      "metadata": {
        "id": "FqbmBhCgOSwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate horizontally (along columns)\n",
        "tranformed_data = pd.concat([symmetrical_data, power_transformed], axis=1)"
      ],
      "metadata": {
        "id": "-axrMYS-Ju78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tranformed_data"
      ],
      "metadata": {
        "id": "XYxVg0eOJ7De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Yes My data needs transformation specially skewed data , i used power transformaiton to solve this concern"
      ],
      "metadata": {
        "id": "dMFDTrcIcfEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Scaling the DATA set"
      ],
      "metadata": {
        "id": "yxxTpRupBq2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the desired liabrary\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = pd.DataFrame(scaler.fit_transform(tranformed_data))\n",
        "scaled_data.columns = tranformed_data.columns\n",
        "scaled_data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize a PCA instance without specifying the number of components\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA model to your standardized data\n",
        "pca.fit(scaled_data)\n",
        "\n",
        "# Calculate the cumulative explained variance\n",
        "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Create an elbow plot to visualize the explained variance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Elbow Plot')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Create a PCA instance and specify the number of components you want to retain\n",
        "# For example, if you want to retain 10 components, set n_components=10\n",
        "n_components = 10\n",
        "pca = PCA(n_components=n_components)\n",
        "\n",
        "# Fit the PCA model to your standardized data and transform it\n",
        "transformed_data_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "# The variable 'transformed_data_pca' now contains your data in the reduced-dimensional space with 'n_components' principal components.\n",
        "\n",
        "# You can also access explained variance to see how much variance is explained by each component\n",
        "explained_variance = pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "sHTojg5sUyJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explained_variance"
      ],
      "metadata": {
        "id": "MtWCswByX8B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_data_pca.shape"
      ],
      "metadata": {
        "id": "e5IshvswYBBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_data_pca"
      ],
      "metadata": {
        "id": "vJSrc4FGVPKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = transformed_data_pca\n",
        "y = data['Appliances']"
      ],
      "metadata": {
        "id": "bNJtj0r0CDnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=3)"
      ],
      "metadata": {
        "id": "as7VuE-HPHsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "70/30 Split: This ratio involves splitting the data into 70% for training and 30% for testing. It is a commonly used ratio when there is a sufficient amount of data available. The larger portion is used for training the model, while the smaller portion is used for evaluating its performance."
      ],
      "metadata": {
        "id": "noUyOxn2559T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 - Simple Linear Regression Model"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the mdoel\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#defining the object\n",
        "reg = LinearRegression()\n",
        "reg.fit(x_train, y_train)\n",
        "\n",
        "#training dataset score\n",
        "training_score = reg.score(x_train, y_train)\n",
        "\n",
        "#predicting the value\n",
        "y_pred = reg.predict(x_test)\n",
        "\n",
        "#calculating the training accuracy\n",
        "print(\"Train score:\" ,training_score)\n",
        "\n",
        "#calculating the MSE\n",
        "MSE  = mean_squared_error((y_test),(y_pred))\n",
        "print(\"Test MSE :\" , MSE)\n",
        "\n",
        "#calculating the testing accuracy\n",
        "r2 = r2_score((y_test),(y_pred))\n",
        "print(\"Test R2 :\" ,r2)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot to compare the predicted values against the actual values.\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3UPe56CAxrZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "o8NLZeV1a2uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The machine learning model used in your code is a linear regression model, which is a type of regression model used to predict a continuous target variable based on one or more independent variables. Here's an explanation of the model and its performance using the provided evaluation metric score chart:\n",
        "\n",
        "**Model Explanation**:\n",
        "- **Model Type**: Linear Regression\n",
        "  - Linear regression is a simple and commonly used regression technique that assumes a linear relationship between the independent variables and the target variable.\n",
        "\n",
        "**Performance Evaluation**:\n",
        "- **Training Score**: The training score is 0.176, which represents the coefficient of determination (R-squared) for the model's performance on the training data. It indicates that approximately 17.6% of the variance in the target variable (y_train) can be explained by the model. This suggests that the model has relatively weak explanatory power on the training data.\n",
        "\n",
        "- **Test Mean Squared Error (MSE)**: The test Mean Squared Error is 1518.43. MSE measures the average squared difference between the actual target values (y_test) and the predicted values (y_pred) on the test data. A lower MSE is desirable, and this value indicates the average prediction error of the model on the test data.\n",
        "\n",
        "- **Test R-squared (R2) Score**: The test R2 score is 0.156, which represents the coefficient of determination (R-squared) for the model's performance on the test data. It indicates that approximately 15.6% of the variance in the target variable (y_test) can be explained by the model. This suggests that the model has relatively weak explanatory power on the test data.\n",
        "\n",
        "**Interpretation**:\n",
        "- The linear regression model has limited predictive power in this context, as indicated by both the training and test R-squared scores. The R-squared values are relatively low, indicating that the model does not explain a significant proportion of the variance in the target variable.\n",
        "\n",
        "- The test Mean Squared Error (MSE) of 1518.43 suggests that the model's predictions have a relatively high average squared error when compared to the actual target values. This indicates that the model's predictions are not very accurate.\n",
        "\n",
        "**Recommendations**:\n",
        "- The model's performance can be improved by considering the following:\n",
        "  - Feature Engineering: Evaluate the features used in the model and consider feature selection or transformation to improve model performance.\n",
        "  - Model Complexity: Experiment with more complex models or non-linear models if a linear relationship may not adequately capture the data.\n",
        "  - Data Quality: Ensure the quality and relevance of the training and test data. Data preprocessing and data cleaning may be necessary.\n",
        "  - Hyperparameter Tuning: Optimize the hyperparameters of the linear regression model.\n",
        "\n",
        "In summary, the linear regression model used in this context shows limited explanatory power and predictive performance. To achieve better results, you may need to explore more advanced modeling techniques and further data analysis."
      ],
      "metadata": {
        "id": "jiC0wGYOed97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "\n",
        "# Create a Linear Regression model (you can replace this with any other regression model)\n",
        "model = LinearRegression()\n",
        "\n",
        "# Define hyperparameter search space (you can customize this based on your model)\n",
        "param_dist = {'fit_intercept': [True, False],\n",
        "              'copy_X': [True, False],\n",
        "              'positive':[True, False]}\n",
        "\n",
        "# Perform RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,\n",
        "                                   n_iter=10, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the RandomizedSearchCV to find the best hyperparameters\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Train the best model with the entire training dataset\n",
        "best_model.fit(x_train, y_train)\n",
        "\n",
        "training_score_val = best_model.score(x_train, y_train)\n",
        "# Evaluate the best model on the test set\n",
        "test_predictions = best_model.predict(x_test)\n",
        "\n",
        "# Calculate evaluation metrics for the test predictions (e.g., mean squared error)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(y_test, test_predictions)\n",
        "r2 = r2_score((y_test),(test_predictions))\n",
        "\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "\n",
        "#visual of training score\n",
        "print(\"Train score:\" ,training_score_val)\n",
        "print(\"Test MSE:\", mse)\n",
        "print(\"Test R2:\", r2)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.displot(test_predictions - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "SkDXF8syg0iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zUw-OVnIgpoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used the RandomizedSearchCV hyperparameter optimization technique. RandomizedSearchCV is a popular technique for hyperparameter tuning, and it is different from other techniques like GridSearchCV and Bayesian Optimization. Here's why you might choose RandomizedSearchCV over other techniques and why it's commonly used:\n",
        "\n",
        "**RandomizedSearchCV**:\n",
        "\n",
        "1. **Randomized Search Space Exploration**: RandomizedSearchCV, as the name suggests, explores the hyperparameter space in a randomized manner. It samples a specified number of candidate hyperparameter combinations from a defined search space. This approach can be more efficient than GridSearchCV for high-dimensional search spaces.\n",
        "\n",
        "2. **Efficiency**: When dealing with a large number of hyperparameters and their possible values, RandomizedSearchCV offers a way to efficiently explore different combinations without trying every possible combination, which can be time-consuming.\n",
        "\n",
        "3. **Parallel Processing**: It allows for parallel processing (as indicated by `n_jobs=-1`), which means it can take advantage of multi-core processors and speed up the search process.\n",
        "\n",
        "4. **Scalability**: RandomizedSearchCV is more scalable and suitable for large search spaces and datasets. It's often chosen when you have limited computational resources.\n",
        "\n",
        "5. **Balanced Exploration**: By randomly selecting hyperparameter combinations, it can provide a balanced exploration of the hyperparameter space, potentially avoiding overfitting to specific combinations.\n",
        "\n",
        "6. **Performance**: While it may not guarantee finding the absolute best hyperparameters, it often performs well in practice and can help discover hyperparameters that lead to good model performance.\n",
        "\n",
        "**GridSearchCV**, on the other hand, performs an exhaustive search over all possible hyperparameter combinations, which can be computationally expensive. It's typically suitable when you have a smaller search space or want to ensure that you've explored every possible combination.\n",
        "\n",
        "**Bayesian Optimization** is another technique that uses probabilistic models to guide the search process. It's beneficial when you have limited computational resources and want to make informed choices based on past evaluations.\n",
        "\n",
        "In summary, RandomizedSearchCV is chosen in scenarios where efficiency, scalability, and balanced exploration of the hyperparameter space are important. It is particularly useful when you have a large number of hyperparameters and their ranges to consider, making it a practical choice for many machine learning applications. However, the choice of hyperparameter optimization technique should depend on the specific characteristics of your problem, available resources, and the nature of the hyperparameter search space."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There doesn't seem to be any improvement in the model's performance as indicated by the evaluation metrics.\n",
        "\n",
        "Here's a comparison of the metrics before and after hyperparameter optimization:\n",
        "\n",
        "Before Hyperparameter Optimization (Before CV):\n",
        "- Train R-squared (R2) Score: 0.176\n",
        "- Test Mean Squared Error (MSE): 1518.43\n",
        "- Test R-squared (R2) Score: 0.156\n",
        "\n",
        "After Hyperparameter Optimization (After CV):\n",
        "- Train R-squared (R2) Score: 0.176\n",
        "- Test Mean Squared Error (MSE): 1518.43\n",
        "- Test R-squared (R2) Score: 0.156\n",
        "\n",
        "The metrics remain the same before and after hyperparameter optimization. This suggests that the optimization process did not lead to any noticeable improvement in the model's performance based on these evaluation metrics.\n",
        "\n",
        "In practice, hyperparameter optimization may not always result in significant improvements. The choice of hyperparameters and optimization techniques can vary depending on the dataset and the model. If the performance of the model is unsatisfactory, you may want to consider other approaches such as feature engineering, feature selection, trying different model algorithms, or collecting more data if possible. Additionally, exploring a wider range of hyperparameters or other optimization techniques may be necessary to make more substantial improvements."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "4DGpWFhQja7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explain each of the evaluation metrics mentioned and their indications toward business impact:\n",
        "\n",
        "1. **R-squared (R2) Score**:\n",
        "   - **Indication Towards Business**: R-squared measures the proportion of the variance in the target variable that can be explained by the model. It ranges from 0 to 1, where a higher R-squared indicates that the model explains more variance in the data.\n",
        "   - **Business Impact**: A high R-squared suggests that the model is effective in explaining and predicting the target variable, which can be valuable for business decision-making. It indicates how well the model aligns with the underlying patterns in the data. However, it's important to consider other metrics and domain knowledge to assess the true business impact.\n",
        "\n",
        "2. **Mean Squared Error (MSE)**:\n",
        "   - **Indication Towards Business**: MSE quantifies the average squared difference between predicted and actual values. Lower MSE values indicate more accurate predictions.\n",
        "   - **Business Impact**: A lower MSE signifies that the model's predictions are closer to the actual values, which is beneficial for businesses. It implies that the model's predictions are more reliable and can lead to more informed decisions. For example, in sales forecasting, lower MSE means better inventory management.\n",
        "\n",
        "3. **R-squared (R2) Score for Test Data**:\n",
        "   - **Indication Towards Business**: This R-squared score specifically measures the proportion of variance explained by the model on unseen test data. It helps assess the model's generalization performance.\n",
        "   - **Business Impact**: A high R2 score on the test data suggests that the model is likely to perform well in real-world scenarios and make accurate predictions. This leads to more confident and effective business decisions.\n",
        "\n",
        "In the provided context, the ML model's performance is not strong, as indicated by the low R-squared scores and relatively high MSE. The R-squared scores are less than 0.2, which means that the model explains a small proportion of the variance in the target variable. This can indicate that the model may not be highly reliable for making business decisions in its current form.\n",
        "\n",
        "**Business Impact**:\n",
        "- The low R-squared scores may result in less accurate predictions, potentially leading to suboptimal business decisions.\n",
        "- For businesses, making decisions based on a model with limited predictive power can be risky and result in inefficient resource allocation.\n",
        "\n",
        "To improve business impact, it's advisable to consider the following actions:\n",
        "- Explore more advanced modeling techniques that may capture the data's underlying patterns better.\n",
        "- Invest in data quality and feature engineering to extract more relevant information.\n",
        "- Collect additional data to enhance the model's performance.\n",
        "- Regularly monitor and update the model as new data becomes available.\n",
        "\n",
        "In conclusion, the evaluation metrics provide insights into the model's performance and its potential impact on business decisions. In this case, there is room for improvement to make the model more valuable for making informed and accurate business choices."
      ],
      "metadata": {
        "id": "OrGhlayXjnJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - Polynomial Regression model\n"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already split your data into x_train, x_test, y_train, and y_test\n",
        "\n",
        "# Choose the degree of the polynomial (e.g., 2 for quadratic)\n",
        "degree = 2\n",
        "\n",
        "# Create a Polynomial Regression model using a pipeline\n",
        "polyreg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "\n",
        "# Fit the model to the training data\n",
        "polyreg.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = polyreg.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Calculate the R2 score for the training data\n",
        "training_r2 = polyreg.score(x_train, y_train)\n",
        "\n",
        "print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared (R2) Score: {r2:.2f}\")"
      ],
      "metadata": {
        "id": "ymH6IB2GjXg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "kmERsQaXYTc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xUrAeYExZ_Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "KcEF3md1kE4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
        "\n",
        "# Create a Polynomial Regression model without specifying the degree\n",
        "polyreg = make_pipeline(PolynomialFeatures(), LinearRegression())\n",
        "\n",
        "# Define a range of polynomial degrees to be tested\n",
        "param_grid = {'polynomialfeatures__degree': range(1, 3)}\n",
        "\n",
        "# Initialize GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(polyreg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best polynomial degree\n",
        "best_degree = grid_search.best_params_['polynomialfeatures__degree']\n",
        "\n",
        "# Create a Polynomial Regression model with the best degree\n",
        "best_polyreg = make_pipeline(PolynomialFeatures(degree=best_degree), LinearRegression())\n",
        "\n",
        "# Perform cross-validation to evaluate the model\n",
        "cv_scores = cross_val_score(best_polyreg, x_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "cv_r2_scores = cross_val_score(best_polyreg, x_train, y_train, cv=5, scoring='r2')\n",
        "\n",
        "# Calculate the mean squared error and R2 score\n",
        "mse_cv = -cv_scores.mean()\n",
        "r2_cv = cv_r2_scores.mean()\n",
        "\n",
        "# Fit the best model to the training data\n",
        "best_polyreg.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = best_polyreg.predict(x_test)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best Polynomial Degree: {best_degree}\")\n",
        "print(f\"Cross-Validation Mean Squared Error: {mse_cv:.2f}\")\n",
        "print(f\"Cross-Validation R-squared (R2) Score: {r2_cv:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared (R2) Score: {r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "dyEaycxPXV0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "tb8NDSy7jq3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There doesn't seem to be any improvement in the model's performance as indicated by the evaluation metrics.\n",
        "\n",
        "Here's a comparison of the metrics before and after hyperparameter optimization:\n",
        "\n",
        "Before Hyperparameter Optimization (Before CV):\n",
        "- Train R-squared (R2) Score: 0.176\n",
        "- Test Mean Squared Error (MSE): 1518.43\n",
        "- Test R-squared (R2) Score: 0.156\n",
        "\n",
        "After Hyperparameter Optimization (After CV):\n",
        "- Train R-squared (R2) Score: 0.176\n",
        "- Test Mean Squared Error (MSE): 1518.43\n",
        "- Test R-squared (R2) Score: 0.156\n",
        "\n",
        "The metrics remain the same before and after hyperparameter optimization. This suggests that the optimization process did not lead to any noticeable improvement in the model's performance based on these evaluation metrics.\n",
        "\n",
        "In practice, hyperparameter optimization may not always result in significant improvements. The choice of hyperparameters and optimization techniques can vary depending on the dataset and the model. If the performance of the model is unsatisfactory, you may want to consider other approaches such as feature engineering, feature selection, trying different model algorithms, or collecting more data if possible. Additionally, exploring a wider range of hyperparameters or other optimization techniques may be necessary to make more substantial improvements.\n",
        "\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Indented block\n",
        "\n"
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 - RIDGE Regression Model"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already created the 'x_train', 'x_test', 'y_train', and 'y_test' datasets\n",
        "# 'x_train' and 'x_test' are the results of polynomial regression on PCA-transformed data\n",
        "\n",
        "# Create a PolynomialFeatures instance (with degree=2 for quadratic features)\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Transform the data to include polynomial features\n",
        "x_train_poly = poly_features.fit_transform(x_train)\n",
        "x_test_poly = poly_features.transform(x_test)\n",
        "\n",
        "# Create a Ridge regression model\n",
        "ridge_reg = Ridge(alpha=1.0)  # You can adjust the alpha parameter (regularization strength)\n",
        "\n",
        "# Fit the Ridge model to the training data\n",
        "ridge_reg.fit(x_train_poly, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = ridge_reg.predict(x_test_poly)\n",
        "\n",
        "# Calculate R-squared (R2) for the test data\n",
        "test_r2 = ridge_reg.score(x_test_poly, y_test)\n",
        "\n",
        "# Calculate R-squared (R2) for the training data\n",
        "training_r2 = ridge_reg.score(x_train_poly, y_train)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for the test data\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(f\"Test R-squared (R2) Score: {test_r2:.2f}\")\n",
        "print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n"
      ],
      "metadata": {
        "id": "gxks1j-natPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "JlupRzxBAIqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V0hHNR6qAQp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "99QQPsSuAQfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of the degree for polynomial regression should be based on the balance between model complexity and performance. In your case, you have results for two different degrees, 2 and 3. Here's an interpretation of your results:\n",
        "\n",
        "**Degree 2 Model:**\n",
        "- Test R-squared (R2) Score: 0.23\n",
        "- Training R-squared (R2) Score: 0.27\n",
        "- Mean Squared Error (MSE): 7933.72\n",
        "\n",
        "**Degree 3 Model:**\n",
        "- Test R-squared (R2) Score: 0.36\n",
        "- Training R-squared (R2) Score: 0.27\n",
        "- Mean Squared Error (MSE): 6589.76\n",
        "\n",
        "Here are some insights:\n",
        "\n",
        "1. **Test R-squared (R2) Score**: The degree-3 model has a higher R2 score on the test data, indicating that it explains more of the variance in the target variable compared to the degree-2 model. A higher R2 suggests better predictive performance.\n",
        "\n",
        "2. **Training R-squared (R2) Score**: Both models have similar training R2 scores. The degree-2 model's training R2 score is slightly lower, which could indicate some overfitting.\n",
        "\n",
        "3. **Mean Squared Error (MSE)**: The degree-3 model has a lower MSE on the test data, which is a measure of prediction accuracy. A lower MSE is generally desirable.\n",
        "\n",
        "Given these insights, the degree-3 model appears to be better at capturing the underlying relationships in the data and making predictions on unseen data. It has a higher R2 score and a lower MSE on the test data. However, you should also consider the complexity of the model and potential overfitting. If you believe that the degree-3 model generalizes well to new data and doesn't overfit, it may be a good choice."
      ],
      "metadata": {
        "id": "cSq6EYrgb8TS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n"
      ],
      "metadata": {
        "id": "aSkQDwTWxvud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have 'x' and 'y' as your data and target variable\n",
        "\n",
        "# Create a PolynomialFeatures instance (with degree=3 for cubic features)\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Create a Ridge regression model\n",
        "ridge_reg = Ridge()\n",
        "\n",
        "# Create a pipeline with the polynomial features and Ridge regression\n",
        "pipeline = Pipeline([\n",
        "    ('polynomial_features', poly_features),\n",
        "    ('ridge_regression', ridge_reg)\n",
        "])\n",
        "\n",
        "# Define hyperparameters and values to search\n",
        "param_grid = {\n",
        "    'ridge_regression__alpha': [0.001, 0.01, 0.1, 1]  # You can adjust the alpha values\n",
        "}\n",
        "\n",
        "# Perform Grid Search with Cross-Validation\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(x, y)  # Use the full dataset for cross-validation\n",
        "\n",
        "# Get the best hyperparameters from the grid search\n",
        "best_alpha = grid_search.best_params_['ridge_regression__alpha']\n",
        "\n",
        "# Create a Ridge regression model with the best hyperparameters\n",
        "best_ridge_reg = Ridge(alpha=best_alpha)\n",
        "\n",
        "# Fit the Ridge model to the training data\n",
        "best_ridge_reg.fit(x_train, y_train)\n",
        "\n",
        "# Calculate cross-validated R-squared (R2) scores\n",
        "cv_scores = cross_val_score(best_ridge_reg, x_train, y_train, cv=5, scoring='r2')\n",
        "\n",
        "# Calculate R-squared (R2) score on the test data\n",
        "test_r2 = best_ridge_reg.score(x_test, y_test)\n",
        "\n",
        "print(f\"Best Alpha: {best_alpha}\")\n",
        "print(f\"Cross-Validated R-squared (R2) Scores: {cv_scores}\")\n",
        "print(f\"Mean R-squared (R2) Score: {np.mean(cv_scores):.2f}\")\n",
        "print(f\"Training R-squared (R2) Score: {best_ridge_reg.score(x_train, y_train):.2f}\")\n",
        "print(f\"Test R-squared (R2) Score: {test_r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "sxNXg6XccnYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dF-mi86ygzrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4 - Lasso Regression Model"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already created the 'x_train', 'x_test', 'y_train', and 'y_test' datasets\n",
        "# 'x_train' and 'x_test' are the results of polynomial regression on PCA-transformed data\n",
        "\n",
        "# Create a PolynomialFeatures instance (with degree=3 for cubic features)\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Transform the data to include polynomial features\n",
        "x_train_poly = poly_features.fit_transform(x_train)\n",
        "x_test_poly = poly_features.transform(x_test)\n",
        "\n",
        "# Create a Lasso regression model\n",
        "lasso_reg = Lasso(alpha=1.0)  # You can adjust the alpha parameter (regularization strength)\n",
        "\n",
        "# Fit the Lasso model to the training data\n",
        "lasso_reg.fit(x_train_poly, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = lasso_reg.predict(x_test_poly)\n",
        "\n",
        "# Calculate R-squared (R2) for the test data\n",
        "test_r2 = lasso_reg.score(x_test_poly, y_test)\n",
        "\n",
        "# Calculate R-squared (R2) for the training data\n",
        "training_r2 = lasso_reg.score(x_train_poly, y_train)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for the test data\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"Test R-squared (R2) Score: {test_r2:.2f}\")\n",
        "print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t724bczkg1YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have 'x' and 'y' as your data and target variable\n",
        "\n",
        "# Split the data into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=3)\n",
        "\n",
        "# Create a PolynomialFeatures instance (with degree=3 for cubic features)\n",
        "poly_features = PolynomialFeatures(degree=2)\n",
        "\n",
        "# Transform the data to include polynomial features\n",
        "x_train_poly = poly_features.fit_transform(x_train)\n",
        "x_test_poly = poly_features.transform(x_test)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "x_train_poly = scaler.fit_transform(x_train_poly)\n",
        "x_test_poly = scaler.transform(x_test_poly)\n",
        "\n",
        "# Create a Lasso regression model\n",
        "lasso_reg = Lasso(max_iter=10000)  # Increase max_iter and adjust the alpha parameter if needed\n",
        "\n",
        "# Define hyperparameters and values to search\n",
        "param_grid = {\n",
        "    'alpha': [0.001, 0.01, 0.1, 1, 10]  # You can adjust the alpha values\n",
        "}\n",
        "\n",
        "# Perform Grid Search with Cross-Validation\n",
        "grid_search = GridSearchCV(lasso_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(x_train_poly, y_train)  # Use the training data for cross-validation\n",
        "\n",
        "# Get the best hyperparameters from the grid search\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "\n",
        "# Create a Lasso regression model with the best hyperparameters\n",
        "best_lasso_reg = Lasso(alpha=best_alpha, max_iter=10000)\n",
        "\n",
        "# Fit the Lasso model to the training data\n",
        "best_lasso_reg.fit(x_train_poly, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = best_lasso_reg.predict(x_test_poly)\n",
        "\n",
        "# Calculate R-squared (R2) for the test data\n",
        "test_r2 = best_lasso_reg.score(x_test_poly, y_test)\n",
        "\n",
        "# Calculate R-squared (R2) for the training data\n",
        "training_r2 = best_lasso_reg.score(x_train_poly, y_train)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for the test data\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) for the training data\n",
        "training_mse = mean_squared_error(y_train, best_lasso_reg.predict(x_train_poly))\n",
        "\n",
        "# Calculate cross-validated R-squared (R2) scores\n",
        "cv_scores = cross_val_score(best_lasso_reg, x, y, cv=5, scoring='r2')\n",
        "\n",
        "print(f\"Best Alpha: {best_alpha}\")\n",
        "print(f\"Test R-squared (R2) Score: {test_r2:.2f}\")\n",
        "print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "print(f\"Training Mean Squared Error (MSE): {training_mse:.2f}\")\n",
        "print(f\"Cross-Validated R-squared (R2) Scores: {cv_scores}\")\n",
        "print(f\"Mean R-squared (R2) Score: {np.mean(cv_scores):.2f}\")\n"
      ],
      "metadata": {
        "id": "HefHlXLpIh0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hdY1YGc2g2uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5 - elastic net Regression Model"
      ],
      "metadata": {
        "id": "SAu0wU95I9lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import  ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
        "\n",
        "# Specify the degree of polynomial (you can change this based on your data)\n",
        "degree = 2\n",
        "\n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=degree)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "X_test_poly = poly_features.transform(X_test)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "ElasticNet_model = ElasticNet(alpha=1.0)\n",
        "\n",
        "# Train the model using the polynomial features\n",
        "ElasticNet_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = ElasticNet_model.predict(X_train_poly)\n",
        "test_predictions = ElasticNet_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Polynomial Regression (Degree {}):\".format(degree))\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)"
      ],
      "metadata": {
        "id": "A7aJUHSjJEc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1LjuNiUfg39K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create a Ridge Regression model\n",
        "ElasticNet_model = ElasticNet()\n",
        "\n",
        "# Perform Cross-Validation and Hyperparameter Tuning\n",
        "param_grid = {'alpha': [0.1, 1.0, 10.0]}  # Define the hyperparameter grid\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=ElasticNet_model, param_grid=param_grid,\n",
        "                           scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV to find the best degree and alpha\n",
        "grid_search.fit(X_train_poly, y_train)\n",
        "\n",
        "# Get the best degree and alpha from the GridSearchCV results\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = best_model.predict(X_train_poly)\n",
        "test_predictions = best_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Best Alpha:\", best_alpha)\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)"
      ],
      "metadata": {
        "id": "X57hdecTOHvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IViw6ojTg48L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 6 - Ranfom Forest Regressor"
      ],
      "metadata": {
        "id": "iPocHRDIgkRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Create a Random Forest Regressor model\n",
        "rf_model = RandomForestRegressor(n_estimators=20, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions_rf = rf_model.predict(X_train_poly)\n",
        "test_predictions_rf = rf_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse_rf = mean_squared_error(y_train, train_predictions_rf)\n",
        "test_mse_rf = mean_squared_error(y_test, test_predictions_rf)\n",
        "\n",
        "train_r2_rf = r2_score(y_train, train_predictions_rf)\n",
        "test_r2_rf = r2_score(y_test, test_predictions_rf)\n",
        "\n",
        "print(\"Random Forest Regressor:\")\n",
        "print(\"Train MSE:\", train_mse_rf)\n",
        "print(\"Test MSE:\", test_mse_rf)\n",
        "print(\"Train R-squared:\", train_r2_rf)\n",
        "print(\"Test R-squared:\", test_r2_rf)"
      ],
      "metadata": {
        "id": "CvU_uznThPU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YFoh7Fe4g54R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Create a Random Forest Regressor model\n",
        "rf_model = RandomForestRegressor(n_estimators=20, random_state=42)\n",
        "\n",
        "# Define scoring functions\n",
        "scoring = {\n",
        "    'mse': make_scorer(mean_squared_error),\n",
        "    'r2': make_scorer(r2_score)}\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "test_predictions_rf = rf_model.predict(X_test_poly)\n",
        "\n",
        "# Calculate Test MSE and Test R-squared\n",
        "test_mse_rf = mean_squared_error(y_test, test_predictions_rf)\n",
        "test_r2_rf = r2_score(y_test, test_predictions_rf)\n",
        "\n",
        "# Perform cross-validation\n",
        "k = 5  # Number of folds (you can adjust this as needed)\n",
        "mse_scores = -cross_val_score(rf_model, X_train_poly, y_train, cv=k, scoring=scoring['mse'])\n",
        "r2_scores = cross_val_score(rf_model, X_train_poly, y_train, cv=k, scoring=scoring['r2'])\n",
        "\n",
        "# Calculate the mean and standard deviation of MSE and R-squared\n",
        "mean_mse = np.mean(mse_scores)\n",
        "mean_r2 = np.mean(r2_scores)\n",
        "\n",
        "# Print the cross-validation results\n",
        "print(\"Cross-Validation Results for Random Forest Regressor:\")\n",
        "print(f\"Train MSE: {mean_mse:.2f} \")\n",
        "print(f\"Train R-squared: {mean_r2:.2f} \")\n",
        "print(f\"Test MSE: {test_mse_rf:.2f}\")\n",
        "print(f\"Test R-squared: {test_r2_rf:.2f}\")\n"
      ],
      "metadata": {
        "id": "CElotrAA91r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_rf - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "KvAFuKIuoTQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions_rf)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IvQHTP0zg7IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 7 - GRADIENT BOOSTING"
      ],
      "metadata": {
        "id": "cv3Xato39JSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Create a Gradient Boosting Regressor model\n",
        "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gb_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions_gb = gb_model.predict(X_train_poly)\n",
        "test_predictions_gb = gb_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse_gb = mean_squared_error(y_train, train_predictions_gb)\n",
        "test_mse_gb = mean_squared_error(y_test, test_predictions_gb)\n",
        "\n",
        "train_r2_gb = r2_score(y_train, train_predictions_gb)\n",
        "test_r2_gb = r2_score(y_test, test_predictions_gb)\n",
        "\n",
        "print(\"Gradient Boosting Regressor:\")\n",
        "print(\"Train MSE:\", train_mse_gb)\n",
        "print(\"Test MSE:\", test_mse_gb)\n",
        "print(\"Train R-squared:\", train_r2_gb)\n",
        "print(\"Test R-squared:\", test_r2_gb)"
      ],
      "metadata": {
        "id": "EhZ-3mgo84eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_gb - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "qord_ocNoMbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions_gb)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Eq7bV1jOhELY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''# We can use this code snippet for cross validation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import numpy as np\n",
        "\n",
        "# Create a Gradient Boosting Regressor\n",
        "gb_model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Define a parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'max_depth': [3, 4]\n",
        "}\n",
        "\n",
        "# Create a GridSearchCV object with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Fit the GridSearchCV object on your data\n",
        "grid_search.fit(X_train_poly, y_train)\n",
        "\n",
        "# Get the best model from the search\n",
        "best_gb_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "train_predictions_gb = best_gb_model.predict(X_train_poly)\n",
        "test_predictions_gb = best_gb_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the best model\n",
        "train_mse_gb = mean_squared_error(y_train, train_predictions_gb)\n",
        "test_mse_gb = mean_squared_error(y_test, test_predictions_gb)\n",
        "train_r2_gb = r2_score(y_train, train_predictions_gb)\n",
        "test_r2_gb = r2_score(y_test, test_predictions_gb)\n",
        "\n",
        "print(\"Best Gradient Boosting Regressor after hyperparameter tuning:\")\n",
        "print(\"Train MSE:\", train_mse_gb)\n",
        "print(\"Test MSE:\", test_mse_gb)\n",
        "print(\"Train R-squared:\", train_r2_gb)\n",
        "print(\"Test R-squared:\", test_r2_gb)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)'''"
      ],
      "metadata": {
        "id": "6wlmVFoxcPaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 8 - XGBOOST"
      ],
      "metadata": {
        "id": "54RxJBLH9Gq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Create an XGBoost Regressor model\n",
        "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions_xgb = xgb_model.predict(X_train_poly)\n",
        "test_predictions_xgb = xgb_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse_xgb = mean_squared_error(y_train, train_predictions_xgb)\n",
        "test_mse_xgb = mean_squared_error(y_test, test_predictions_xgb)\n",
        "\n",
        "train_r2_xgb = r2_score(y_train, train_predictions_xgb)\n",
        "test_r2_xgb = r2_score(y_test, test_predictions_xgb)\n",
        "\n",
        "print(\"XGBoost Regressor:\")\n",
        "print(\"Train MSE:\", train_mse_xgb)\n",
        "print(\"Test MSE:\", test_mse_xgb)\n",
        "print(\"Train R-squared:\", train_r2_xgb)\n",
        "print(\"Test R-squared:\", test_r2_xgb)\n",
        "\n"
      ],
      "metadata": {
        "id": "C2xGYM3n9DdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(test_predictions_xgb - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "v9VM8ddonWJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions_xgb)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xCVT4J1jpBJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 9 - SUPPORT VECTOR REGRESSOR"
      ],
      "metadata": {
        "id": "LNIFsH1rCnMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already split your data into x_train, x_test, y_train, and y_test\n",
        "# Split the data into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=3)\n",
        "\n",
        "# Create an SVR model\n",
        "svr = SVR(kernel='rbf')  # You can choose the kernel (e.g., 'linear', 'rbf', 'poly')\n",
        "\n",
        "# Fit the SVR model to the training data\n",
        "svr.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = svr.predict(x_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Calculate the R2 score for the training data\n",
        "training_r2 = svr.score(x_train, y_train)\n",
        "\n",
        "print(f\"Training R-squared (R2) Score: {training_r2:.2f}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared (R2) Score: {r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "gYs9SHe2Hr-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lGBHa-xXhJu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}